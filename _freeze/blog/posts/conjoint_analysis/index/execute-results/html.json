{
  "hash": "cf15d3639ee51fda8ab893430ec42080",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Anytime-valid Inference on the AMCE in Conjoint Experiments\"\ndescription: |\n  An empirical example based on <a href=\"https://doi.org/10.1093/pan/mpt024\">Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments by Hainmueller, Hopkins, and Yamamoto.</a>\ndate: \"3/6/2025\"\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-overflow: scroll\n    code-fold: true\n    code-summary: \"Code\"\n    fig-dpi: 300\nexecute:\n  freeze: true\npage-layout: full\ncategories: [Conjoints, Anytime-valid Inference]\nimage: \"./thumbnail.jpg\"\n---\n\n\n\nIn this post I'll be exploring an empirical example to demonstrate how we can\ntarget the Average Marginal Component Effect (AMCE) and perform a standard,\nfixed-n analysis, and how we can adopt methods from the anytime-valid\nliterature to perform a sequential analysis of the AMCE.\n\nFor background, see\n[this super-detailed discussion of conjoint analysis](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/)\nand [this one on marginal effects](https://www.andrewheiss.com/blog/2022/05/20/marginalia/)\nby Andrew Heiss.\n\n## Candidate experiment --- setup\n\nThe empirical example we'll base this analysis on is the candidate experiment\nin Hainmueller et al.'s\n[Causal Inference in Conjoint Analysis](https://doi.org/10.1093/pan/mpt024)\npaper. Instead of describing it myself I'll just include the paper's\ndescription of the experiment:\n\n> The choice between competing candidates for elected office is central to democracy. Candidates\ntypically differ on a variety of dimensions, including their personal background and demographic\ncharacteristics, issue positions, and prior experience. The centrality of partisanship to voter decision\nmaking is amply documented [ref], so\nwe focus here on the less-examined role of candidates’ personal traits [ref].\nWithin the United States, there is constant speculation about the role of candidates’ personal\nbackgrounds in generating support or opposition; here, we harness conjoint analysis to examine\nthose claims. <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We focus on eight attributes of would-be presidential candidates, all of which have emerged in\nrecent campaigns. Six of these attributes can take on one of six values, including the candidates’\nreligion (Catholic, Evangelical Protestant, Mainline Protestant, Mormon, Jewish, or None), college\neducation (no college, state university, community college, Baptist college, Ivy League college, or\nsmall college), profession (lawyer, high school teacher, business owner, farmer, doctor, or car\ndealer), annual income ($32K, $54K, $65K, $92K, $210K, and $5.1M), racial/ethnic background\n(Hispanic, White, Caucasian, Black, Asian American, and Native American), and age (36, 45, 52,\n60, 68, and 75). 3 Two other attributes take on only two values: military service (served or not) and\ngender (male or female). Each respondent to our online survey---administered through Amazon’s\nMechanical Turk [ref]---saw six pairs of profiles that were generated\nusing the fully randomized approach described below. Figure A.1 in the Supplemental Information\n(SI) illustrates one choice presented to one respondent. The profiles were presented side-by-side,\nwith each pair of profiles on a separate screen. To ease the cognitive burden for respondents while\nalso minimizing primacy and recency effects, the attributes were presented in a randomized order\nthat was fixed across the six pairings for each respondent.\n<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the same screen as each candidate pairing, respondents were asked multiple questions which\nserve as dependent variables. First, they were asked to choose between the two candidates, a\n“forced-choice” design that enables us to evaluate the role of each attribute value in the assessment\nof one profile relative to another. This question closely resembles real-world voter decision making,\nin which respondents must cast a single ballot between competing candidates who vary on multiple\ndimensions. In the second and third questions following the profiles, the respondents rated \neach candidate on a one to seven scale, enabling evaluations of the levels of absolute support or\nopposition to each profile separately.\n\n## Candidate experiment --- fixed-n analysis\n\nThe data for the candidate experiment is publically available\n[here](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/data/candidate.dta)\nor [here](https://doi.org/10.7910/DVN/THJYQR). Let's begin by loading some\nnecessary packages and the data:\n\n### Dependencies and data prep\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(fixest)\nlibrary(forcats)\nlibrary(future)\nlibrary(future.apply)\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(haven)\nlibrary(marginaleffects)\nlibrary(progressr)\nlibrary(scales)\nlibrary(tibble)\nlibrary(tidyr)\n\ncandidate <- read_dta(\"/Users/dmolitor/Downloads/candidate.dta\")\n```\n:::\n\n\n\nNow let's make the column names be nice and readable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariable_lookup <- tribble(\n  ~variable,    ~variable_nice,\n  \"atmilitary\", \"Military\",\n  \"atreligion\", \"Religion\",\n  \"ated\",       \"Education\",\n  \"atprof\",     \"Profession\",\n  \"atmale\",     \"Gender\",\n  \"atinc\",      \"Income\",\n  \"atrace\",     \"Race\",\n  \"atage\",      \"Age\"\n)\n\ncandidate <- as_factor(candidate) |>\n  rename(respondent = resID) |>\n  rename_with(\n    .cols = atmilitary:atmale,\n    .fn = \\(x) vapply(\n      x,\n      \\(y) filter(variable_lookup, variable == y)$variable_nice,\n      character(1)\n    )\n  )\n\nglimpse(candidate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,466\nColumns: 11\n$ respondent <fct> A2NEN4NSNS2O8S, A2NEN4NSNS2O8S, A2NEN4NSNS2O8S, A2NEN4NSNS2…\n$ Military   <fct> Did Not Serve, Served, Did Not Serve, Served, Served, Serve…\n$ Religion   <fct> Mormon, None, Catholic, Mainline protestant, None, Mainline…\n$ Education  <fct> Community college, No BA, Small college, Small college, Bap…\n$ Profession <fct> Car dealer, High school teacher, Farmer, Doctor, Doctor, La…\n$ Income     <fct> 5.1M, 65K, 32K, 54K, 5.1M, 54K, 5.1M, 5.1M, 32K, 54K, 32K, …\n$ Race       <fct> White, Asian American, Native American, White, Native Ameri…\n$ Age        <fct> 75, 60, 68, 75, 45, 52, 52, 75, 75, 60, 45, 45, 45, 60, 36,…\n$ Gender     <fct> Female, Male, Female, Male, Female, Male, Male, Female, Fem…\n$ selected   <dbl> 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,…\n$ rating     <dbl> 0.3333333, 0.5000000, 0.6666667, 0.6666667, 0.3333333, 0.50…\n```\n\n\n:::\n:::\n\n\n\nEach row in the dataset represents one candidate profile out of 1,733 candidate comparisons made\nby 311 survey respondents. Each of the candidate comparisons has 2 candidate profiles to choose\nbetween, so the total number of profiles in the data is 3,466 (1,733 x 2). The `selected` column\nis a binary outcome indicating whether the survey respondent (identified in the `respondent` column)\nselected that profile as the preferred candidate.\n\n### Estimating fixed-n AMCEs\n\nThe first estimand of interest is the AMCE. In potential outcomes notation you can define the \nAMCE as the average difference in potential outcomes when you change the level of a\nparticular attribute while averaging over the joint distribution of the other attributes.\nFor example, let\n\n- $Y_i(X_j=x_j,X_{−j})$ be the potential outcome for respondent $i$ given\ncandidate profile $X$ when the candidate profile has attribute $j$ set to $X_j=x_j$ and\nthe other attributes $X_{-j}$​ set to some arbitrary values.\n- $X_j^*$ be the baseline (or reference) level for attribute $j$.\n\nThen the AMCE for changing attribute $j$ from $X_j=X_j^*$​ to $X_j=x_j$​ can be written as\n$$\\text{AMCE}(X_j)=E_{X_{−j}}[Y_i(X_j=x_j,X_{−j})−Y_i(X_j=X_j^∗, X_{−j})],$$\nwhere expectation $E_{X_{-j}}$​​ is taken over the joint distribution of the other attributes.\n\nFortunately, this estimand can be easily estimated with a regression model. Even better,\nwe can simultaneously estimate AMCEs for all attributes by including all attribute\nvariables in our regression model. **Note:** Since survey participants each responded to\nmultiple questions it is essential that we cluster standard errors at the respondent level.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to add the reference level for each of our attributes\nadd_reference <- function(data, low = \"conf.low\", high = \"conf.high\") {\n  data <- data |>\n    separate(col = \"contrast\", into = c(\"contrast\", \"reference\"), sep = \" - \")\n  data <- bind_rows(\n    data,\n    data |>\n      mutate(contrast = reference) |>\n      distinct(term, contrast, .keep_all = TRUE) |>\n      mutate(across(c(estimate, {{low}}, {{high}}), ~ 0))\n  )\n  return(data)\n}\n\n# Estimate AMCEs with a logistic regression model\nmodel <- feglm(\n  selected ~ Military + Religion + Education + Profession + Income + Race + Age\n    + Gender,\n  data = candidate,\n  family = \"binomial\",\n  cluster = ~ respondent\n)\n\n# Calculate the marginal effects (AMCEs) as probabilities\nmarginal_effects <- avg_slopes(model, newdata = \"mean\")\n\nggplot(\n  add_reference(marginal_effects),\n  aes(\n    x = estimate,\n    y = contrast,\n    xmin = conf.low,\n    xmax = conf.high,\n    color = term\n  )\n) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(), legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Fixed-n AMCE estimation-1.png){width=2100}\n:::\n:::\n\n\n\n### Estimating fixed-n marginal means\n\nAnother estimand of interest is a purely descriptive quantity---marginal means.\nThe marginal mean for a specific value $x_j$​ of some attribute $j$ is defined as the\naverage observed outcome for all profiles where that attribute takes on the value\n$X_j = x_j$​, averaging over the joint distribution of all other attributes.\nIn mathematical notation, this is expressed as $$\\mu(x_j) := E[Y | X_j=x_j],$$\nwhere $Y$ are the observed outcomes, $X_j$​ is the attribute of interest, and\nthe expectation $E$ is taken over the distribution of all other attributes $X_{-j}$​.\nThis expression simply calculates the average observed outcome when $X_j=x_j$​, without\ninvoking any counterfactual or potential outcomes framework.\n\nWe could estimate this non-parametrically just with `group_by()` and `summarize()`\nbut we can also get the same result with corresponding CIs using a regression like\nabove.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A function to calculate the marginal mean for a single attribute\nmarginal_mean <- function(model, attribute) {\n  mms <- avg_predictions(model, \"balanced\", by = attribute)\n  mms <- mms |>\n    rename(\"level\" = {{attribute}}) |>\n    mutate(\"term\" = {{attribute}})\n  return(mms)\n}\n\nmarginal_means <- bind_rows(\n  lapply(\n    c(\n      \"Military\",\n      \"Religion\",\n      \"Education\",\n      \"Profession\",\n      \"Income\",\n      \"Race\",\n      \"Age\",\n      \"Gender\"\n    ),\n    function(attribute) marginal_mean(model, attribute)\n  )\n)\n\nggplot(\n  marginal_means,\n  aes(\n    x = estimate,\n    y = level,\n    xmin = conf.low,\n    xmax = conf.high,\n    color = term\n  )\n) +\n  geom_vline(xintercept = 0.5, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(x = \"Marginal Means\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(), legend.position = \"none\")\n```\n:::\n\n\n\n## Candidate experiment --- sequential analysis\n\nAll the analysis performed up to this point has been utilizing estimators\nthat are fixed-n valid. In short, this means that they are only valid for\na pre-specified $N$. We run an experiment, collect $N$ samples, and analyze\nthe data. Our estimators give us certain statistical guarantees at this\nfixed sample size. For example, each of the estimated parameters above has\na corresponding 95% confidence interval (CI). In the fixed-n setting, each\nindividual AMCE (or marginal mean) CI has a 95% probability of containing\nthe true parameter value---at least from a frequentist perspective.\n\nAs researchers, we may be particularly interested in estimating AMCEs for\ncertain attributes and we may want to ensure that we have sufficient sample\nsize to have sufficient statistical power to detect non-zero AMCEs for those\nattributes. Typically researchers will utilize power calculations to determine\nsufficient sample sizes to detect treatment effects of interest prior to\nrunning an experiment. However, power calculations rely on unverifiable\nassumptions which can cause them to be highly inaccurate not to mention\ndifficult to perform, particularly in the context of conjoint experiments.\n\nWhat we would **love** is to be able to continuously monitor our conjoint\nexperiment as we collect data and watch our AMCE (or marginal means) estimates\nand their CIs and to stop the experiment as soon as the AMCE for our\nattribute of interest is statistically significant. Our current estimators\nfor the AMCE are not suitable for this. For a little more detail on why,\nsee my [simulations on anytime-valid linear models](https://www.dmolitor.com/blog/posts/anytime_valid_linear_models/). In short, if we repeatedly calculate\nour AMCE estimates and 95% CIs and collect more data until they are\nstatistically significant, our probability of committing a Type 1 error\nwill massively increase and will converge to 1 the more often we do this.\n\nHowever, there is a solution to this! The blog post above details methods\nfor estimating linear regression coefficients that allow us to do\nexactly this while keeping the probability of committing a Type 1 error\nbelow our nominal significance level (e.g. $\\alpha = 0.05$). We refer\nto this as anytime-valid inference on our regression coefficients.\nIn particular, the methods above develop anytime-valid p-values and confidence\nsequences (anytime-valid corollary to CIs) for linear regression coefficients.\n\n### Estimating anytime-valid p-values and confidence sequences\n\nThe wonderful thing about these anytime-valid p-values and CSs is that they can\nbe easily estimated directly from standard regression outputs, e.g. from `lm()`\nin R. Below I will quickly demonstrate how we can adapt the code from above for\nestimating AMCES to output anytime-valid CSs and p-values instead of the usual\nfixed-n CIs and p-values.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n## Same code as before\n\n# Estimate AMCEs with a logistic regression model\nmodel <- feglm(\n  selected ~ Military + Religion + Education + Profession + Income + Race + Age\n    + Gender,\n  data = candidate,\n  family = \"binomial\",\n  cluster = ~ respondent\n)\n\n# Calculate the marginal effects (AMCEs) as probabilities\nmarginal_effects <- avg_slopes(model, newdata = \"mean\")\n\n## Calculate anytime-valid p-values and CSs\n\nmarginal_effects_sequential <- sequential_f_cs(\n  delta = marginal_effects$estimate,\n  se = marginal_effects$std.error,\n  n = model$nobs,\n  n_params = model$nparams,\n  Z = solve(get_vcov(marginal_effects)),\n  term = marginal_effects$term,\n  contrast = marginal_effects$contrast\n)\n```\n:::\n\n\n\nCheck the full code for calculating the anytime-valid values\n[here](https://github.com/dmolitor/dmolitor.github.io/blob/main/blog/posts/conjoint_analysis/utils.R).\n\nFrom now on, we will take these methods for granted and will simply show how to\napply them to our data so that we can continuously monitor our conjoint\nexperiment and stop it once our estimates of interest are statistically\nsignificant!\n\n### Estimating sequential AMCEs\n\n<!-- For all following code we will focus on a couple attributes of interest; military\nservice and religion. We will simulate a dynamic experiment where we are running\nthe conjoint and collecting data in real time. Using our anytime-valid linear\nregression estimates, we can estimate and plot our AMCEs and 95% confidence\nsequences (anytime-valid corollary to CIs) as often as we want while controlling\nthe probability of committing a Type 1 error.\n\nWe will iterate through our conjoint in batches of 70 survey responses\nand we will estimate our AMCES and confidence sequences (CSs) after each batch.\nWe will observe how our AMCE estimates change over time and how we can monitor statistical\nsignificance (and even choose to stop the conjoint early) based on this!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample a random row from a dataframe;\n# Return the row and the dataframe minus the row\nsample_rows <- function(data, size = 1) {\n  idx <- sample(1:nrow(data), size = size)\n  row <- data[idx, , drop = FALSE]\n  data <- data[-idx, , drop = FALSE]\n  list(\"rows\" = row, \"data\" = data)\n}\n\nchunk_size <- 99\ncandidate_sim <- candidate\nsim_data <- tibble()\nsim_estimates <- tibble()\n\nfor (i in seq(100, nrow(candidate_sim), by = chunk_size)) {\n  # Randomly sample a set of survey responses\n  sim_data <- bind_rows(\n    sim_data,\n    sample_rows(candidate_sim, size = chunk_size)$rows\n  )\n  # Estimate our model\n  sim_model <- feglm(\n    selected ~ Military + Religion + Education + Profession + Income + Race + Age\n        + Gender,\n    data = sim_data,\n    family = \"binomial\",\n    cluster = ~ respondent\n  )\n  # Calculate marginal effects\n  marginal_eff_sim <- avg_slopes(sim_model, newdata = \"mean\")\n  # Calculate sequential p-values and CSs\n  marginal_eff_sim_seq <- sequential_f_cs(\n    delta = marginal_eff_sim$estimate,\n    se = marginal_eff_sim$std.error,\n    n = sim_model$nobs,\n    n_params = sim_model$nparams,\n    Z = solve(get_vcov(marginal_eff_sim)),\n    term = marginal_eff_sim$term,\n    contrast = marginal_eff_sim$contrast\n  ) |>\n    add_reference(low = \"cs_lower\", high = \"cs_upper\") |>\n    filter(term %in% c(\"Military\", \"Profession\")) |>\n    mutate(iter = floor(i/chunk_size))\n  # Append data\n  sim_estimates <- bind_rows(sim_estimates, marginal_eff_sim_seq)\n}\n\n# Plot the CSs over time\nplot_anim <- ggplot(\n  sim_estimates,\n    aes(\n      x = estimate,\n      y = contrast,\n      xmin = cs_lower,\n      xmax = cs_upper,\n      color = term\n    )\n  ) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(xlim = c(-.5, .5)) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(), legend.position = \"none\") +\n  transition_states(\n    iter,\n    transition_length = 1,\n    state_length = 1\n  )\n\n# Turn this into a video\nanimation <- animate(\n  plot_anim,\n  renderer = av_renderer(),\n  height = 4,\n  width = 4,\n  units = \"in\",\n  res = 200\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n<div style=\"text-align: center;\">\n  <video controls style=\"width: 60%;\">\n    <source src=\"./anytime_valid_cs.mp4\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\nWe can watch through the duration of our simulated conjoint as the precision of\nour estimates increases and some of them are estimated as statistically significant.\n\n### Simulations -->\n\nFirst, let's chalk up a function that will simulate a random respondent for a\nconjoint experiment with specified AMCEs. Then we'll simulate a conjoint experiment\nwith a binary outcome (selected or not selected) and two attributes. Attribute 1 has\n3 levels and Attribute 2 has 5 levels. The true AMCEs for each attribute and \nattribute level are as follows:\n\n| Attribute | Level | True AMCE |\n|-----------|-------|-----------|\n| 1 | 2 | 0.2 |\n| 1 | 3 | 0.1 |\n| 2 | 2 | -0.1 |\n| 2 | 3 | 0.15 |\n| 2 | 4 | 0.3 |\n| 2 | 5 | -0.2 |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to simulate a random survey participant\nsimulate_profile <- function(amce_attr1, amce_attr2, intercept = 0.5) {\n  attr1_levels <- c(\"Level 1\", names(amce_attr1))\n  attr2_levels <- c(\"Level 1\", names(amce_attr2))\n  # Randomly sample a level for each attribute (uniformly)\n  attr1_val <- sample(attr1_levels, 1)\n  attr2_val <- sample(attr2_levels, 1)\n  # Determine the effect for attribute 1: baseline gets effect 0\n  effect1 <- if (attr1_val == \"Level 1\") 0 else amce_attr1[attr1_val]\n  # Determine the effect for attribute 2: baseline gets effect 0\n  effect2 <- if (attr2_val == \"Level 1\") 0 else amce_attr2[attr2_val]\n  # Compute the latent probability p\n  p <- intercept + effect1 + effect2\n  # Ensure that p is within [0, 1]\n  stopifnot(p >= 0 && p <= 1)\n  # Simulate the binary outcome using p as the success probability.\n  outcome <- rbinom(1, size = 1, prob = p)\n  return(tibble(\n    attr1 = attr1_val,\n    attr2 = attr2_val,\n    outcome = outcome,\n    p = p\n  ))\n}\n\n# Specify the attribute-level AMCEs for attribute 1 & 2\namce_attr1 <- c(\"Level 2\" = 0.2, \"Level 3\" = 0.1)\namce_attr2 <- c(\"Level 2\" = -0.1, \"Level 3\" = 0.15, \"Level 4\" = 0.3, \"Level 5\" = -0.2)\n```\n:::\n\n\n\nNext, let's simulate running our conjoint experiment online and estimating\nour AMCEs in an online fashion. We'll first simulate 100 survey participants\nand then we will estimate our AMCEs for every additional participant for a total sample of $N = 1000$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \"Gather\" 100 survey responses before estimating\nsim_data <- bind_rows(lapply(1:100, \\(i) simulate_profile(amce_attr1, amce_attr2, 0.3)))\n# Initialize tibble to collect our AMCE estimates\nsim_estimates <- tibble()\n\n# Simulate a sequential conjoint with total N=1000\npb <- txtProgressBar(min = 1, max = 1000, style = 3)\nfor (i in 1:1000) {\n  # Randomly simulate a survey participant\n  sim_data <- bind_rows(\n    sim_data,\n   simulate_profile(amce_attr1, amce_attr2, 0.3)\n  )\n  # Estimate our model\n  sim_model <- feglm(\n    outcome ~ attr1 + attr2,\n    data = sim_data,\n    family = \"binomial\"\n  )\n  # Calculate marginal effects\n  marginal_eff_sim <- avg_slopes(sim_model, newdata = \"mean\")\n  # Calculate sequential p-values and CSs\n  marginal_eff_sim_seq <- sequential_f_cs(\n    delta = marginal_eff_sim$estimate,\n    se = marginal_eff_sim$std.error,\n    n = sim_model$nobs,\n    n_params = sim_model$nparams,\n    Z = solve(get_vcov(marginal_eff_sim)),\n    term = marginal_eff_sim$term,\n    contrast = marginal_eff_sim$contrast\n  ) |>\n    add_reference(low = \"cs_lower\", high = \"cs_upper\") |>\n    mutate(iter = i)\n  # Append data\n  sim_estimates <- bind_rows(sim_estimates, marginal_eff_sim_seq)\n  setTxtProgressBar(pb, i)\n}\nclose(pb)\n```\n:::\n\n\n\nNow, let's plot the progression of the AMCE estimates over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntruth <- tribble(\n  ~term, ~contrast, ~estimate,\n  \"attr1\", \"Level 2\", 0.2,\n  \"attr1\", \"Level 3\", 0.1,\n  \"attr2\", \"Level 2\", -0.1,\n  \"attr2\", \"Level 3\", 0.15,\n  \"attr2\", \"Level 4\", 0.3,\n  \"attr2\", \"Level 5\", -0.2\n)\n\n# Plot the CSs over time\nplot_anim <- ggplot(\n  sim_estimates,\n    aes(\n      x = estimate,\n      y = contrast,\n      xmin = cs_lower,\n      xmax = cs_upper,\n      color = term\n    )\n  ) +\n  geom_point(\n    data = truth,\n    aes(x = estimate, y = contrast),\n    inherit.aes = FALSE,\n    shape = 17,\n    size = 2\n  ) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(xlim = c(-.5, .5)) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(), legend.position = \"none\") +\n  transition_states(\n    iter,\n    transition_length = 3,\n    state_length = 0\n  ) +\n  ease_aes()\n\n# Turn this into a video\nanimation <- animate(\n  plot_anim,\n  renderer = av_renderer(),\n  nframes = 300,\n  height = 4,\n  width = 4,\n  units = \"in\",\n  res = 200\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n<div style=\"text-align: center;\">\n  <video controls playsinline webkit-playsinline style=\"width: 60%;\">\n    <source src=\"./anytime_valid_sim.mp4\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\nWe can watch as our estimates and CSs converge towards the truth (diamond shaped points)\nover time, providing valid coverage despite estimating AMCEs and corresponding\np-values and CSs at each step!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}