{
  "hash": "378a2333dde25be17d391d3aebceaf26",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Anytime-valid Inference on the AMCE in Conjoint Experiments\"\ndescription: |\n  An empirical example based on <a href=\"https://doi.org/10.1093/pan/mpt024\">Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments by Hainmueller, Hopkins, and Yamamoto.</a>\ndate: \"3/6/2025\"\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-overflow: scroll\n    code-fold: true\n    code-summary: \"Code\"\n    fig-dpi: 300\nexecute:\n  freeze: true\npage-layout: article\ncategories: [Conjoints, Anytime-valid Inference]\nimage: \"./thumbnail.jpg\"\n---\n\n\n\nIn this post, I demonstrate how to estimate Average Marginal Component Effects (AMCE)\nin a conjoint experiment using both a fixed‑n approach and a sequential, anytime‑valid\napproach.\n\nFor background, see\n[this super-detailed discussion of conjoint analysis](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/)\nand [this guide on marginal effects](https://www.andrewheiss.com/blog/2022/05/20/marginalia/)\nby Andrew Heiss.\n\n## Candidate Experiment -- Setup\n\nOur analysis is based on the candidate experiment from Hainmueller et al.'s\n[Causal Inference in Conjoint Analysis](https://doi.org/10.1093/pan/mpt024).\nBelow is the paper’s description:\n\n> The choice between competing candidates for elected office is central to democracy. Candidates\ntypically differ on a variety of dimensions, including their personal background and demographic\ncharacteristics, issue positions, and prior experience. The centrality of partisanship to voter decision\nmaking is amply documented [ref], so\nwe focus here on the less-examined role of candidates’ personal traits [ref].\nWithin the United States, there is constant speculation about the role of candidates’ personal\nbackgrounds in generating support or opposition; here, we harness conjoint analysis to examine\nthose claims. <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We focus on eight attributes of would-be presidential candidates, all of which have emerged in\nrecent campaigns. Six of these attributes can take on one of six values, including the candidates’\nreligion (Catholic, Evangelical Protestant, Mainline Protestant, Mormon, Jewish, or None), college\neducation (no college, state university, community college, Baptist college, Ivy League college, or\nsmall college), profession (lawyer, high school teacher, business owner, farmer, doctor, or car\ndealer), annual income ($32K, $54K, $65K, $92K, $210K, and $5.1M), racial/ethnic background\n(Hispanic, White, Caucasian, Black, Asian American, and Native American), and age (36, 45, 52,\n60, 68, and 75). 3 Two other attributes take on only two values: military service (served or not) and\ngender (male or female). Each respondent to our online survey---administered through Amazon’s\nMechanical Turk [ref]---saw six pairs of profiles that were generated\nusing the fully randomized approach described below. Figure A.1 in the Supplemental Information\n(SI) illustrates one choice presented to one respondent. The profiles were presented side-by-side,\nwith each pair of profiles on a separate screen. To ease the cognitive burden for respondents while\nalso minimizing primacy and recency effects, the attributes were presented in a randomized order\nthat was fixed across the six pairings for each respondent.\n<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the same screen as each candidate pairing, respondents were asked multiple questions which\nserve as dependent variables. First, they were asked to choose between the two candidates, a\n“forced-choice” design that enables us to evaluate the role of each attribute value in the assessment\nof one profile relative to another. This question closely resembles real-world voter decision making,\nin which respondents must cast a single ballot between competing candidates who vary on multiple\ndimensions. In the second and third questions following the profiles, the respondents rated \neach candidate on a one to seven scale, enabling evaluations of the levels of absolute support or\nopposition to each profile separately.\n\n## Candidate Experiment -- Fixed-N Analysis\n\nThe data for the candidate experiment is publically available\n[here](https://www.andrewheiss.com/blog/2023/07/25/conjoint-bayesian-frequentist-guide/data/candidate.dta)\nor [here](https://doi.org/10.7910/DVN/THJYQR). Let's begin by loading the\nnecessary packages and the data:\n\n### Data Prep\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(fixest)\nlibrary(forcats)\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(haven)\nlibrary(marginaleffects)\nlibrary(scales)\nlibrary(tibble)\nlibrary(tidyr)\n\ncandidate <- read_dta(\"/Users/dmolitor/Downloads/candidate.dta\")\n\n# Make the variable names nice and readable\nvariable_lookup <- tribble(\n  ~variable,    ~variable_nice,\n  \"atmilitary\", \"Military\",\n  \"atreligion\", \"Religion\",\n  \"ated\",       \"Education\",\n  \"atprof\",     \"Profession\",\n  \"atmale\",     \"Gender\",\n  \"atinc\",      \"Income\",\n  \"atrace\",     \"Race\",\n  \"atage\",      \"Age\"\n)\n\ncandidate <- as_factor(candidate) |>\n  rename(respondent = resID) |>\n  rename_with(\n    .cols = atmilitary:atmale,\n    .fn = \\(x) vapply(\n      x,\n      \\(y) filter(variable_lookup, variable == y)$variable_nice,\n      character(1)\n    )\n  )\n\nglimpse(candidate)\n```\n:::\n\n\n\nEach row in the dataset represents one candidate profile out of 1,733 candidate comparisons made\nby 311 survey respondents. Each of the candidate comparisons has 2 candidate profiles to choose\nbetween, so the total number of profiles in the data is 3,466 (1,733 x 2). The `selected` column\nis a binary outcome indicating whether the survey respondent (identified in the `respondent` column)\nselected that profile as the preferred candidate.\n\nEach row represents one candidate profile from 1,733 comparisons (311 respondents × 2 profiles per comparison = 3,466 profiles).\nThe `selected` column indicates whether the respondent (identified in the `respondent` column) chose that profile.\n\n### Estimating AMCEs\n\nThe primary estimand of interest is the Average Marginal Component Effect (AMCE). \nThe AMCE measures the average change in the outcome when an attribute shifts from a\nbaseline level to an alternative, averaging over the joint distribution other attributes.\nIn potential outcomes notation:\n\n- $Y_i(X_j=x_j,X_{−j})$ be the potential outcome for respondent $i$ when attribute $j$ is set to $X_j=x_j$​ and other attributes $X_{-j}$​​ vary.\n- $X_j^*$ be the baseline (or reference) level for attribute $j$.\n\nThen,\n$$\\text{AMCE}(X_j)=E_{X_{−j}}[Y_i(X_j=x_j,X_{−j})−Y_i(X_j=X_j^∗, X_{−j})],$$\nwhere expectation $E_{X_{-j}}$​​ is taken over the joint distribution of the other attributes.\n\nWe estimate this using a regression model that includes all attributes. **Note**: Cluster standard errors at the respondent level since each respondent provides multiple observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to add the reference level for each of our attributes\nadd_reference <- function(data, low = \"conf.low\", high = \"conf.high\") {\n  data <- data |>\n    separate(col = \"contrast\", into = c(\"contrast\", \"reference\"), sep = \" - \")\n  data <- bind_rows(\n    data,\n    data |>\n      mutate(contrast = reference) |>\n      distinct(term, contrast, .keep_all = TRUE) |>\n      mutate(across(c(estimate, {{low}}, {{high}}), ~ 0))\n  )\n  return(data)\n}\n\n# Estimate AMCEs with a logistic regression model\nmodel <- feglm(\n  selected ~ Military + Religion + Education + Profession + Income + Race + Age\n    + Gender,\n  data = candidate,\n  family = \"binomial\",\n  cluster = ~ respondent\n)\n\n# Calculate the marginal effects (AMCEs) as probabilities\nmarginal_effects <- avg_slopes(model, newdata = \"mean\")\n\nggplot(\n  add_reference(marginal_effects),\n  aes(\n    x = estimate,\n    y = contrast,\n    xmin = conf.low,\n    xmax = conf.high,\n    color = term\n  )\n) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Fixed-n AMCE estimation-1.png){width=2100}\n:::\n:::\n\n\n\n### Estimating Marginal Means\n\nAnother estimand of interest is a purely descriptive quantity---marginal means.\nThe marginal mean for a particular value $x_j$​ of attribute $j$ is:\n$$\\mu(x_j) := E_{X_{-j}}[Y | X_j=x_j],$$\nwhere $Y$ are the observed outcomes, $X_j$​ is the attribute of interest, and\nthe expectation $E$ is taken over the joint distribution of all other attributes $X_{-j}$​.\n\nWe could estimate this non-parametrically (using `group_by()` and `summarize()`)\nor via regression which will give us corresponding CIs, p-values, etc. in one fell\nswoop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A function to calculate the marginal mean for a single attribute\nmarginal_mean <- function(model, attribute) {\n  mms <- avg_predictions(model, \"balanced\", by = attribute)\n  mms <- mms |>\n    rename(\"level\" = {{attribute}}) |>\n    mutate(\"term\" = {{attribute}})\n  return(mms)\n}\n\nmarginal_means <- bind_rows(\n  lapply(\n    c(\n      \"Military\",\n      \"Religion\",\n      \"Education\",\n      \"Profession\",\n      \"Income\",\n      \"Race\",\n      \"Age\",\n      \"Gender\"\n    ),\n    function(attribute) marginal_mean(model, attribute)\n  )\n)\n\nggplot(\n  marginal_means,\n  aes(\n    x = estimate,\n    y = level,\n    xmin = conf.low,\n    xmax = conf.high,\n    color = term\n  )\n) +\n  geom_vline(xintercept = 0.5, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(x = \"Marginal Means\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n:::\n\n\n\n## Candidate Experiment -- Sequential Analysis\n\nFixed‑$N$ estimators provide valid inference at a single predetermined sample size $N$\n(e.g., 95% confidence intervals are valid only at single $N$). Typically,\nresearchers will utilize power calculations to determine sufficient sample sizes\nto detect treatment effects of interest prior to running an experiment.\nHowever, power calculations rely on unverifiable assumptions which can cause them to\nbe inaccurate not to mention difficult to perform, particularly in the context of conjoint experiments.\nInstead of relying on power calculations, researchers often would like to monitor \nstatistical results as data accumulate and stop once their estimates become\nstatistically significant. However, if we repeatedly calculate\nour AMCE estimates and fixed-$N$ 95% CIs and collect more data until they are\nstatistically significant, our probability of committing a Type 1 error\nwill far exceed the nominal significance level ($\\alpha = 0.05$).\n\nAnytime‑valid inference offers a solution by yielding p‑values and\nconfidence sequences (CSs) that remain valid even if we \"peek\" at the data\nand stop the experiment once our estimates are statistically significant.\nIn [this blog post](https://www.dmolitor.com/blog/posts/anytime_valid_linear_models/)\nI explore methods by Lindon et al. that develop anytime-valid p-values and CSs for\nlinear regression models. These methods adapt standard regression outputs\n(e.g., from lm() in R) to produce anytime‑valid p‑values and CSs.\n\n### Estimating Anytime-Valid p-values and Confidence Sequences\n\nBelow I will demonstrate how we can adapt our code for\nestimating AMCES to output anytime-valid CSs and p-values instead of the usual\nfixed-n CIs and p-values.  Check the full code for calculating anytime-valid CSs\n[here](https://github.com/dmolitor/dmolitor.github.io/blob/main/blog/posts/conjoint_analysis/utils.R).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n## Same code as before\n\n# Estimate AMCEs with a logistic regression model\nmodel <- feglm(\n  selected ~ Military + Religion + Education + Profession + Income + Race + Age\n    + Gender,\n  data = candidate,\n  family = \"binomial\",\n  cluster = ~ respondent\n)\n\n# Calculate the marginal effects (AMCEs) as probabilities\nmarginal_effects <- avg_slopes(model, newdata = \"mean\")\n\n## Calculate anytime-valid p-values and CSs\n\nmarginal_effects_sequential <- sequential_f_cs(\n  delta = marginal_effects$estimate,\n  se = marginal_effects$std.error,\n  n = model$nobs,\n  n_params = model$nparams,\n  Z = solve(get_vcov(marginal_effects)),\n  term = marginal_effects$term,\n  contrast = marginal_effects$contrast\n)\n```\n:::\n\n\n\n### Estimating sequential AMCEs\n\nWe begin by writing a function that will simulate a random respondent from a conjoint experiment with specified AMCEs.\nOur simulated conjoint experiment will have a binary outcome and two attributes (Attribute 1 with 3 levels, Attribute 2 with 5 levels). The true AMCEs are:\n\n| Attribute | Level | True AMCE |\n|-----------|-------|-----------|\n| 1 | 2 | 0.2 |\n| 1 | 3 | 0.1 |\n| 2 | 2 | -0.1 |\n| 2 | 3 | 0.15 |\n| 2 | 4 | 0.3 |\n| 2 | 5 | -0.2 |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to simulate a random survey participant\nsimulate_profile <- function(amce_attr1, amce_attr2, intercept = 0.5) {\n  attr1_levels <- c(\"Level 1\", names(amce_attr1))\n  attr2_levels <- c(\"Level 1\", names(amce_attr2))\n  # Randomly sample a level for each attribute (uniformly)\n  attr1_val <- sample(attr1_levels, 1)\n  attr2_val <- sample(attr2_levels, 1)\n  # Determine the effect for attribute 1: baseline gets effect 0\n  effect1 <- if (attr1_val == \"Level 1\") 0 else amce_attr1[attr1_val]\n  # Determine the effect for attribute 2: baseline gets effect 0\n  effect2 <- if (attr2_val == \"Level 1\") 0 else amce_attr2[attr2_val]\n  # Compute the latent probability p\n  p <- intercept + effect1 + effect2\n  # Ensure that p is within [0, 1]\n  stopifnot(p >= 0 && p <= 1)\n  # Simulate the binary outcome using p as the success probability.\n  outcome <- rbinom(1, size = 1, prob = p)\n  return(tibble(\n    attr1 = attr1_val,\n    attr2 = attr2_val,\n    outcome = outcome,\n    p = p\n  ))\n}\n\n# Specify the attribute-level AMCEs for attribute 1 & 2\namce_attr1 <- c(\"Level 2\" = 0.2, \"Level 3\" = 0.1)\namce_attr2 <- c(\"Level 2\" = -0.1, \"Level 3\" = 0.15, \"Level 4\" = 0.3, \"Level 5\" = -0.2)\n```\n:::\n\n\n\nWe simulate 100 survey participants initially and update our AMCE estimates and\nanytime-valid 95% CSs with each additional participant until reaching $N=1000$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \"Gather\" 100 survey responses before estimating\nsim_data <- bind_rows(lapply(1:100, \\(i) simulate_profile(amce_attr1, amce_attr2, 0.3)))\n# Initialize tibble to collect our AMCE estimates\nsim_estimates <- tibble()\n\n# Simulate a sequential conjoint with total N=1000\npb <- txtProgressBar(min = 1, max = 1000, style = 3)\nfor (i in 1:1000) {\n  # Randomly simulate a survey participant\n  sim_data <- bind_rows(\n    sim_data,\n   simulate_profile(amce_attr1, amce_attr2, 0.3)\n  )\n  # Estimate our model\n  sim_model <- feglm(\n    outcome ~ attr1 + attr2,\n    data = sim_data,\n    family = \"binomial\"\n  )\n  # Calculate marginal effects\n  marginal_eff_sim <- avg_slopes(sim_model, newdata = \"mean\")\n  # Calculate sequential p-values and CSs\n  marginal_eff_sim_seq <- sequential_f_cs(\n    delta = marginal_eff_sim$estimate,\n    se = marginal_eff_sim$std.error,\n    n = sim_model$nobs,\n    n_params = sim_model$nparams,\n    Z = solve(get_vcov(marginal_eff_sim)),\n    term = marginal_eff_sim$term,\n    contrast = marginal_eff_sim$contrast\n  ) |>\n    add_reference(low = \"cs_lower\", high = \"cs_upper\") |>\n    mutate(iter = i)\n  # Append data\n  sim_estimates <- bind_rows(sim_estimates, marginal_eff_sim_seq)\n  setTxtProgressBar(pb, i)\n}\nclose(pb)\n```\n:::\n\n\n\nFinally, we plot the progression of the AMCE estimates and CSs across the entire\nexperiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntruth <- tribble(\n  ~term, ~contrast, ~estimate,\n  \"attr1\", \"Level 2\", 0.2,\n  \"attr1\", \"Level 3\", 0.1,\n  \"attr2\", \"Level 2\", -0.1,\n  \"attr2\", \"Level 3\", 0.15,\n  \"attr2\", \"Level 4\", 0.3,\n  \"attr2\", \"Level 5\", -0.2\n)\n\n# Plot the CSs over time\nplot_anim <- ggplot(\n  sim_estimates,\n    aes(\n      x = estimate,\n      y = contrast,\n      xmin = cs_lower,\n      xmax = cs_upper,\n      color = term\n    )\n  ) +\n  geom_point(\n    data = truth,\n    aes(x = estimate, y = contrast),\n    inherit.aes = FALSE,\n    shape = 17,\n    size = 2\n  ) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(xlim = c(-.5, .5)) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  transition_states(\n    iter,\n    transition_length = 3,\n    state_length = 0\n  ) +\n  ease_aes()\n\n# Turn this into a video\nanimation_mp4 <- animate(\n  plot_anim,\n  renderer = av_renderer(),\n  nframes = 300,\n  height = 4,\n  width = 4,\n  units = \"in\",\n  res = 200\n)\nanimation_gif <- animate(\n  plot_anim,\n  renderer = gifski_renderer(),\n  nframes = 300,\n  height = 4,\n  width = 4,\n  units = \"in\",\n  res = 200\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n<video controls playsinline style=\"width: 60%;\">\n  <source src=\"./anytime_valid_sim.mp4\" type=\"video/mp4\">\n  <source src=\"./anytime_valid_sim.webm\" type=\"video/webm\">\n  Your browser does not support the video tag.\n</video>\n\nWe see our estimates and CSs converge towards the true AMCEs (marked by diamond-shaped points)\nover time, maintaining valid coverage despite estimating AMCEs and corresponding\np-values and CSs at each step!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}