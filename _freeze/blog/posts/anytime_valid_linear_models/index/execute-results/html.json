{
  "hash": "ff36347821fc200a2f7a7baecd6a3adf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Anytime-Valid Regression Adjusted Causal Inference\"\ndescription: |\n  Investigating the methods described in <a href=\"https://arxiv.org/abs/2210.08589\">Anytime-Valid Linear Models and Regression Adjusted Causal Inference in Randomized Experiments\n  by Lindon, et al.</a> via simulations.\ndate: \"2/21/2025\"\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-overflow: scroll\n    fig-width: 7\n    fig-height: 4\n    fig-dpi: 300\nexecute: \n  freeze: true\npage-layout: full\ncategories: [Anytime-valid Inference]\nimage: \"./thumbnail.jpg\"\n---\n\n\n\n\nRandomized experiments (A/B tests) are ubiquitous in both academia and the tech sector. They enable robust causal inference regarding the impact of interventions on user or participant outcomes. Under randomization, various estimators provide valid estimates of the average treatment effect (ATE), defined as $\\tau = E[Y_i(1) - Y_i(0)].$ These estimators typically yield both point estimates and confidence intervals (CIs) such that $$P(\\tau \\in \\text{CI}) <= \\alpha,$$ where $\\alpha$ is the significance level and $\\text{CI}$ is a random interval constructed from our data.\n\nOne major issue with common estimators like ordinary least squares (OLS) is that the statistical guarantees for their confidence intervals only hold for a pre-specified sample size $N$. A surprisingly common practice is to collect some data, estimate $\\tau$ and its corresponding 95% CI, and then, if the effect is large but not statistically significant, collect more data and re-estimate $\\tau$. This process of “data-peeking” may be repeated multiple times. However, such practices violate the statistical guarantees of standard confidence intervals, causing the probability of a Type I error to far exceed the nominal significance level $\\alpha$.\n\nTo address this issue, researchers at Netflix have developed anytime-valid Confidence Sequences (CSs) for linear regression parameters. These CSs have the property that $$P(\\forall n, \\delta \\in \\text{CS}_{\\delta, n}) \\geq 1 - \\alpha,$$ where $\\delta$ represents the regression parameter. Essentially, this means that you can estimate the parameter and update the CS as often as you like, and the probability of committing a Type I error will always remain below $\\alpha$.\n\nThis method is not only practically significant but also easy to implement using standard linear regression outputs in R. The simulations that follow will demonstrate the usefulness of this approach.\n\n## Simulate RCT\n\nThese simulations will rely on the following packages:\n\n### Dependencies\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlibrary(dplyr)\nlibrary(ggplot2)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### Data generation and helper functions\n\nWe simulate a simple randomized controlled trial (RCT) where binary treatment is assigned randomly. The potential outcomes are defined by\n$$Y_i(W_i) = 0.5 + 0.5*W_i + 2*X_{i1} + 1.2*X_{i2} + 0.4*X_{i3} + \\epsilon_i$$\n\nwhere $W_i$​ indicates the treatment status of unit $i$, the true average treatment effect (ATE) is $\\tau = 0.5$, and $\\epsilon_i \\sim \\text{Normal}(0, 1)$.\n\nIn this setting, a simple regression of $Y_i$​ on an intercept and $W_i$​, i.e. $Y_i \\sim \\alpha + \\tau * W_i$​, is equivalent to the difference-in-means estimator and produces an unbiased estimate $\\hat{\\tau}_{DM}$ of $\\tau$. Similarly, a covariate-adjusted regression, given by\n$Y_i \\sim \\alpha + \\tau * W_i + \\beta^T*X_i$,\nalso provides an unbiased estimate $\\hat{\\tau}_{CA}$, but with lower variance than $\\hat{\\tau}_{DM}$.\n\nFirst, we define a function to simulate a random tuple $(X_i, W_i, Y_i)$ according to the RCT setup described above:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ndraw <- function(arm, ate, sigma = 1.0) {\n  covar_coef <- c(2, 1.2, 0.4)\n  covar <- rbinom(3, size = 1, prob = c(0.1, 0.5, 0.9))\n  y <- 0.5 + ate*arm + drop(covar_coef %*% covar) + rnorm(1, 0, sigma)\n  cbind(data.frame(\"y\" = y, \"t\" = arm), data.frame(t(covar)))\n}\n```\n:::\n\n\n\nNext, for a dataset comprising these units $\\{(X_i, W_i, Y_i)\\}_{i=1}^N$​, we construct a function to estimate both $\\hat{\\tau}_{DM}$ and\n$\\hat{\\tau}_{CA}$​, along with their corresponding exact and asymptotic confidence sequences (CSs):\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncompare <- function(model_ca, model_dm, data, iter, ate = 0.5) {\n  seq_f <- sequential_f_cs(model_ca, phi = 10)\n  seq_f_dm <- sequential_f_cs(model_dm, phi = 10)\n  seq_asymp <- ate_cs_asymp(model_ca, treat_name = \"t\", lambda = 100)\n  seq_asymp_dm <- ate_cs_asymp(\n    model_dm,\n    lambda = 100,\n    treat_name = \"t\"\n  )\n  comparison_df <- data.frame(\n    \"i\" = iter,\n    \"method\" = c(\n      \"f_test\",\n      \"f_test_unadj\",\n      \"asymp\",\n      \"asymp_unadj\"\n    ),\n    \"estimate\" = c(\n      subset(seq_f, covariate == \"t\")$estimate,\n      subset(seq_f_dm, covariate == \"t\")$estimate,\n      seq_asymp$estimate,\n      seq_asymp_dm$estimate\n    ),\n    \"lower\" = c(\n      subset(seq_f, covariate == \"t\")$cs_lower,\n      subset(seq_f_dm, covariate == \"t\")$cs_lower,\n      seq_asymp$cs_lower,\n      seq_asymp_dm$cs_lower\n    ),\n    \"upper\" = c(\n      subset(seq_f, covariate == \"t\")$cs_upper,\n      subset(seq_f_dm, covariate == \"t\")$cs_upper,\n      seq_asymp$cs_upper,\n      seq_asymp_dm$cs_upper\n    )\n  )\n  comparison_df$covered <- (\n    comparison_df$lower <= ate & ate <= comparison_df$upper\n  )\n  return(comparison_df)\n}\n```\n:::\n\n\n\nFinally, we create a function that simulates an experiment of size $N$ where each tuple $(X_i, W_i, Y_i)$ is received sequentially. At each sample size $n \\leq N$, we re-estimate $\\hat{\\tau}_{DM}$, $\\hat{\\tau}_{CA}$​, and their CSs: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nsimulate <- function(model_ca_fn, model_dm_fn, draw_fn, n, ate) {\n  # Warm-start with 20 observations so that no regression coefs are NA\n  # at any point\n  df <- do.call(rbind, lapply(1:20, function(x) draw_fn(ate)))\n  estimates <- data.frame()\n  for (i in 1:n) {\n    observation <- draw_fn(ate)\n    df <- rbind(df, observation)\n    model <- model_ca_fn(df)\n    model_dm <- model_dm_fn(df)\n    estimates <- rbind(\n      estimates,\n      compare(model, model_dm, df, iter = i, ate = ate)\n    )\n  }\n  estimates <- estimates |> \n    mutate(\n      stat_sig = 0 < lower | 0 > upper,\n      method = case_when(\n        method == \"asymp\" ~ \"Sequential asymptotic CS\",\n        method == \"asymp_unadj\" ~ \"Sequential asymptotic CS w/o covariates\",\n        method == \"f_test\" ~ \"Sequential CS\",\n        method == \"f_test_unadj\" ~ \"Sequential CS w/o covariates\",\n        method == \"lm\" ~ \"Fixed-N CS\",\n        method == \"lm_unadj\" ~ \"Fixed-N CS w/o covariates\"\n      )\n    ) |>\n    group_by(method) |>\n    mutate(transition = (!lag(stat_sig, default = FALSE)) & stat_sig) |>\n    mutate(\n      stat_sig_i_min = if_else(\n        any(stat_sig & !lag(stat_sig, default = FALSE)),\n        min(i[stat_sig & !lag(stat_sig, default = FALSE)]),\n        NA_integer_\n      ),\n      stat_sig_i_max = if_else(\n        any(transition),\n        max(i[transition]),\n        NA_integer_\n      )  \n    ) |>\n    ungroup()\n  return(estimates)\n}\n```\n:::\n\n\n\n### Run the simulation\n\nWe then run our simulation for $N = 500$ and plot the estimates $\\hat{\\tau}_{DM}$ and\n$\\hat{\\tau}_{CA}$​ along with their CSs at each step. The red dotted line is the true value of $\\tau$ and the vertical blue line indicates the sample size when the corresponding estimator becomes statistically significant.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nestimates <- simulate(\n  model_ca_fn = function(data) lm(y ~ ., data),\n  model_dm_fn = function(data) lm(y ~ t, data),\n  draw_fn = function(ate) draw(rbinom(1, 1, 0.5), ate = ate),\n  n = 500,\n  ate = 0.5\n)\n\nggplot(estimates, aes(x = i, y = estimate, ymin = lower, ymax = upper)) +\n  geom_line(linewidth = 0.2) +\n  geom_ribbon(alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_vline(aes(xintercept = stat_sig_i_max), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(\n    ~ method,\n    ncol = 2,\n    scales = \"free\",\n  ) +\n  coord_cartesian(ylim = c(-0.5, 2)) +\n  labs(y = \"ATE\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=2100}\n:::\n:::\n\n\n\n## Simulate non-fixed-probability randomized data (but known propensity scores)\n\nNext, we simulate a randomized experiment where propensity scores are generated as a known function of covariates. Specifically, the propensity score for unit $i$ is given by $$e_i = 0.05 + 0.5*Z_{i1} + 0.3*Z_{i2} + 0.1*Z_{i3},$$​ where $\\{Z_{i1}, Z_{i2}, Z_{i3}\\}$ are binary covariates. The potential outcomes are defined as $$Y_i(W_i) = 0.5 + 0.5*W_i + 3*X_{i1} + 3*X_{i2} + 1*X_{i3} + 0.5*Z_{i1} + 0.3*Z_{i2} + 0.1*Z_{i3} + \\epsilon_i,$$ with $W_i$​ representing the treatment status, the true average treatment effect (ATE) $\\tau = 0.5$, and $\\epsilon_i \\sim \\text{Normal}(0, 1)$. In this setup, a simple regression of $Y_i$​ on an intercept and $W_i$, weighted by the inverse propensity scores $\\frac{1}{e_i}$, yields an unbiased estimate $\\hat{\\tau}_{IPW}$​ of $\\tau$. Similarly, the covariate-adjusted regression $Y_i \\sim \\alpha + \\tau * W_i + \\beta^T*X_i$, when weighted by the inverse propensity scores, produces an unbiased estimate $\\hat{\\tau}_{CAIPW}$ that typically has lower variance.\n\nWe then create a function to generate data based on this experimental setup and run our simulation for $N = 2000$. As before, we plot $\\hat{\\tau}_{IPW}$,\n$\\hat{\\tau}_{CAIPW}$ and their corresponding confidence sequences (CSs) at each step.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ndraw_prop <- function(ate = 2.0, sigma = 1.0) {\n  prop_covar_coef <- c(0.5, 0.3, 0.1)\n  prop_covar <- rbinom(3, size = 1, prob = c(0.1, 0.4, 0.7))\n  prop <- drop(prop_covar_coef %*% prop_covar) + 0.05\n  covar_coef <- c(3, 2, 1)\n  covar <- rbinom(3, size = 1, prob = c(0.25, 0.5, 0.75))\n  arm <- rbinom(1, 1, prop)\n  y <- (\n    0.5\n    + ate*arm\n    + drop(covar_coef %*% covar)\n    + drop(prop_covar_coef %*% prop_covar)\n    + rnorm(1, 0, sigma)\n  )\n  cbind(data.frame(\"y\" = y, \"t\" = arm, \"p\" = ifelse(arm, prop, 1 - prop)), data.frame(t(covar)))\n}\n\n# Simulation estimates\nestimates <- simulate(\n  function(data) lm(y ~ . - p, data, weights = 1/data$p),\n  function(data) lm(y ~ t, data, weights = 1/data$p),\n  draw_fn = function(ate) draw_prop(ate = ate),\n  n = 2000,\n  ate = 0.5\n)\n\nggplot(\n  estimates,\n  aes(x = i, y = estimate, ymin = lower, ymax = upper)\n) +\n  geom_line(linewidth = 0.2) +\n  geom_ribbon(alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_vline(aes(xintercept = stat_sig_i_max), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(\n    ~ method,\n    ncol = 2,\n    scales = \"free\",\n  ) +\n  coord_cartesian(ylim = c(-0.5, 2)) +\n  labs(y = \"ATE\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=2100}\n:::\n:::\n\n\n\nAs expected, including covariates significantly improves the precision of our ATE estimates.\n\n## Type 1 error control\n\nFinally, we assess the empirical Type I error control by running multiple simulations under the null hypothesis (i.e., with $\\tau = 0$). For each simulation, we record whether a Type 1 error is ever committed; that is, whether the CS ever fails to cover $\\tau$. The overall empirical Type I error is then the fraction of simulations in which the CS commits a Type I error. This global guarantee is analogous to the property of a standard CI: before data is observed, we know that the probability of a Type I error is $\\leq \\alpha$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nestimates <- lapply(\n  1:100,\n  function(index) {\n    simulate(\n      model_ca_fn = function(data) lm(y ~ ., data),\n      model_dm_fn = function(data) lm(y ~ t, data),\n      draw_fn = function(ate) draw(rbinom(1, 1, 0.5), ate = ate),\n      n = 1000,\n      ate = 0\n    ) |>\n    mutate(sim_i = index)\n  }\n)\n\nerror_rate <- estimates |>\n  bind_rows() |>\n  group_by(sim_i, method) |>\n  summarize(any_error = !all(covered)) |>\n  ungroup() |>\n  group_by(Method = method) |>\n  summarize(`Type 1 error` = mean(any_error))\n\nerror_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  Method                                  `Type 1 error`\n  <chr>                                            <dbl>\n1 Sequential CS                                     0.04\n2 Sequential CS w/o covariates                      0.04\n3 Sequential asymptotic CS                          0.04\n4 Sequential asymptotic CS w/o covariates           0.03\n```\n\n\n:::\n:::\n\n\n\nWe see that, indeed, Type 1 error is $\\leq \\alpha = 0.05$ for all estimators.\n\n### Credits\n\nA lot of the code for this was based on the paper's [official repository](https://github.com/michaellindon/avlm) and was crafted in tandem with my trusty AI assistant (ChatGPT).",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}