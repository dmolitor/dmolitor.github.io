[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Molitor",
    "section": "",
    "text": "Hello there! I‚Äôm a second-year PhD student at Cornell‚Äôs School of Information Science. My research interests center primarily on policy-relevant applications of adaptive/online experimentation and causal inference methods. I love open source software and enjoy building tools and learning new technologies whenever I can. I‚Äôm very thankful to have my work supported by an NSF Graduate Research Fellowship!\n\n\n\nNews\n\n\n\n\n\n\n\nMay 2024\nReceived Outstanding PhD TA Award for Studying Social Inequality with Data Science taught by Ian Lundberg.\n\n\nMay 2024\nJointly presented our work on Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination at ACIC 2024.\n\n\nApril 2023\nAwarded an NSF Graduate Research Fellowship."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html",
    "href": "blog/posts/dynamic_surveys/index.html",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "",
    "text": "Understanding discriminatory human choices are of central interest across the social sciences. Typically when studying such questions, researchers employ standard designs such as experimental audit studies or conjoint analyses. Recent advances in the adaptive experimentation literature have explored how multi-arm bandit (MAB) algorithms can be used to answer the same questions with lower cost and greater data efficiency while also mitigating ethical concerns that may arise in some randomized experiments (e.g.¬†assigning participants to harmful treatment arms)1 2.\nAlthough MAB methods can provide significant improvements over standard experimental methods, implementing adaptive experiments or surveys can pose a challenge. There are many survey platforms at the researcher‚Äôs disposal such as Qualtrics, Google Forms, etc. that can quickly accomodate standard survey designs, but these platforms do not easily support the design of adaptive surveys. Without such tools at their disposal, the researcher is stuck needing to design their own custom solution. This is the exact situation that my research team and I ran into a few months ago.\nIt began with a straightforward enough question. We would like to know, for example, how American adults in the U.S. discriminate on the basis of education when choosing which immigrants to prioritize for immigrant visas?3 Our goal was to explore how we could adopt methods from the adaptive experimentation literature to answer these questions more efficiently than standard methods.\nTo this end, we framed the question as a stochastic multi-arm bandit problem. Each arm of the bandit was defined as one set of immigrant characteristics and the outcome of interest (reward) was whether the survey respondent chose to prioritize the immigrant with higher education, given that set of characteristics. We wanted to understand under which set of characteristics American adults are the most likely to discriminate against an immigrant who has lower education.\nTo uncover the set of characteristics with the most discrimination, we employed a classic algorithm from the adaptive literature in Thompson Sampling (TS)4. TS is a dynamic algorithm. It starts out by assuming that the probability of discrimination is the same across all sets of immigrant characteristics. Every time a new survey respondent takes the survey, it assigns them to the set of characteristics that has the highest probability of resulting in a discriminatory response. TS then observes whether or not that respondent discriminates, and it updates the probability of discrimination for the set of characteristics which they were assigned to. As the algorithm learns which sets of characteristics are most likely to elicit discriminatory responses, the algorithm progressively assigns more respondents to those arms and stops assigning respondents to characteristics that fail to elicit a discriminatory response."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#background",
    "href": "blog/posts/dynamic_surveys/index.html#background",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "",
    "text": "Understanding discriminatory human choices are of central interest across the social sciences. Typically when studying such questions, researchers employ standard designs such as experimental audit studies or conjoint analyses. Recent advances in the adaptive experimentation literature have explored how multi-arm bandit (MAB) algorithms can be used to answer the same questions with lower cost and greater data efficiency while also mitigating ethical concerns that may arise in some randomized experiments (e.g.¬†assigning participants to harmful treatment arms)1 2.\nAlthough MAB methods can provide significant improvements over standard experimental methods, implementing adaptive experiments or surveys can pose a challenge. There are many survey platforms at the researcher‚Äôs disposal such as Qualtrics, Google Forms, etc. that can quickly accomodate standard survey designs, but these platforms do not easily support the design of adaptive surveys. Without such tools at their disposal, the researcher is stuck needing to design their own custom solution. This is the exact situation that my research team and I ran into a few months ago.\nIt began with a straightforward enough question. We would like to know, for example, how American adults in the U.S. discriminate on the basis of education when choosing which immigrants to prioritize for immigrant visas?3 Our goal was to explore how we could adopt methods from the adaptive experimentation literature to answer these questions more efficiently than standard methods.\nTo this end, we framed the question as a stochastic multi-arm bandit problem. Each arm of the bandit was defined as one set of immigrant characteristics and the outcome of interest (reward) was whether the survey respondent chose to prioritize the immigrant with higher education, given that set of characteristics. We wanted to understand under which set of characteristics American adults are the most likely to discriminate against an immigrant who has lower education.\nTo uncover the set of characteristics with the most discrimination, we employed a classic algorithm from the adaptive literature in Thompson Sampling (TS)4. TS is a dynamic algorithm. It starts out by assuming that the probability of discrimination is the same across all sets of immigrant characteristics. Every time a new survey respondent takes the survey, it assigns them to the set of characteristics that has the highest probability of resulting in a discriminatory response. TS then observes whether or not that respondent discriminates, and it updates the probability of discrimination for the set of characteristics which they were assigned to. As the algorithm learns which sets of characteristics are most likely to elicit discriminatory responses, the algorithm progressively assigns more respondents to those arms and stops assigning respondents to characteristics that fail to elicit a discriminatory response."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#necessity-of-a-custom-survey-form",
    "href": "blog/posts/dynamic_surveys/index.html#necessity-of-a-custom-survey-form",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Necessity of a custom survey form",
    "text": "Necessity of a custom survey form\nGiven the dynamic nature of TS, we needed a survey form that would allow us to estimate a variety of parameters and update historical data every time a new user connected to our form or submitted a survey response. At first we were bullish on Qualtrics for meeting our needs. In fact, the Qualtrics API surfaces endpoints that allow the researcher to deploy certain actions every time a user submits a survey response. Unfortunately, we quickly discovered that this functionality is only available to users with special access. When using an account under an institutional subscription (which is the case at Cornell and probably most universities), you don‚Äôt have this special access and so this was a non-starter for us. It also seemed undesirable to be downloading thousands of user responses and manually updating algorithmic parameters.\nNot to be deterred, I confidently announced that it would be no problem to build such a survey with Shiny üò¨. It turned out to be harder than I initially imagined, but it was indeed possible!"
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#architecture",
    "href": "blog/posts/dynamic_surveys/index.html#architecture",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Architecture",
    "text": "Architecture\nAs I began sketching out the codebase for our survey, I split the structure into three main services:\n\nDatabase: We needed some form of database to store algorithmic parameters and user responses throughout the duration of the survey. For our application we opted for PostgreSQL, though pretty much any database solution would have worked.\nAPI: The API was built with FastAPI and was the workhorse of the application, handling all interactions between the survey form and the database. When a new user would connect to our application, the API would retrieve the historical data from the database, perform an iteration of the TS algorithm, and update the survey form with necessary information such as which bandit arm the user would be assigned to. When the user had finished and submitted the survey, the API would update the corresponding tables and parameters in the database in preparation for new survey respondents.\nFrontend: The frontend was built with Shiny and was the actual survey form. This survey form was not in charge any computational steps, but instead collected all user response data and orchestrated the communication between the API and the database.\n\nAfter creating a working survey application, the next step was to deploy this survey so that it could actually be used by real survey respondents."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#deployment",
    "href": "blog/posts/dynamic_surveys/index.html#deployment",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Deployment",
    "text": "Deployment\n\nContainerization\nI began by taking each of the three services described above and putting it in its own Docker container. With our services containerized, we could easily deploy our application on any cloud services that support Docker.\n\n\nAWS\nOur cloud provider of choice is AWS, so the next step was to build a simple custom AMI based on Ubuntu that had Docker installed. With our AMI in hand, the final piece of the puzzle was to scale our survey appropriately. There are many tools that could have served our purposes including Kubernetes, AWS Fargate, AWS ECS/EKS, and Docker Swarm. For our purposes, I opted to go with Docker Swarm as this struck a balance between serving our scaling needs while not becoming overly complex.\n\n\nDocker Swarm mode\nFor our survey, we recruited participants from Prolific and budgeted for a maximum of 10,000 participants. From past Prolific surveys, we expected to see ~1,000 respondents per hour with anywhere between 30-50 concurrent users at all times. To ensure that our survey could easily handle any realistic level of traffic, I deployed our containerized services on a Docker Swarm cluster comprised of one manager AWS instance and ~60 worker AWS instances. All instances were equipped with the custom AMI described above.\nAt this point, our survey was online with plenty of compute resources available to handle a large number of survey respondents.\n\n\n\n\n\n\nEffectively, Docker acts as a load balancer for your swarm services so there‚Äôs no need to worry about setting up a load balancer yourself!"
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#summary",
    "href": "blog/posts/dynamic_surveys/index.html#summary",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Summary",
    "text": "Summary\nTLDR; we built our custom survey form and served it to ~10,000 survey respondents in 10 hours with the following steps.\n\nDeveloped the survey form with Shiny and added necessary scaffolding (API, database).\nContainerized these services to make deployment as easy as possible.\nDeployed services on cloud provider (AWS) and scaled the services as necessary with Docker Swarm."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#implementation-tips",
    "href": "blog/posts/dynamic_surveys/index.html#implementation-tips",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Implementation tips",
    "text": "Implementation tips\nThe following are a bunch of very specific tips based mostly on things that bit me when building this, or things that would make it better that I just never got around to adding.\n\nShiny\n\nThis is probably really obvious, but especially when you need your Shiny app to be somewhat performant, try to streamline time-consuming calculations as much as possible. For example, in our app, I structured any time-consuming steps so that they would happen either at run-time or after the user had clicked ‚ÄúSubmit‚Äù on the survey form.\nThe R Shiny software is much more mature than its Python counterpart and as a result the Python API may not surface all features that the R API does. For example the R Shiny API has a well defined way to access client data being sent to the server. To access the current URL you would do something like:\nserver &lt;- function(input, output, session) {\n  # Return the components of the URL in a string:\n  output$urlText &lt;- renderText({\n    paste(\n      sep = \"\",\n      \"protocol: \", session$clientData$url_protocol, \"\\n\",\n      \"hostname: \", session$clientData$url_hostname, \"\\n\",\n      \"pathname: \", session$clientData$url_pathname, \"\\n\",\n      \"port: \", session$clientData$url_port, \"\\n\",\n      \"search: \", session$clientData$url_search, \"\\n\"\n    )\n  })\n}\nThis feature has yet to be officially implemented in Py Shiny, as noted in this GitHub issue but can be worked around as described in this issue. The workaround solution would look something like:\ndef server(input, output, session):\n    @render.text\n    def urlText():\n        url_text = (\n          f\"protocol: {session.input['.clientdata_url_protocol']()}\\n\",\n          + f\"hostname: {session.input['.clientdata_url_hostname']()}\\n\",\n          + f\"pathname: {session.input['.clientdata_url_pathname']()}\\n\",\n          + f\"port: {session.input['.clientdata_url_port']()}\\n\",\n          + f\"search: {session.input['.clientdata_url_search']()}\\n\"\n        )\n        return url_text\n\n\n\nAWS and Docker Swarm mode\n\nAWS network rules: Ports, ports, ports. You need to make sure that all instances in your swarm have Security Group(s) attached with the necessary inbound/outbound rules defined. When running Docker Swarm the following inbound rules are absolutely essential otherwise Swarm mode will not work:\n\nTCP port 2377 for cluster management communications.\nTCP and UDP port 7946 for communication among nodes.\nUDP port 4789 for overlay network traffic.\n\nIn addition, make sure you add any rules for other ports that are specific to your application. In our case our app was exposed on port 8000 so I needed to add an additional inbound rule for TCP port 8000.\nAdding worker nodes to swarm: When adding a worker instance to the swarm on AWS, it is essential to include the --advertise-addr argument. For example:\ndocker swarm join --token SWMTKN-1-49nj1abc... manager.node.ip:2377 --advertise-addr worker.node.ip\nConfiguring HTTPS: When you deploy your Shiny application on an AWS compute instance, e.g.¬†on port 8000, the application will be available at a url looking something like http://manager.node.ip:8000. While this works fine, it will look unusual to the average user and may be flagged by some browsers as insecure and result in warning messages being sent to the user. If it is important to have HTTPS configured for your application, there are a couple ways to approach this. Both require having a domain or sub-domain name available.\n\nOnce you have launched your application (or swarm) on AWS, configure a DNS record on your domain to forward your sub-domain to the public IP address of the server where your application is hosted. This process may vary slightly depending on where you purchased your domain name (e.g.¬†Bluehost, Namecheap).\nInstall and configure nginx to forward traffic from port 80 to whatever port your application is running on.\nConfigure nginx with SSL/TLS certificates using Let‚Äôs Encrypt.\n\nThere are many good online tutorials on exactly how to do steps 2 and 3. The main shortcoming with the method described above is that you would have to do all three steps separately every time you re-deploy your services on AWS. If you only deploy once, this may not be an issue. But if you think you might terminate and re-deploy your application multiple times, it may get tiresome. One way around this is to allocate an AWS Elastic IP address to your account and then create a DNS record on your domain pointing to the elastic IP per step 1 above. Then, every time you launch your application on a new AWS compute instance you can associate the elastic IP address with your instance, and you don‚Äôt need to re-do step 1. You will still have to do steps 2 and 3, but you can do these steps programmatically. Step 1 is by far the most time consuming and you will only have to do that once.\nPersisting data with AWS volumes: When running your application on AWS, or any cloud provider, there is always the concern that your compute resources might get terminated without any warning. As such, it is essential that all your data be backed up so that it will persist regardless of whether your application terminates or not. For our application, we created an AWS volume and mounted that volume to the local filesystem of the compute instance where our database container was running. We then used a bind mount to mount that directory on the host machine into the PostgreSQL Docker container.\nScaling the application: Py Shiny is built on uvicorn. As a direct result, a user can deploy a Shiny application by simply running the following on the command line:\nuvicorn app:app --host 0.0.0.0 --port 8000\n\n\n\n\n\n\nThe first app references the file app.py where your application is defined, and the second app references the final line in your app.py file that should look something like:\napp = App(app_ui, server)\n\n\n\nUvicorn has scaling built-in via the --workers argument. If you wanted to scale your application in a super simple way and avoid all the hassle above, it‚Äôs as easy as deploying your application on a very large AWS server and running it with something like:\nuvicorn app:app --host 0.0.0.0 --port 8000 --workers 20\nFor several reasons this approach didn‚Äôt work for our situation, but it may be a reasonable approach for many people. To see more about self-hosted deployment, see the Shiny docs or the uvicorn docs.\n\n\n\nGeneral\nIn building and deploying our application there were a bunch of small, almost unnoticeable steps that go into each larger step. For example, when I wanted to deploy our survey onto AWS, there were several preliminary steps:\n\nBuild and push the Docker images for all three services to DockerHub.\nCreate all AWS resources (e.g.¬†security group, volume, etc.)\nSpin up the Docker swarm and deploy our application.\n\nWhen you‚Äôre actively developing a project it‚Äôs easy to remember all the pre-requisite steps that go into each larger step. However, it‚Äôs really easy to quickly forget these things and to come back weeks or months later and struggle to build or deploy your application. So do your future self a favor and use a build tool like just! Not only does this remove a lot of key-strokes when you‚Äôre developing an application, but it codifies all the easy-to-forget steps for your future self."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#code",
    "href": "blog/posts/dynamic_surveys/index.html#code",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Code",
    "text": "Code\nTo browse the code that corresponds to each part of this post, check out the GitHub repo here and feel free to reach out with any questions or drop an issue on the repo!"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Daniel Molitor",
    "section": "",
    "text": "The Causal Effect of Parent Occupation on Child Occupation: A Multivalued Treatment with Positivity Constraints. Ian Lundberg, Jennie Brand, Daniel Molitor. Conditionally accepted, Sociological Methods and Research.\n\n\nAbstract\n\n\nContemporary social mobility research often adopts an ostensibly descriptive goal: to document associations between parent and child socioeconomic outcomes and their variation over time and place. To complement descriptive research, we adopt a causal goal: to estimate the degree to which parent occupation causes child occupation. We formalize this causal goal in the potential outcomes framework to define precise counterfactuals. We highlight a difficulty connected to the positivity assumption for causal inference: when the treatment is parent occupation, many counterfactuals never happen in observed data. Parents without college degrees are never employed as physicians, for instance. We show how to select causal estimands involving only the counterfactuals that can be studied with data. We demonstrate our approach using the National Longitudinal Survey of Youth 1979. Our causal approach points to open questions about how specific aspects of family background, such as parent occupation, causally shape the life chances of children.\n\n\nEstimating Value-added Returns to Labor Training Programs with Causal Machine Learning. Mintaka Angell, et al.¬†OSF Pre-prints.\n\n\nAbstract\n\n\nThe mismatch between the skills that employers seek and the skills that workers possess will increase substantially as demand for technically skilled workers accelerates. Skill mismatches disproportionately affect low-income workers and those within industries where relative demand growth for technical skills is strongest. As a result, much emphasis is placed on reskilling workers to ease transitions into new careers. However, utilization of training programs may be sub-optimal if workers are uncertain about the returns to their investment in training. While the U.S. spends billions of dollars annually on reskilling programs and unemployment insurance, there are few measures of program effectiveness that workers or government can use to guide training investment and ensure valuable reskilling outcomes. We demonstrate a causal machine learning method for estimating the value-added returns to training programs in Rhode Island, where enrollment increases future quarterly earnings by $605 on average, ranging from -$1,570 to $3,470 for individual programs. In a nationwide survey (N=2,014), workers prefer information on the value-added returns to earnings following training enrollment, establishing the importance of our estimates for guiding training decisions. For every 10% increase in expected earnings, workers are 17.4% more likely to express interest in training. State and local governments can provide this preferred information on value-added returns using our method and existing administrative data."
  },
  {
    "objectID": "research/index.html#pre-prints",
    "href": "research/index.html#pre-prints",
    "title": "Daniel Molitor",
    "section": "",
    "text": "The Causal Effect of Parent Occupation on Child Occupation: A Multivalued Treatment with Positivity Constraints. Ian Lundberg, Jennie Brand, Daniel Molitor. Conditionally accepted, Sociological Methods and Research.\n\n\nAbstract\n\n\nContemporary social mobility research often adopts an ostensibly descriptive goal: to document associations between parent and child socioeconomic outcomes and their variation over time and place. To complement descriptive research, we adopt a causal goal: to estimate the degree to which parent occupation causes child occupation. We formalize this causal goal in the potential outcomes framework to define precise counterfactuals. We highlight a difficulty connected to the positivity assumption for causal inference: when the treatment is parent occupation, many counterfactuals never happen in observed data. Parents without college degrees are never employed as physicians, for instance. We show how to select causal estimands involving only the counterfactuals that can be studied with data. We demonstrate our approach using the National Longitudinal Survey of Youth 1979. Our causal approach points to open questions about how specific aspects of family background, such as parent occupation, causally shape the life chances of children.\n\n\nEstimating Value-added Returns to Labor Training Programs with Causal Machine Learning. Mintaka Angell, et al.¬†OSF Pre-prints.\n\n\nAbstract\n\n\nThe mismatch between the skills that employers seek and the skills that workers possess will increase substantially as demand for technically skilled workers accelerates. Skill mismatches disproportionately affect low-income workers and those within industries where relative demand growth for technical skills is strongest. As a result, much emphasis is placed on reskilling workers to ease transitions into new careers. However, utilization of training programs may be sub-optimal if workers are uncertain about the returns to their investment in training. While the U.S. spends billions of dollars annually on reskilling programs and unemployment insurance, there are few measures of program effectiveness that workers or government can use to guide training investment and ensure valuable reskilling outcomes. We demonstrate a causal machine learning method for estimating the value-added returns to training programs in Rhode Island, where enrollment increases future quarterly earnings by $605 on average, ranging from -$1,570 to $3,470 for individual programs. In a nationwide survey (N=2,014), workers prefer information on the value-added returns to earnings following training enrollment, establishing the importance of our estimates for guiding training decisions. For every 10% increase in expected earnings, workers are 17.4% more likely to express interest in training. State and local governments can provide this preferred information on value-added returns using our method and existing administrative data."
  },
  {
    "objectID": "research/index.html#publications",
    "href": "research/index.html#publications",
    "title": "Daniel Molitor",
    "section": "Publications",
    "text": "Publications\nDelivering Unemployment Assistance in Times of Crisis: Scalable Cloud Solutions Can Keep Essential Government Programs Running and Supporting Those in Need. Mintaka Angell, et al.¬†(2020). Digital Government: Research and Practice.\n\n\nAbstract\n\n\nThe COVID-19 public health emergency caused widespread economic shutdown and unemployment. The resulting surge in Unemployment Insurance claims threatened to overwhelm the legacy systems state workforce agencies rely on to collect, process, and pay claims. In Rhode Island, we developed a scalable cloud solution to collect Pandemic Unemployment Assistance claims as part of a new program created under the Coronavirus Aid, Relief and Economic Security Act to extend unemployment benefits to independent contractors and gig-economy workers not covered by traditional Unemployment Insurance. Our new system was developed, tested, and deployed within 10 days following the passage of the Coronavirus Aid, Relief and Economic Security Act, making Rhode Island the first state in the nation to collect, validate, and pay Pandemic Unemployment Assistance claims. A cloud-enhanced interactive voice response system was deployed a week later to handle the corresponding surge in weekly certifications for continuing unemployment benefits. Cloud solutions can augment legacy systems by offloading processes that are more efficiently handled in modern scalable systems, reserving the limited resources of legacy systems for what they were originally designed. This agile use of combined technologies allowed Rhode Island to deliver timely Pandemic Unemployment Assistance benefits with an estimated cost savings of $502,000 (representing a 411% return on investment)."
  },
  {
    "objectID": "research/index.html#works-in-progress",
    "href": "research/index.html#works-in-progress",
    "title": "Daniel Molitor",
    "section": "Works in Progress",
    "text": "Works in Progress\nData-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination. Jennah Gosciak, Daniel Molitor, Ian Lundberg."
  }
]