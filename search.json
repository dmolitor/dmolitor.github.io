[
  {
    "objectID": "blog/posts/dynamic_surveys/index.html",
    "href": "blog/posts/dynamic_surveys/index.html",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "",
    "text": "Understanding discriminatory human choices are of central interest across the social sciences. Typically when studying such questions, researchers employ standard designs such as experimental audit studies or conjoint analyses. Recent advances in the adaptive experimentation literature have explored how multi-armed bandit (MAB) algorithms can be used to answer the same questions with lower cost and greater data efficiency while also mitigating ethical concerns that may arise in some randomized experiments (e.g. assigning participants to harmful treatment arms)1 2.\nAlthough MAB methods can provide significant improvements over standard experimental methods, implementing adaptive experiments or surveys can pose a challenge. There are many survey platforms at the researcher’s disposal such as Qualtrics, Google Forms, etc. that can quickly accomodate standard survey designs, but these platforms do not easily support the design of adaptive surveys. Without such tools at their disposal, the researcher is stuck needing to design their own custom solution. This is the exact situation that my research team and I ran into a few months ago.\nIt began with a straightforward enough question. We would like to know, for example, how American adults in the U.S. discriminate on the basis of education when choosing which immigrants to prioritize for immigrant visas?3 Our goal was to explore how we could adopt methods from the adaptive experimentation literature to answer these questions more efficiently than standard methods.\nTo this end, we framed the question as a stochastic MAB problem. Each arm of the bandit was defined as one set of immigrant characteristics and the outcome of interest (reward) was whether the survey respondent chose to prioritize the immigrant with higher education, given that set of characteristics. We wanted to understand under which set of characteristics American adults are the most likely to discriminate against an immigrant who has lower education.\nTo uncover the set of characteristics with the most discrimination, we employed a classic algorithm from the adaptive literature in Thompson Sampling (TS)4. TS is a dynamic algorithm. It starts out by assuming that the probability of discrimination is the same across all sets of immigrant characteristics. Every time a new survey respondent takes the survey, it assigns them to the set of characteristics that has the highest probability of resulting in a discriminatory response. TS then observes whether or not that respondent discriminates, and it updates the probability of discrimination for the set of characteristics which they were assigned to. As the algorithm learns which sets of characteristics are most likely to elicit discriminatory responses, the algorithm progressively assigns more respondents to those arms and stops assigning respondents to characteristics that fail to elicit a discriminatory response."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#background",
    "href": "blog/posts/dynamic_surveys/index.html#background",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "",
    "text": "Understanding discriminatory human choices are of central interest across the social sciences. Typically when studying such questions, researchers employ standard designs such as experimental audit studies or conjoint analyses. Recent advances in the adaptive experimentation literature have explored how multi-armed bandit (MAB) algorithms can be used to answer the same questions with lower cost and greater data efficiency while also mitigating ethical concerns that may arise in some randomized experiments (e.g. assigning participants to harmful treatment arms)1 2.\nAlthough MAB methods can provide significant improvements over standard experimental methods, implementing adaptive experiments or surveys can pose a challenge. There are many survey platforms at the researcher’s disposal such as Qualtrics, Google Forms, etc. that can quickly accomodate standard survey designs, but these platforms do not easily support the design of adaptive surveys. Without such tools at their disposal, the researcher is stuck needing to design their own custom solution. This is the exact situation that my research team and I ran into a few months ago.\nIt began with a straightforward enough question. We would like to know, for example, how American adults in the U.S. discriminate on the basis of education when choosing which immigrants to prioritize for immigrant visas?3 Our goal was to explore how we could adopt methods from the adaptive experimentation literature to answer these questions more efficiently than standard methods.\nTo this end, we framed the question as a stochastic MAB problem. Each arm of the bandit was defined as one set of immigrant characteristics and the outcome of interest (reward) was whether the survey respondent chose to prioritize the immigrant with higher education, given that set of characteristics. We wanted to understand under which set of characteristics American adults are the most likely to discriminate against an immigrant who has lower education.\nTo uncover the set of characteristics with the most discrimination, we employed a classic algorithm from the adaptive literature in Thompson Sampling (TS)4. TS is a dynamic algorithm. It starts out by assuming that the probability of discrimination is the same across all sets of immigrant characteristics. Every time a new survey respondent takes the survey, it assigns them to the set of characteristics that has the highest probability of resulting in a discriminatory response. TS then observes whether or not that respondent discriminates, and it updates the probability of discrimination for the set of characteristics which they were assigned to. As the algorithm learns which sets of characteristics are most likely to elicit discriminatory responses, the algorithm progressively assigns more respondents to those arms and stops assigning respondents to characteristics that fail to elicit a discriminatory response."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#necessity-of-a-custom-survey-form",
    "href": "blog/posts/dynamic_surveys/index.html#necessity-of-a-custom-survey-form",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Necessity of a custom survey form",
    "text": "Necessity of a custom survey form\nGiven the dynamic nature of TS, we needed a survey form that would allow us to estimate a variety of parameters and update historical data every time a new user connected to our form or submitted a survey response. At first we were bullish on Qualtrics for meeting our needs. In fact, the Qualtrics API surfaces endpoints that allow the researcher to deploy certain actions every time a user submits a survey response. Unfortunately, we quickly discovered that this functionality is only available to users with special access. When using an account under an institutional subscription (which is the case at Cornell and probably most universities), you don’t have this special access and so this was a non-starter for us. It also seemed undesirable to be downloading thousands of user responses and manually updating algorithmic parameters.\nNot to be deterred, I confidently announced that it would be no problem to build such a survey with Shiny 😬. It turned out to be harder than I initially imagined, but it was indeed possible!"
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#architecture",
    "href": "blog/posts/dynamic_surveys/index.html#architecture",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Architecture",
    "text": "Architecture\nAs I began sketching out the codebase for our survey, I split the structure into three main services:\n\nDatabase: We needed some form of database to store algorithmic parameters and user responses throughout the duration of the survey. For our application we opted for PostgreSQL, though pretty much any database solution would have worked.\nAPI: The API was built with FastAPI and was the workhorse of the application, handling all interactions between the survey form and the database. When a new user would connect to our application, the API would retrieve the historical data from the database, perform an iteration of the TS algorithm, and update the survey form with necessary information such as which bandit arm the user would be assigned to. When the user had finished and submitted the survey, the API would update the corresponding tables and parameters in the database in preparation for new survey respondents.\nFrontend: The frontend was built with Shiny and was the actual survey form. This survey form was not in charge any computational steps, but instead collected all user response data and orchestrated the communication between the API and the database.\n\nAfter creating a working survey application, the next step was to deploy this survey so that it could actually be used by real survey respondents."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#deployment",
    "href": "blog/posts/dynamic_surveys/index.html#deployment",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Deployment",
    "text": "Deployment\n\nContainerization\nI began by taking each of the three services described above and putting it in its own Docker container. With our services containerized, we could easily deploy our application on any cloud services that support Docker.\n\n\nAWS\nOur cloud provider of choice is AWS, so the next step was to build a simple custom AMI based on Ubuntu that had Docker installed. With our AMI in hand, the final piece of the puzzle was to scale our survey appropriately. There are many tools that could have served our purposes including Kubernetes, AWS Fargate, AWS ECS/EKS, and Docker Swarm. For our purposes, I opted to go with Docker Swarm as this struck a balance between serving our scaling needs while not becoming overly complex.\n\n\nDocker Swarm mode\nFor our survey, we recruited participants from Prolific and budgeted for a maximum of 10,000 participants. From past Prolific surveys, we expected to see ~1,000 respondents per hour with anywhere between 30-50 concurrent users at all times. To ensure that our survey could easily handle any realistic level of traffic, I deployed our containerized services on a Docker Swarm cluster comprised of one manager AWS instance and ~60 worker AWS instances. All instances were equipped with the custom AMI described above.\nAt this point, our survey was online with plenty of compute resources available to handle a large number of survey respondents.\n\n\n\n\n\n\nEffectively, Docker acts as a load balancer for your swarm services so there’s no need to worry about setting up a load balancer yourself!"
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#summary",
    "href": "blog/posts/dynamic_surveys/index.html#summary",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Summary",
    "text": "Summary\nTLDR; we built our custom survey form and served it to ~10,000 survey respondents in 10 hours with the following steps.\n\nDeveloped the survey form with Shiny and added necessary scaffolding (API, database).\nContainerized these services to make deployment as easy as possible.\nDeployed services on cloud provider (AWS) and scaled the services as necessary with Docker Swarm."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#implementation-tips",
    "href": "blog/posts/dynamic_surveys/index.html#implementation-tips",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Implementation tips",
    "text": "Implementation tips\nThe following are a bunch of very specific tips based mostly on things that bit me when building this, or things that would make it better that I just never got around to adding.\n\nShiny\n\nThis is probably really obvious, but especially when you need your Shiny app to be somewhat performant, try to streamline time-consuming calculations as much as possible. For example, in our app, I structured any time-consuming steps so that they would happen either at run-time or after the user had clicked “Submit” on the survey form.\nThe R Shiny software is much more mature than its Python counterpart and as a result the Python API may not surface all features that the R API does. For example the R Shiny API has a well defined way to access client data being sent to the server. To access the current URL you would do something like:\nserver &lt;- function(input, output, session) {\n  # Return the components of the URL in a string:\n  output$urlText &lt;- renderText({\n    paste(\n      sep = \"\",\n      \"protocol: \", session$clientData$url_protocol, \"\\n\",\n      \"hostname: \", session$clientData$url_hostname, \"\\n\",\n      \"pathname: \", session$clientData$url_pathname, \"\\n\",\n      \"port: \", session$clientData$url_port, \"\\n\",\n      \"search: \", session$clientData$url_search, \"\\n\"\n    )\n  })\n}\nThis feature has yet to be officially implemented in Py Shiny, as noted in this GitHub issue but can be worked around as described in this issue. The workaround solution would look something like:\ndef server(input, output, session):\n    @render.text\n    def urlText():\n        url_text = (\n          f\"protocol: {session.input['.clientdata_url_protocol']()}\\n\",\n          + f\"hostname: {session.input['.clientdata_url_hostname']()}\\n\",\n          + f\"pathname: {session.input['.clientdata_url_pathname']()}\\n\",\n          + f\"port: {session.input['.clientdata_url_port']()}\\n\",\n          + f\"search: {session.input['.clientdata_url_search']()}\\n\"\n        )\n        return url_text\n\n\n\nAWS and Docker Swarm mode\n\nAWS network rules: Ports, ports, ports. You need to make sure that all instances in your swarm have Security Group(s) attached with the necessary inbound/outbound rules defined. When running Docker Swarm the following inbound rules are absolutely essential otherwise Swarm mode will not work:\n\nTCP port 2377 for cluster management communications.\nTCP and UDP port 7946 for communication among nodes.\nUDP port 4789 for overlay network traffic.\n\nIn addition, make sure you add any rules for other ports that are specific to your application. In our case our app was exposed on port 8000 so I needed to add an additional inbound rule for TCP port 8000.\nAdding worker nodes to swarm: When adding a worker instance to the swarm on AWS, it is essential to include the --advertise-addr argument. For example:\ndocker swarm join --token SWMTKN-1-49nj1abc... manager.node.ip:2377 --advertise-addr worker.node.ip\nConfiguring HTTPS: When you deploy your Shiny application on an AWS compute instance, e.g. on port 8000, the application will be available at a url looking something like http://manager.node.ip:8000. While this works fine, it will look unusual to the average user and may be flagged by some browsers as insecure and result in warning messages being sent to the user. If it is important to have HTTPS configured for your application, there are a couple ways to approach this. Both require having a domain or sub-domain name available.\n\nOnce you have launched your application (or swarm) on AWS, configure a DNS record on your domain to forward your sub-domain to the public IP address of the server where your application is hosted. This process may vary slightly depending on where you purchased your domain name (e.g. Bluehost, Namecheap).\nInstall and configure nginx to forward traffic from port 80 to whatever port your application is running on.\nConfigure nginx with SSL/TLS certificates using Let’s Encrypt.\n\nThere are many good online tutorials on exactly how to do steps 2 and 3. The main shortcoming with the method described above is that you would have to do all three steps separately every time you re-deploy your services on AWS. If you only deploy once, this may not be an issue. But if you think you might terminate and re-deploy your application multiple times, it may get tiresome. One way around this is to allocate an AWS Elastic IP address to your account and then create a DNS record on your domain pointing to the elastic IP per step 1 above. Then, every time you launch your application on a new AWS compute instance you can associate the elastic IP address with your instance, and you don’t need to re-do step 1. You will still have to do steps 2 and 3, but you can do these steps programmatically. Step 1 is by far the most time consuming and you will only have to do that once.\nPersisting data with AWS volumes: When running your application on AWS, or any cloud provider, there is always the concern that your compute resources might get terminated without any warning. As such, it is essential that all your data be backed up so that it will persist regardless of whether your application terminates or not. For our application, we created an AWS volume and mounted that volume to the local filesystem of the compute instance where our database container was running. We then used a bind mount to mount that directory on the host machine into the PostgreSQL Docker container.\nScaling the application: Py Shiny is built on uvicorn. As a direct result, a user can deploy a Shiny application by simply running the following on the command line:\nuvicorn app:app --host 0.0.0.0 --port 8000\n\n\n\n\n\n\nThe first app references the file app.py where your application is defined, and the second app references the final line in your app.py file that should look something like:\napp = App(app_ui, server)\n\n\n\nUvicorn has scaling built-in via the --workers argument. If you wanted to scale your application in a super simple way and avoid all the hassle above, it’s as easy as deploying your application on a very large AWS server and running it with something like:\nuvicorn app:app --host 0.0.0.0 --port 8000 --workers 20\nFor several reasons this approach didn’t work for our situation, but it may be a reasonable approach for many people. To see more about self-hosted deployment, see the Shiny docs or the uvicorn docs.\n\n\n\nGeneral\nIn building and deploying our application there were a bunch of small, almost unnoticeable steps that go into each larger step. For example, when I wanted to deploy our survey onto AWS, there were several preliminary steps:\n\nBuild and push the Docker images for all three services to DockerHub.\nCreate all AWS resources (e.g. security group, volume, etc.)\nSpin up the Docker swarm and deploy our application.\n\nWhen you’re actively developing a project it’s easy to remember all the pre-requisite steps that go into each larger step. However, it’s really easy to quickly forget these things and to come back weeks or months later and struggle to build or deploy your application. So do your future self a favor and use a build tool like just! Not only does this remove a lot of key-strokes when you’re developing an application, but it codifies all the easy-to-forget steps for your future self."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#code",
    "href": "blog/posts/dynamic_surveys/index.html#code",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Code",
    "text": "Code\nTo browse the code that corresponds to each part of this post, check out the GitHub repo here and feel free to reach out with any questions or drop an issue on the repo!"
  },
  {
    "objectID": "blog/posts/conformal-beyond-exchangeability/index.html",
    "href": "blog/posts/conformal-beyond-exchangeability/index.html",
    "title": "Conformal Prediction Beyond Exchangeability",
    "section": "",
    "text": "Dependencies\n\nfrom great_tables import GT, md\nimport numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nfrom pathlib import Path\nimport plotnine as pn\nimport statsmodels.api as sm\nfrom tqdm import tqdm\nfrom typing import Any, Callable, List\n\nbase_dir = Path().cwd()\ngenerator = np.random.default_rng()\n\n\n\nData\n\n# Electricity data\nelectricity = pd.read_csv(base_dir / \"data\" / \"electricity-normalized.csv\")\nelectricity = (\n    electricity\n    .iloc[17760:]\n    .assign(period=lambda x: x.period*24)\n    .loc[lambda df: (df[\"period\"] &gt;= 9) & (df[\"period\"] &lt;= 12)]\n    [[\"transfer\", \"nswprice\", \"vicprice\", \"nswdemand\", \"vicdemand\"]]\n    .reset_index(drop=True)\n)\npermuted = electricity.sample(frac=1).reset_index(drop=True)\n\n# Function to generate simulated data\ndef sim_data(N: int, d: int, setting: int) -&gt; tuple[pd.DataFrame, npt.NDArray]:\n    X = np.random.multivariate_normal(mean=np.zeros(d), cov=np.eye(d), size=N)\n    if setting == 1:\n        beta = np.array([2, 1, 0, 0])\n        y = X @ beta + np.random.normal(0, 1, N)\n        X = pd.DataFrame(X, columns=[f\"feature_{i+1}\" for i in range(d)])\n    elif setting == 2:\n        beta_1 = np.array([2, 1, 0, 0])\n        beta_2 = np.array([0, -2, -1, 0])\n        beta_3 = np.array([0, 0, 2, 1])\n        y = np.zeros(N)\n        # Generate y for different segments\n        y[:500] = X[:500] @ beta_1 + np.random.normal(0, 1, 500)\n        y[500:1500] = X[500:1500] @ beta_2 + np.random.normal(0, 1, 1000)\n        y[1500:] = X[1500:] @ beta_3 + np.random.normal(0, 1, 500)\n        X = pd.DataFrame(X, columns=[f\"feature_{i+1}\" for i in range(d)])\n    else:\n        beta_start = np.array([2, 1, 0, 0])\n        beta_end = np.array([0, 0, 2, 1])\n        beta = np.linspace(beta_start, beta_end, N)\n        y = np.array([X[i] @ beta[i] + np.random.normal(0, 1) for i in range(N)])\n        X = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(d)])\n    return (X, y)\n\n\n\nFunctions\nThe nexcp_split function implements non-exchangeable split conformal prediction (CP). However, we can force it to also implement standard CP, which assumes exchangeability, by setting uniform weights. So we only need one function to replicate the results!\n\ndef normalize_weights(weights: npt.NDArray):\n    return weights / weights.sum()\n\n\ndef nexcp_split(\n    model: Callable[[npt.NDArray, pd.DataFrame, npt.NDArray], Any],\n    split_function: Callable[[int], npt.NDArray],\n    y: npt.NDArray,\n    X: pd.DataFrame,\n    tag_function: Callable[[int], npt.NDArray],\n    weight_function: Callable[[int], npt.NDArray],\n    alpha: float,\n    test_index: int\n):\n    \"\"\"Implements non-exchangeable split conformal prediction\"\"\"\n    \n    # Pull test observation from data\n    y_test = y[test_index]\n    X_test = X.iloc[[test_index]]\n    # Select all observations up to that point\n    y = y[:test_index]\n    X = X.iloc[:test_index]\n    # Generate indices for train/calibration split\n    split_indices = split_function(test_index)\n    # Split data, tags, and weights\n    X_train = X.iloc[split_indices]\n    y_train = y[split_indices]\n    X_calib = X.drop(split_indices)\n    y_calib = np.delete(y, split_indices)\n    # Generate tags and weights\n    tags = tag_function(test_index)\n    weights = weight_function(test_index)\n    # Train model\n    model_base = model(y_train, X_train, weights=tags[split_indices])\n    model_fitted = model_base.fit()\n    # Generate residuals\n    residuals = np.abs(y_calib - model_fitted.predict(X_calib))\n    # Calculate weighted quantile of residuals\n    weights_calib = normalize_weights(np.delete(weights[:test_index], split_indices))\n    q_hat = np.quantile(\n        residuals,\n        1 - alpha,\n        weights=weights_calib,\n        method=\"inverted_cdf\"\n    )\n    # Calculate predicted value\n    y_hat = model_fitted.predict(X_test).iloc[0]\n    # Generate CI\n    lb = y_hat - q_hat\n    ub = y_hat + q_hat\n    covered = lb &lt;= y_test &lt;= ub\n    return {\"ci\": np.array([lb, y_hat, ub]), \"covered\": covered, \"width\": ub-lb}\n\n\ndef plot_rolling_coverage(\n    results: List[dict],\n    alpha: float = 0.1,\n    window: int = 300,\n    rows: int = 2,\n    repeated: bool = False\n):\n    \"\"\"Plot the algorithm's mean coverage over a sliding window\"\"\"\n\n    coverage_df = pd.DataFrame(results)\n    if repeated:\n        coverage_df = (\n            coverage_df\n            .groupby([\"method\", \"dataset\", \"index\"])[\"covered\"]\n            .mean()\n            .reset_index()\n        )\n    coverage_df[\"coverage_mean\"] = (\n        coverage_df\n        .groupby([\"method\", \"dataset\"])[\"covered\"]\n        .transform(lambda x: x.rolling(window=window).mean())\n    )\n    coverage_df[\"time\"] = coverage_df.groupby([\"method\", \"dataset\"]).cumcount() + 1\n    coverage_df = coverage_df.dropna(subset=[\"coverage_mean\"])\n    coverage_plot = (\n        pn.ggplot(\n            coverage_df,\n            pn.aes(x=\"time\", y=\"coverage_mean\", color=\"method\", group=\"method\")\n        )\n        + pn.geom_line()\n        + pn.geom_hline(yintercept=1-alpha, linetype=\"solid\")\n        + pn.scale_y_continuous(limits=(0, 1))\n        + pn.facet_wrap(\"~ dataset\", nrow=rows, scales=\"free\")\n        + pn.theme_538()\n        + pn.labs(x=\"Time\", y=\"Coverage\", color=\"Method\")\n    )\n    return coverage_plot\n\n\ndef plot_rolling_width(\n    results: dict,\n    window: int = 300,\n    rows: int = 2,\n    repeated: bool = False\n):\n    \"\"\"Plot the algorithm's mean prediction interval width over a sliding window\"\"\"\n\n    width_df = pd.DataFrame(results)\n    if repeated:\n        width_df = (\n            width_df\n            .groupby([\"method\", \"dataset\", \"index\"])[\"width\"]\n            .mean()\n            .reset_index()\n        )\n    width_df[\"width_mean\"] = (\n        width_df\n        .groupby([\"method\", \"dataset\"])[\"width\"]\n        .transform(lambda x: x.rolling(window=window).mean())\n    )\n    width_df[\"time\"] = width_df.groupby([\"method\", \"dataset\"]).cumcount() + 1\n    width_df = width_df.dropna(subset=[\"width_mean\"])\n    width_plot = (\n        pn.ggplot(\n            width_df,\n            pn.aes(x=\"time\", y=\"width_mean\", color=\"method\", group=\"method\")\n        )\n        + pn.geom_line()\n        + pn.facet_wrap(\"~ dataset\", nrow=rows, scales=\"free\")\n        + pn.theme_538()\n        + pn.labs(x=\"Time\", y=\"Width\", color=\"Method\")\n    )\n    return width_plot\n\n\n\nElectricity example\nNote: I implement non-exchangeable split CP with least-squares by using WLS and setting all the tags to a uniform value. Standard CP is implemented by setting all the weights to a uniform value as mentioned above.\n\nsplit_fn = lambda x: np.sort(generator.choice(x, int(np.floor(x*0.3)), replace=False))\nresults = []\n\n# Create X and y for the normal and permuted data\nX, y = (electricity.drop(\"transfer\", axis=1), electricity[\"transfer\"].to_numpy())\nX_perm, y_perm = (permuted.drop(\"transfer\", axis=1), permuted[\"transfer\"].to_numpy())\n\n# Predict for each observation from N=100 to N=len(electricity)\nfor i in tqdm(range(100, len(electricity)), total=len(electricity)-100):\n    for method in [\"NexCP+LS\", \"NexCP+WLS\", \"CP+LS\"]:\n        for dataset in [\"Electricity\", \"Permuted\"]:\n            if dataset == \"Electricity\":\n                X_model, y_model = (X, y)\n            else:\n                X_model, y_model = (X_perm, y_perm)\n            if method == \"NexCP+LS\":\n                tag_fn = lambda x: np.array([1.]*(x + 1))\n                weight_fn = lambda x: 0.99**np.arange(x, -1, -1)\n            elif method == \"NexCP+WLS\":\n                tag_fn = lambda x: 0.99**np.arange(x, -1, -1)\n                weight_fn = tag_fn\n            else:\n                tag_fn = lambda x: np.array([1.]*(x + 1))\n                weight_fn = tag_fn\n            out = nexcp_split(\n                model=sm.WLS,\n                split_function=split_fn,\n                y=y_model,\n                X=X_model,\n                tag_function=tag_fn,\n                weight_function=weight_fn,\n                alpha=0.1,\n                test_index=i\n            )\n            out[\"method\"] = method\n            out[\"dataset\"] = dataset\n            out[\"index\"] = i\n            del out[\"ci\"]\n            results.append(out)\n\n\nPlots\ncoverage_plot = plot_rolling_coverage(results, alpha=0.1, window=300)\ncoverage_plot.show()\n\nwidth_plot = plot_rolling_width(results, window=300)\nwidth_plot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\ntable = (\n    pd\n    .DataFrame(results)\n    .groupby([\"method\", \"dataset\"])\n    .mean()\n    .reset_index()\n)\ntable = (\n    table\n    .pivot_table(\n        index='method',\n        columns='dataset',\n        values=['covered', 'width']\n    )\n)\ntable.columns = [f'{col[0]}_{col[1].lower()}' for col in table.columns]\ntable = table.reset_index()\ntable = (\n    GT(table, rowname_col=\"method\")\n    .tab_spanner(\n        label=\"Electricity data\",\n        columns=[\"covered_electricity\", \"width_electricity\"]\n    )\n    .tab_spanner(\n        label=\"Permuted electricity data\",\n        columns=[\"covered_permuted\", \"width_permuted\"]\n    )\n    .fmt_number(\n        columns = [\n            \"covered_electricity\",\n            \"width_electricity\",\n            \"covered_permuted\",\n            \"width_permuted\"\n        ],\n        decimals=3\n    )\n    .cols_label(\n        covered_electricity = \"Coverage\",\n        width_electricity = \"Width\",\n        covered_permuted = \"Coverage\",\n        width_permuted = \"Width\"\n    )\n)\ntable.show()\n\n\n\n\n\n\n\n\nElectricity data\nPermuted electricity data\n\n\nCoverage\nWidth\nCoverage\nWidth\n\n\n\n\nCP+LS\n0.859\n0.558\n0.895\n0.628\n\n\nNexCP+LS\n0.878\n0.581\n0.902\n0.638\n\n\nNexCP+WLS\n0.880\n0.497\n0.895\n0.629\n\n\n\n\n\n\n        \n\n\n\n\n\nSimulated example\nThis demonstrates the conformal prediction algorithm in the following data settings: i.i.d. data, data generating process with changepoints, and data with distribution drift. In the paper they repeat this 200 times to smooth the estimates, but for computational purposes here I only repeated it 50 times.\n\nsplit_fn = lambda x: np.sort(generator.choice(x, int(np.floor(x*0.3)), replace=False))\nresults = []\n\n# Predict for each observation from N=100 to N=len(electricity)\nfor i in tqdm(range(100, 2000), total=2000-100):\n    for rep in range(50):\n        for method in [\"NexCP+LS\", \"NexCP+WLS\", \"CP+LS\"]:\n            for dataset in [\"setting_1\", \"setting_2\", \"setting_3\"]:\n                if dataset == \"setting_1\":\n                    X_model, y_model = sim_data(2000, 4, setting=1)\n                elif dataset == \"setting_2\":\n                    X_model, y_model = sim_data(2000, 4, setting=2)\n                else:\n                    X_model, y_model = sim_data(2000, 4, setting=3)\n                if method == \"NexCP+LS\":\n                    tag_fn = lambda x: np.array([1.]*(x + 1))\n                    weight_fn = lambda x: 0.99**np.arange(x, -1, -1)\n                elif method == \"NexCP+WLS\":\n                    tag_fn = lambda x: 0.99**np.arange(x, -1, -1)\n                    weight_fn = tag_fn\n                else:\n                    tag_fn = lambda x: np.array([1.]*(x + 1))\n                    weight_fn = tag_fn\n                out = nexcp_split(\n                    model=sm.WLS,\n                    split_function=split_fn,\n                    y=y_model,\n                    X=X_model,\n                    tag_function=tag_fn,\n                    weight_function=weight_fn,\n                    alpha=0.1,\n                    test_index=i\n                )\n                out[\"method\"] = method\n                out[\"dataset\"] = dataset\n                out[\"index\"] = i\n                del out[\"ci\"]\n                results.append(out)\n\n\nPlots\ncoverage_plot = plot_rolling_coverage(\n    results,\n    alpha=0.1,\n    window=10,\n    rows=3,\n    repeated=True\n)\ncoverage_plot.show()\n\nwidth_plot = plot_rolling_width(results, window=10, rows=3, repeated=True)\nwidth_plot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\ntable = (\n    pd\n    .DataFrame(results)\n    .groupby([\"method\", \"dataset\", \"index\"])\n    .mean()\n    .reset_index()\n    .drop(labels=[\"index\"], axis=1)\n    .groupby([\"method\", \"dataset\"])\n    .mean()\n    .reset_index()\n)\ntable = (\n    table\n    .pivot_table(\n        index=\"method\",\n        columns=\"dataset\",\n        values=[\"covered\", \"width\"]\n    )\n)\ntable.columns = [f\"{col[0]}_{col[1].lower()}\" for col in table.columns]\ntable = table.reset_index()\ntable = (\n    GT(table, rowname_col=\"method\")\n    .tab_spanner(\n        label=\"Setting 1 (i.i.d. data)\",\n        columns=[\"covered_setting_1\", \"width_setting_1\"]\n    )\n    .tab_spanner(\n        label=\"Setting 2 (changepoints)\",\n        columns=[\"covered_setting_2\", \"width_setting_2\"]\n    )\n    .tab_spanner(\n        label=\"Setting 3 (drift)\",\n        columns=[\"covered_setting_3\", \"width_setting_3\"]\n    )\n    .fmt_number(\n        columns = [\n            \"covered_setting_1\",\n            \"width_setting_1\",\n            \"covered_setting_2\",\n            \"width_setting_2\",\n            \"covered_setting_3\",\n            \"width_setting_3\"\n        ],\n        decimals=3\n    )\n    .cols_label(\n        covered_setting_1 = \"Coverage\",\n        width_setting_1 = \"Width\",\n        covered_setting_2 = \"Coverage\",\n        width_setting_2 = \"Width\",\n        covered_setting_3 = \"Coverage\",\n        width_setting_3 = \"Width\"\n    )\n)\ntable.show()\n\n\n\n\n\n\n\n\nSetting 1 (i.i.d. data)\nSetting 2 (changepoints)\nSetting 3 (drift)\n\n\nCoverage\nWidth\nCoverage\nWidth\nCoverage\nWidth\n\n\n\n\nCP+LS\n0.899\n3.325\n0.832\n6.022\n0.836\n3.754\n\n\nNexCP+LS\n0.898\n3.324\n0.874\n6.692\n0.880\n4.208\n\n\nNexCP+WLS\n0.897\n3.403\n0.896\n4.114\n0.897\n3.440"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Daniel Molitor",
    "section": "",
    "text": "The Causal Effect of Parent Occupation on Child Occupation: A Multivalued Treatment with Positivity Constraints. Ian Lundberg, Jennie Brand, Daniel Molitor. Conditionally accepted, Sociological Methods and Research.\n\n\nAbstract\n\n\nContemporary social mobility research often adopts an ostensibly descriptive goal: to document associations between parent and child socioeconomic outcomes and their variation over time and place. To complement descriptive research, we adopt a causal goal: to estimate the degree to which parent occupation causes child occupation. We formalize this causal goal in the potential outcomes framework to define precise counterfactuals. We highlight a difficulty connected to the positivity assumption for causal inference: when the treatment is parent occupation, many counterfactuals never happen in observed data. Parents without college degrees are never employed as physicians, for instance. We show how to select causal estimands involving only the counterfactuals that can be studied with data. We demonstrate our approach using the National Longitudinal Survey of Youth 1979. Our causal approach points to open questions about how specific aspects of family background, such as parent occupation, causally shape the life chances of children.\n\n\nEstimating Value-added Returns to Labor Training Programs with Causal Machine Learning. Mintaka Angell, et al. OSF Pre-prints.\n\n\nAbstract\n\n\nThe mismatch between the skills that employers seek and the skills that workers possess will increase substantially as demand for technically skilled workers accelerates. Skill mismatches disproportionately affect low-income workers and those within industries where relative demand growth for technical skills is strongest. As a result, much emphasis is placed on reskilling workers to ease transitions into new careers. However, utilization of training programs may be sub-optimal if workers are uncertain about the returns to their investment in training. While the U.S. spends billions of dollars annually on reskilling programs and unemployment insurance, there are few measures of program effectiveness that workers or government can use to guide training investment and ensure valuable reskilling outcomes. We demonstrate a causal machine learning method for estimating the value-added returns to training programs in Rhode Island, where enrollment increases future quarterly earnings by $605 on average, ranging from -$1,570 to $3,470 for individual programs. In a nationwide survey (N=2,014), workers prefer information on the value-added returns to earnings following training enrollment, establishing the importance of our estimates for guiding training decisions. For every 10% increase in expected earnings, workers are 17.4% more likely to express interest in training. State and local governments can provide this preferred information on value-added returns using our method and existing administrative data."
  },
  {
    "objectID": "research/index.html#pre-prints",
    "href": "research/index.html#pre-prints",
    "title": "Daniel Molitor",
    "section": "",
    "text": "The Causal Effect of Parent Occupation on Child Occupation: A Multivalued Treatment with Positivity Constraints. Ian Lundberg, Jennie Brand, Daniel Molitor. Conditionally accepted, Sociological Methods and Research.\n\n\nAbstract\n\n\nContemporary social mobility research often adopts an ostensibly descriptive goal: to document associations between parent and child socioeconomic outcomes and their variation over time and place. To complement descriptive research, we adopt a causal goal: to estimate the degree to which parent occupation causes child occupation. We formalize this causal goal in the potential outcomes framework to define precise counterfactuals. We highlight a difficulty connected to the positivity assumption for causal inference: when the treatment is parent occupation, many counterfactuals never happen in observed data. Parents without college degrees are never employed as physicians, for instance. We show how to select causal estimands involving only the counterfactuals that can be studied with data. We demonstrate our approach using the National Longitudinal Survey of Youth 1979. Our causal approach points to open questions about how specific aspects of family background, such as parent occupation, causally shape the life chances of children.\n\n\nEstimating Value-added Returns to Labor Training Programs with Causal Machine Learning. Mintaka Angell, et al. OSF Pre-prints.\n\n\nAbstract\n\n\nThe mismatch between the skills that employers seek and the skills that workers possess will increase substantially as demand for technically skilled workers accelerates. Skill mismatches disproportionately affect low-income workers and those within industries where relative demand growth for technical skills is strongest. As a result, much emphasis is placed on reskilling workers to ease transitions into new careers. However, utilization of training programs may be sub-optimal if workers are uncertain about the returns to their investment in training. While the U.S. spends billions of dollars annually on reskilling programs and unemployment insurance, there are few measures of program effectiveness that workers or government can use to guide training investment and ensure valuable reskilling outcomes. We demonstrate a causal machine learning method for estimating the value-added returns to training programs in Rhode Island, where enrollment increases future quarterly earnings by $605 on average, ranging from -$1,570 to $3,470 for individual programs. In a nationwide survey (N=2,014), workers prefer information on the value-added returns to earnings following training enrollment, establishing the importance of our estimates for guiding training decisions. For every 10% increase in expected earnings, workers are 17.4% more likely to express interest in training. State and local governments can provide this preferred information on value-added returns using our method and existing administrative data."
  },
  {
    "objectID": "research/index.html#publications",
    "href": "research/index.html#publications",
    "title": "Daniel Molitor",
    "section": "Publications",
    "text": "Publications\nDelivering Unemployment Assistance in Times of Crisis: Scalable Cloud Solutions Can Keep Essential Government Programs Running and Supporting Those in Need. Mintaka Angell, et al. (2020). Digital Government: Research and Practice.\n\n\nAbstract\n\n\nThe COVID-19 public health emergency caused widespread economic shutdown and unemployment. The resulting surge in Unemployment Insurance claims threatened to overwhelm the legacy systems state workforce agencies rely on to collect, process, and pay claims. In Rhode Island, we developed a scalable cloud solution to collect Pandemic Unemployment Assistance claims as part of a new program created under the Coronavirus Aid, Relief and Economic Security Act to extend unemployment benefits to independent contractors and gig-economy workers not covered by traditional Unemployment Insurance. Our new system was developed, tested, and deployed within 10 days following the passage of the Coronavirus Aid, Relief and Economic Security Act, making Rhode Island the first state in the nation to collect, validate, and pay Pandemic Unemployment Assistance claims. A cloud-enhanced interactive voice response system was deployed a week later to handle the corresponding surge in weekly certifications for continuing unemployment benefits. Cloud solutions can augment legacy systems by offloading processes that are more efficiently handled in modern scalable systems, reserving the limited resources of legacy systems for what they were originally designed. This agile use of combined technologies allowed Rhode Island to deliver timely Pandemic Unemployment Assistance benefits with an estimated cost savings of $502,000 (representing a 411% return on investment)."
  },
  {
    "objectID": "research/index.html#works-in-progress",
    "href": "research/index.html#works-in-progress",
    "title": "Daniel Molitor",
    "section": "Works in Progress",
    "text": "Works in Progress\nData-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination. Jennah Gosciak, Daniel Molitor, Ian Lundberg."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Molitor",
    "section": "",
    "text": "Hello there! I’m a third-year PhD student at Cornell’s School of Information Science. My research interests center primarily on policy-relevant applications of adaptive/online experimentation and causal inference methods. I love open source software and enjoy building tools and learning new technologies whenever I can. I’m very thankful to have my work supported by an NSF Graduate Research Fellowship!\n\n\n\nNews\n\n\n\n\n\n\n\nOctober 2024\nPresented Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination at CODE@MIT 2024.\n\n\nAugust 2024\nOur work on Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination was accepted for a presentation at ASA 2024.\n\n\nJuly 2024\nAwarded an AWS Cloud Computing Grant (“Data-Adaptive Experiments to Discover Discrimination in Context”).\n\n\nMay 2024\nReceived Outstanding PhD TA Award for Studying Social Inequality with Data Science taught by Ian Lundberg.\n\n\nMay 2024\nJointly presented our work on Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination at ACIC 2024.\n\n\nApril 2023\nAwarded an NSF Graduate Research Fellowship."
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html",
    "href": "blog/posts/robust_adaptive_exp/index.html",
    "title": "Robust Adaptive Experiments",
    "section": "",
    "text": "Recently I’ve been thinking about how to design adaptive experiments that enable valid inference on treatment effects while maintaining sufficient power to detect nonzero effects across treatment arms (including sub-optimal arms). To explore this, I will run simulations demonstrating how we can achieve these goals. Specifically, I extend the Mixture Adaptive Design (MAD) (Liang & Bojinov, 2024) to produce an adaptive experiment with the following properties:"
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#introducing-the-mad",
    "href": "blog/posts/robust_adaptive_exp/index.html#introducing-the-mad",
    "title": "Robust Adaptive Experiments",
    "section": "Introducing the MAD",
    "text": "Introducing the MAD\nThe MAD combines Bernoulli randomization with arbitrary multi-armed bandit (MAB) algorithms, enabling unbiased ATE estimation with anytime-valid confidence sequences (CSs).\nTo illustrate its usefulness, consider a simple experiment with one control and one treatment arm. Outcomes are sampled as follows:\n\nControl arm: Y ∼ Bernoulli(\\(\\theta\\)=0.5)\nTreatment arm: Y∼Bernoulli(\\(\\theta\\)=0.6)\nTrue ATE: 0.1\n\nWe use Thompson Sampling (TS) as the bandit algorithm and stop the experiment as soon as the ATE reaches statistical significance.\n\n\nShow the code\ngenerator = np.random.default_rng(seed=123)\n\ndef reward_fn(arm: int) -&gt; float:\n    values = {\n        0: generator.binomial(1, 0.5),\n        1: generator.binomial(1, 0.6)  # ATE = 0.1\n    }\n    return values[arm]\n\nexp_simple = MAD(\n    bandit=TSBernoulli(k=2, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(30e3)\n)\nexp_simple.fit(cs_precision=0, verbose=False)\n\n\nFinally, we plot the MAD-estimated ATE over time, showing convergence to the true effect and demonstrating that the corresponding 95% CSs maintain valid coverage.\n\n\nShow the code\n(\n    exp_simple.plot_ate_path()\n    + pn.coord_cartesian(ylim=(-.5, 1.5))\n    + pn.geom_hline(\n        mapping=pn.aes(yintercept=\"ate\", color=\"factor(arm)\"),\n        data=pd.DataFrame({\"arm\": list(range(1, 2)), \"ate\": [0.1]}),\n        linetype=\"dotted\"\n    )\n    + pn.theme(strip_text=pn.element_blank()) \n)\n\n\n\n\n\n\n\n\n\n\nBandit benefits\nThe underlying bandit algorithm provides additional benefits. Below, we show the total sample size assigned to both arms of the experiment:\n\n\nShow the code\nexp_simple.plot_n()\n\n\n\n\n\n\n\n\n\nand the arm assignment probability over time:\n\n\nShow the code\nexp_simple.plot_probabilities()\n\n\n\n\n\n\n\n\n\nThe TS algorithm assigns the majority of the sample to the optimal arm (Arm 1 is the treatment). This demonstrates how we can achieve both valid ATE inference and reward maximization with the bandit algorithm.\n\n\nLimitations\nIn adaptive experiments with multiple treatment arms, a common issue is being under-powered to detect non-zero ATEs in sub-optimal arms. This happens because the bandit algorithm allocates most of the sample to the optimal arm(s), neglecting the others.\nWe demonstrate this with an experiment simulating a control arm and four treatment arms with ATEs of 0.1, 0.12, 0.3, and 0.32, respectively, over a fixed sample size of 20,000. We expect the bandit algorithm to allocate most of the sample to arms 3 and 4, leaving arms 1 and 2 under-powered.\n\n\nShow the code\ndef reward_fn(arm: int) -&gt; float:\n    values = {\n        0: generator.binomial(1, 0.5),  # Control arm\n        1: generator.binomial(1, 0.6),  # ATE = 0.1\n        2: generator.binomial(1, 0.62), # ATE = 0.12\n        3: generator.binomial(1, 0.8),  # ATE = 0.3\n        4: generator.binomial(1, 0.82)  # ATE = 0.32\n    }\n    return values[arm]\n\nexp_complex = MAD(\n    bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(20e3)\n)\nexp_complex.fit(early_stopping=False, verbose=False)\n\nates = pd.concat(\n    [\n        exp_complex.estimates().assign(which=\"mad\"),\n        pd.DataFrame({\n            \"arm\": list(range(1, 5)),\n            \"ate\": [0.1, 0.12, 0.3, 0.32],\n            \"which\": [\"truth\"]*(4)\n        })\n    ],\n    axis=0\n)\n(\n    pn.ggplot(\n        ates,\n        mapping=pn.aes(\n            x=\"factor(arm)\",\n            y=\"ate\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"which\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.3))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.3), width=0.001)\n    + pn.geom_hline(yintercept=0, linetype=\"dashed\", color=\"black\")\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"ATE\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nAs anticipated, we observe strong ATE estimates for arms 3 and 4 but under-powered estimates for arms 1 and 2 (CSs include 0). We can confirm that, indeed, TS focuses the majority of the sample on arms 3 and 4 to the detriment of power in our experiment.\n\n\nShow the code\nexp_complex.plot_n()"
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#mad-modified",
    "href": "blog/posts/robust_adaptive_exp/index.html#mad-modified",
    "title": "Robust Adaptive Experiments",
    "section": "MAD modified",
    "text": "MAD modified\nI propose an extension of the MAD algorithm to address the challenge of inadequate power in sub-optimal arms. For each treatment arm \\(k \\in K\\) and time period \\(t\\), I introduce importance weights \\(w_{tk} \\in [0, 1]\\). Once the estimated ATE for arm \\(k\\) becomes statistically significant, \\(w_{tk}\\) begins to shrink toward zero according to a user-defined function of \\(t\\).\nIn the notation of Liang and Bojinov, let \\(A\\) represent an arbitrary adaptive algorithm. They define \\(p_t^A(k)\\) as the assignment probability for arm \\(k\\) at time \\(t\\) under \\(A\\). By construction, the set \\(p_t^A(k)\\) of adaptive assignment probabilities for all \\(k \\in K\\) forms a valid probability distribution over \\(K\\), meaning \\(\\sum_{k \\in K}{p_t^A(k)}=1\\). I modify these probabilities to \\(g(p_t^A(k))\\) where \\(g\\) re-weights \\(p_t^A(k)\\) based on the importance weight \\(w_{tk}\\).\nFor each treatment arm \\(k \\in K\\) at time \\(t\\), the re-weighted probability \\(g(p_t^A(k))\\) is computed as follows:\n1.) Apply Importance Weights: Each probability is first scaled by its importance weight: \\[p_t^*(k)=w_{tk}*p_t^A(k).\\]\n2.) Compute Lost Probability Mass: The probability mass lost due to down-weighting is: \\[L_t = \\sum_{k \\in K}{p_t^A(k)*(1 - w_{tk})}.\\]\n3.) Compute Relative Redistribution Weights: The total weight sum is: \\[W_t = \\sum_{k \\in K}{w_{tk}}.\\] Each arm’s share of the remaining mass is: \\[r_{tk} = \\frac{w_{tk}}{W_t}.\\]\n4.) Redistribute Lost Mass: Redistribute the lost mass proportionally to the relative weights: \\[p_t^g(k) = p_t^*(k) + (r_{tk} * L_t).\\]\n5.) Normalization Check: Since \\(p_t^g(k)\\) for all \\(k \\in K\\) forms a valid probability distribution over \\(K\\), it satisfies: \\[\\sum_{k \\in K}p_t^g(k)=1.\\]\nThus, the function \\(g\\) modifies the original assignment probabilities by scaling each by its importance weight and redistributing the lost probability mass in a manner that preserves the total probability sum.\n\nUser-Specified Decay of Importance Weights\nThe importance weight function \\(w_{tk}\\)​ controls how quickly the assignment probability for arm \\(k\\) shrinks once its estimated ATE becomes statistically significant. This user-defined function balances two extremes:\n\n\\(w_{tk}=1\\) for all \\(t\\), which keeps \\(g(p_t^A(k))=p_t^A(k)\\), making the algorithm identical to the original MAD design.\n\\(w_{tk}=0\\) after arm \\(k\\) reaches statistical significance, redirecting all future probability mass away from arm \\(k\\) and prioritizing underpowered arms.\nMore generally, the user defines \\(w_{tk}\\) somewhere in between, where:\n\nA slower decay of \\(w_{tk}\\) (closer to 1) retains more influence from the adaptive algorithm’s assignment probabilities.\nA faster decay (closer to 0) shifts the algorithm toward prioritizing underpowered arms at the expense of bandit goals (e.g. reward maximization).\n\n\nReasonable choices for \\(w_{tk}\\) include polynomial or exponential decay, providing flexibility in tuning sample reallocation."
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#algorithm-comparison",
    "href": "blog/posts/robust_adaptive_exp/index.html#algorithm-comparison",
    "title": "Robust Adaptive Experiments",
    "section": "Algorithm comparison",
    "text": "Algorithm comparison\nI compare the two algorithms to highlight the benefits of the modified approach. The modified algorithm significantly improves power to detect non-zero ATEs in all treatment arms and provides more precise ATE estimates than the original MAD algorithm with the same sample size. However, this comes at the cost of assigning more sample to sub-optimal arms, where “optimal” is defined by the underlying bandit algorithm.\n\nImproved power and precision\nThe following plots demonstrate the increased power and precision of the modified MAD algorithm.\n\n\nShow the code\n# Run the modified algorithm\nmad_modified = MADModified(\n    bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(20e3),\n    decay=lambda x: 1./(x**(1./8.))\n)\nmad_modified.fit(cs_precision=0.1, verbose=False, early_stopping=True)\n\n# Run the vanilla algorithm\nmad_vanilla = MAD(\n    bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=mad_modified._bandit._t\n)\nmad_vanilla.fit(verbose=False, early_stopping=False)\n\n# Compare the ATEs and CSs\nates = pd.concat(\n    [\n        mad_modified.estimates().assign(which=\"MADMod\"),\n        mad_vanilla.estimates().assign(which=\"MAD\"),\n        pd.DataFrame({\n            \"arm\": list(range(1, 5)),\n            \"ate\": [0.1, 0.12, 0.3, 0.32],\n            \"which\": [\"Truth\"]*(4)\n        })\n    ],\n    axis=0\n)\n(\n    pn.ggplot(\n        ates,\n        mapping=pn.aes(\n            x=\"factor(arm)\",\n            y=\"ate\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"which\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.3))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.3), width=0.001)\n    + pn.geom_hline(yintercept=0, linetype=\"dashed\", color=\"black\")\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"ATE\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nAnd the following plot compares the sample assignment to the treatment arms of the two algorithms:\n\n\nShow the code\nsample_sizes = pd.concat([\n    pd.DataFrame(x) for x in\n    [\n        {\n            \"arm\": [k for k in range(len(mad_modified._ate))],\n            \"n\": [last(n) for n in mad_modified._n],\n            \"which\": [\"MADMod\"]*len(mad_modified._ate)\n        },\n        {\n            \"arm\": [k for k in range(len(mad_vanilla._ate))],\n            \"n\": [last(n) for n in mad_vanilla._n],\n            \"which\": [\"MAD\"]*len(mad_vanilla._ate)\n        }\n    ]\n])\n(\n    pn.ggplot(sample_sizes, pn.aes(x=\"factor(arm)\", y=\"n\", fill=\"which\", color=\"which\"))\n    + pn.geom_bar(stat=\"identity\", position=pn.position_dodge(width=0.75), width=0.7)\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"N\", color=\"Method\", fill=\"Method\")\n)\n\n\n\n\n\n\n\n\n\n\n\nSimulation results over 1,0000 runs\nWe can more precisely quantify the improvements by running 1,000 simulations, comparing Type 2 error and confidence band width between the vanilla MAD algorithm and the modified algorithm. Each simulation runs for 20,000 iterations with early stopping. If the modified algorithm stops early, the vanilla algorithm will also stop early to maintain equal sample sizes in each simulation.\n\n\nShow the code\ndef delta_fn(x):\n    return 1. / (x ** 0.24)\n\ndef decay_fn(x):\n    return 1. / (x ** (1. / 8.))\n\ndef compare(i):\n    mad_modified = MADModified(\n        bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n        alpha=0.05,\n        delta=delta_fn,\n        t_star=int(2e4),\n        decay=decay_fn\n    )\n    mad_modified.fit(cs_precision=0.1, verbose=False, early_stopping=True)\n\n    # Run the vanilla algorithm\n    mad_vanilla = MAD(\n        bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n        alpha=0.05,\n        delta=delta_fn,\n        t_star=mad_modified._bandit._t\n    )\n    mad_vanilla.fit(verbose=False, early_stopping=False)\n\n    # Calculate the Type 2 error and the Confidence Sequence width\n\n    ## For modified algorithm\n    mad_mod_n = (\n        pd\n        .DataFrame([\n            {\"arm\": k, \"n\": last(mad_modified._n[k])}\n            for k in range(mad_modified._bandit.k())\n            if k != mad_modified._bandit.control()\n        ])\n        .assign(\n            n_pct=lambda x: x[\"n\"].apply(lambda y: y/np.sum(x[\"n\"]))\n        )\n    )\n    mad_mod_df = (\n        mad_modified\n        .estimates()\n        .assign(\n            idx=i,\n            method=\"modified\",\n            width=lambda x: x[\"ub\"] - x[\"lb\"],\n            error=lambda x: ((0 &gt; x[\"lb\"]) & (0 &lt; x[\"ub\"]))\n        )\n        .merge(mad_mod_n, on=\"arm\", how=\"left\")\n    )\n\n    ## For vanilla algorithm\n    mad_van_n = (\n        pd\n        .DataFrame([\n            {\"arm\": k, \"n\": last(mad_vanilla._n[k])}\n            for k in range(mad_vanilla._bandit.k())\n            if k != mad_vanilla._bandit.control()\n        ])\n        .assign(\n            n_pct=lambda x: x[\"n\"].apply(lambda y: y/np.sum(x[\"n\"]))\n        )\n    )\n    mad_van_df = (\n        mad_vanilla\n        .estimates()\n        .assign(\n            idx=i,\n            method=\"mad\",\n            width=lambda x: x[\"ub\"] - x[\"lb\"],\n            error=lambda x: ((0 &gt; x[\"lb\"]) & (0 &lt; x[\"ub\"]))\n        )\n        .merge(mad_van_n, on=\"arm\", how=\"left\")\n    )\n\n    out = {\n        \"metrics\": pd.concat([mad_mod_df, mad_van_df]),\n        \"reward\": {\n            \"modified\": np.sum(mad_modified._rewards),\n            \"mad\": np.sum(mad_vanilla._rewards)\n        }\n    }\n    return out\n\n# Execute in parallel with joblib\ncomparison_results_list = [\n    x for x in\n    joblib.Parallel(return_as=\"generator\", n_jobs=-1)(\n        joblib.delayed(compare)(i) for i in range(100)\n    )\n]\n\n# Compare performance on key metrics across simulations\nmetrics_df = pd.melt(\n    (\n        pd\n        .concat([x[\"metrics\"] for x in comparison_results_list])\n        .reset_index(drop=True)\n        .assign(error=lambda x: x[\"error\"].apply(lambda y: int(y)))\n    ),\n    id_vars=[\"arm\", \"method\"],\n    value_vars=[\"width\", \"error\", \"n\", \"n_pct\"],\n    var_name=\"meas\",\n    value_name=\"value\"\n)\n\n# Compare reward accumulation across simulations\nreward_df = pd.melt(\n    pd.DataFrame([x[\"reward\"] for x in comparison_results_list]),\n    value_vars=[\"modified\", \"mad\"],\n    var_name=\"method\",\n    value_name=\"reward\"\n)\n\nmetrics_summary = (\n    metrics_df\n    .groupby([\"arm\", \"method\", \"meas\"], as_index=False).agg(\n        mean=(\"value\", \"mean\"),\n        std=(\"value\", \"std\"),\n        n=(\"value\", \"count\")\n    )\n    .assign(\n        se=lambda x: x[\"std\"] / np.sqrt(x[\"n\"]),\n        t_val=lambda x: t.ppf(0.975, x[\"n\"] - 1),\n        ub=lambda x: x[\"mean\"] + x[\"t_val\"] * x[\"se\"],\n        lb=lambda x: x[\"mean\"] - x[\"t_val\"] * x[\"se\"]\n    )\n    .drop(columns=[\"se\", \"t_val\"])\n)\n\n\nThe following plot shows the mean (and 95% confidence intervals) of the Type 2 error and CS width for both algorithms.\n\n\nShow the code\nfacet_labels = {\n    \"error\": \"Type 2 error\",\n    \"width\": \"Interval width\",\n    \"n\": \"Sample size\",\n    \"n_pct\": \"Sample size %\"\n}\n(\n    pn.ggplot(\n        metrics_summary[metrics_summary[\"meas\"].isin([\"error\", \"width\"])],\n        pn.aes(\n            x=\"factor(arm)\",\n            y=\"mean\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"method\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.2))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.2), width=0.01)\n    + pn.facet_wrap(\n        \"~ meas\",\n        labeller=lambda x: facet_labels[x],\n        scales=\"free\"\n    )\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nThe modified MAD algorithm achieves far lower Type 2 error and improved ATE precision in all treatment arms.\n\n\nTradeoffs\nThese plots illustrate the tradeoffs of the modified algorithm. On average, it allocates significantly more sample to sub-optimal arms compared to the standard MAD algorithm.\n\n\nShow the code\n(\n    pn.ggplot(\n        metrics_summary[metrics_summary[\"meas\"].isin([\"n\", \"n_pct\"])],\n        pn.aes(\n            x=\"factor(arm)\",\n            y=\"mean\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"method\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.2))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.2), width=0.01)\n    + pn.facet_wrap(\n        \"~ meas\",\n        labeller=lambda x: facet_labels[x],\n        scales=\"free\"\n    )\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nAs a result, this reallocation reduces total reward accumulation. The difference in accumulated reward across the 1,000 simulations is shown below:\n\n\nShow the code\n(\n    pn.ggplot(reward_df, pn.aes(x=\"method\", y=\"reward\"))\n    + pn.geom_boxplot()\n    + pn.theme_538()\n    + pn.labs(x=\"Method\", y=\"Cumulative reward\")\n)"
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#summary",
    "href": "blog/posts/robust_adaptive_exp/index.html#summary",
    "title": "Robust Adaptive Experiments",
    "section": "Summary",
    "text": "Summary\nIn summary, this approach allows us to achieve anytime-valid inference on the ATE, enabling early stopping for greater flexibility and efficiency. It also allows us to ensure dynamic sample allocation, guaranteeing sufficient power for all (or the top n) treatment arms."
  }
]