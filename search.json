[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Molitor",
    "section": "",
    "text": "Hello there! I‚Äôm a third-year PhD student at Cornell‚Äôs School of Information Science. My research interests center primarily on policy-relevant applications of adaptive/online experimentation and causal inference methods. I love open source software and enjoy building tools and learning new technologies whenever I can. I‚Äôm very thankful to have my work supported by an NSF Graduate Research Fellowship!\n\n\n\nNews\n\n\n\n\n\n\n\nOctober 2024\nPresented Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination at CODE@MIT 2024.\n\n\nAugust 2024\nOur work on Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination was accepted for a presentation at ASA 2024.\n\n\nJuly 2024\nAwarded an AWS Cloud Computing Grant (‚ÄúData-Adaptive Experiments to Discover Discrimination in Context‚Äù).\n\n\nMay 2024\nReceived Outstanding PhD TA Award for Studying Social Inequality with Data Science taught by Ian Lundberg.\n\n\nMay 2024\nJointly presented our work on Data-Adaptive Experimentation to Find Contexts with the Most and Least Discrimination at ACIC 2024.\n\n\nApril 2023\nAwarded an NSF Graduate Research Fellowship."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html",
    "href": "blog/posts/dynamic_surveys/index.html",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "",
    "text": "Understanding discriminatory human choices are of central interest across the social sciences. Typically when studying such questions, researchers employ standard designs such as experimental audit studies or conjoint analyses. Recent advances in the adaptive experimentation literature have explored how multi-armed bandit (MAB) algorithms can be used to answer the same questions with lower cost and greater data efficiency while also mitigating ethical concerns that may arise in some randomized experiments (e.g.¬†assigning participants to harmful treatment arms)1 2.\nAlthough MAB methods can provide significant improvements over standard experimental methods, implementing adaptive experiments or surveys can pose a challenge. There are many survey platforms at the researcher‚Äôs disposal such as Qualtrics, Google Forms, etc. that can quickly accomodate standard survey designs, but these platforms do not easily support the design of adaptive surveys. Without such tools at their disposal, the researcher is stuck needing to design their own custom solution. This is the exact situation that my research team and I ran into a few months ago.\nIt began with a straightforward enough question. We would like to know, for example, how American adults in the U.S. discriminate on the basis of education when choosing which immigrants to prioritize for immigrant visas?3 Our goal was to explore how we could adopt methods from the adaptive experimentation literature to answer these questions more efficiently than standard methods.\nTo this end, we framed the question as a stochastic MAB problem. Each arm of the bandit was defined as one set of immigrant characteristics and the outcome of interest (reward) was whether the survey respondent chose to prioritize the immigrant with higher education, given that set of characteristics. We wanted to understand under which set of characteristics American adults are the most likely to discriminate against an immigrant who has lower education.\nTo uncover the set of characteristics with the most discrimination, we employed a classic algorithm from the adaptive literature in Thompson Sampling (TS)4. TS is a dynamic algorithm. It starts out by assuming that the probability of discrimination is the same across all sets of immigrant characteristics. Every time a new survey respondent takes the survey, it assigns them to the set of characteristics that has the highest probability of resulting in a discriminatory response. TS then observes whether or not that respondent discriminates, and it updates the probability of discrimination for the set of characteristics which they were assigned to. As the algorithm learns which sets of characteristics are most likely to elicit discriminatory responses, the algorithm progressively assigns more respondents to those arms and stops assigning respondents to characteristics that fail to elicit a discriminatory response."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#background",
    "href": "blog/posts/dynamic_surveys/index.html#background",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "",
    "text": "Understanding discriminatory human choices are of central interest across the social sciences. Typically when studying such questions, researchers employ standard designs such as experimental audit studies or conjoint analyses. Recent advances in the adaptive experimentation literature have explored how multi-armed bandit (MAB) algorithms can be used to answer the same questions with lower cost and greater data efficiency while also mitigating ethical concerns that may arise in some randomized experiments (e.g.¬†assigning participants to harmful treatment arms)1 2.\nAlthough MAB methods can provide significant improvements over standard experimental methods, implementing adaptive experiments or surveys can pose a challenge. There are many survey platforms at the researcher‚Äôs disposal such as Qualtrics, Google Forms, etc. that can quickly accomodate standard survey designs, but these platforms do not easily support the design of adaptive surveys. Without such tools at their disposal, the researcher is stuck needing to design their own custom solution. This is the exact situation that my research team and I ran into a few months ago.\nIt began with a straightforward enough question. We would like to know, for example, how American adults in the U.S. discriminate on the basis of education when choosing which immigrants to prioritize for immigrant visas?3 Our goal was to explore how we could adopt methods from the adaptive experimentation literature to answer these questions more efficiently than standard methods.\nTo this end, we framed the question as a stochastic MAB problem. Each arm of the bandit was defined as one set of immigrant characteristics and the outcome of interest (reward) was whether the survey respondent chose to prioritize the immigrant with higher education, given that set of characteristics. We wanted to understand under which set of characteristics American adults are the most likely to discriminate against an immigrant who has lower education.\nTo uncover the set of characteristics with the most discrimination, we employed a classic algorithm from the adaptive literature in Thompson Sampling (TS)4. TS is a dynamic algorithm. It starts out by assuming that the probability of discrimination is the same across all sets of immigrant characteristics. Every time a new survey respondent takes the survey, it assigns them to the set of characteristics that has the highest probability of resulting in a discriminatory response. TS then observes whether or not that respondent discriminates, and it updates the probability of discrimination for the set of characteristics which they were assigned to. As the algorithm learns which sets of characteristics are most likely to elicit discriminatory responses, the algorithm progressively assigns more respondents to those arms and stops assigning respondents to characteristics that fail to elicit a discriminatory response."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#necessity-of-a-custom-survey-form",
    "href": "blog/posts/dynamic_surveys/index.html#necessity-of-a-custom-survey-form",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Necessity of a custom survey form",
    "text": "Necessity of a custom survey form\nGiven the dynamic nature of TS, we needed a survey form that would allow us to estimate a variety of parameters and update historical data every time a new user connected to our form or submitted a survey response. At first we were bullish on Qualtrics for meeting our needs. In fact, the Qualtrics API surfaces endpoints that allow the researcher to deploy certain actions every time a user submits a survey response. Unfortunately, we quickly discovered that this functionality is only available to users with special access. When using an account under an institutional subscription (which is the case at Cornell and probably most universities), you don‚Äôt have this special access and so this was a non-starter for us. It also seemed undesirable to be downloading thousands of user responses and manually updating algorithmic parameters.\nNot to be deterred, I confidently announced that it would be no problem to build such a survey with Shiny üò¨. It turned out to be harder than I initially imagined, but it was indeed possible!"
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#architecture",
    "href": "blog/posts/dynamic_surveys/index.html#architecture",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Architecture",
    "text": "Architecture\nAs I began sketching out the codebase for our survey, I split the structure into three main services:\n\nDatabase: We needed some form of database to store algorithmic parameters and user responses throughout the duration of the survey. For our application we opted for PostgreSQL, though pretty much any database solution would have worked.\nAPI: The API was built with FastAPI and was the workhorse of the application, handling all interactions between the survey form and the database. When a new user would connect to our application, the API would retrieve the historical data from the database, perform an iteration of the TS algorithm, and update the survey form with necessary information such as which bandit arm the user would be assigned to. When the user had finished and submitted the survey, the API would update the corresponding tables and parameters in the database in preparation for new survey respondents.\nFrontend: The frontend was built with Shiny and was the actual survey form. This survey form was not in charge any computational steps, but instead collected all user response data and orchestrated the communication between the API and the database.\n\nAfter creating a working survey application, the next step was to deploy this survey so that it could actually be used by real survey respondents."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#deployment",
    "href": "blog/posts/dynamic_surveys/index.html#deployment",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Deployment",
    "text": "Deployment\n\nContainerization\nI began by taking each of the three services described above and putting it in its own Docker container. With our services containerized, we could easily deploy our application on any cloud services that support Docker.\n\n\nAWS\nOur cloud provider of choice is AWS, so the next step was to build a simple custom AMI based on Ubuntu that had Docker installed. With our AMI in hand, the final piece of the puzzle was to scale our survey appropriately. There are many tools that could have served our purposes including Kubernetes, AWS Fargate, AWS ECS/EKS, and Docker Swarm. For our purposes, I opted to go with Docker Swarm as this struck a balance between serving our scaling needs while not becoming overly complex.\n\n\nDocker Swarm mode\nFor our survey, we recruited participants from Prolific and budgeted for a maximum of 10,000 participants. From past Prolific surveys, we expected to see ~1,000 respondents per hour with anywhere between 30-50 concurrent users at all times. To ensure that our survey could easily handle any realistic level of traffic, I deployed our containerized services on a Docker Swarm cluster comprised of one manager AWS instance and ~60 worker AWS instances. All instances were equipped with the custom AMI described above.\nAt this point, our survey was online with plenty of compute resources available to handle a large number of survey respondents.\n\n\n\n\n\n\nEffectively, Docker acts as a load balancer for your swarm services so there‚Äôs no need to worry about setting up a load balancer yourself!"
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#summary",
    "href": "blog/posts/dynamic_surveys/index.html#summary",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Summary",
    "text": "Summary\nTLDR; we built our custom survey form and served it to ~10,000 survey respondents in 10 hours with the following steps.\n\nDeveloped the survey form with Shiny and added necessary scaffolding (API, database).\nContainerized these services to make deployment as easy as possible.\nDeployed services on cloud provider (AWS) and scaled the services as necessary with Docker Swarm."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#implementation-tips",
    "href": "blog/posts/dynamic_surveys/index.html#implementation-tips",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Implementation tips",
    "text": "Implementation tips\nThe following are a bunch of very specific tips based mostly on things that bit me when building this, or things that would make it better that I just never got around to adding.\n\nShiny\n\nThis is probably really obvious, but especially when you need your Shiny app to be somewhat performant, try to streamline time-consuming calculations as much as possible. For example, in our app, I structured any time-consuming steps so that they would happen either at run-time or after the user had clicked ‚ÄúSubmit‚Äù on the survey form.\nThe R Shiny software is much more mature than its Python counterpart and as a result the Python API may not surface all features that the R API does. For example the R Shiny API has a well defined way to access client data being sent to the server. To access the current URL you would do something like:\nserver &lt;- function(input, output, session) {\n  # Return the components of the URL in a string:\n  output$urlText &lt;- renderText({\n    paste(\n      sep = \"\",\n      \"protocol: \", session$clientData$url_protocol, \"\\n\",\n      \"hostname: \", session$clientData$url_hostname, \"\\n\",\n      \"pathname: \", session$clientData$url_pathname, \"\\n\",\n      \"port: \", session$clientData$url_port, \"\\n\",\n      \"search: \", session$clientData$url_search, \"\\n\"\n    )\n  })\n}\nThis feature has yet to be officially implemented in Py Shiny, as noted in this GitHub issue but can be worked around as described in this issue. The workaround solution would look something like:\ndef server(input, output, session):\n    @render.text\n    def urlText():\n        url_text = (\n          f\"protocol: {session.input['.clientdata_url_protocol']()}\\n\",\n          + f\"hostname: {session.input['.clientdata_url_hostname']()}\\n\",\n          + f\"pathname: {session.input['.clientdata_url_pathname']()}\\n\",\n          + f\"port: {session.input['.clientdata_url_port']()}\\n\",\n          + f\"search: {session.input['.clientdata_url_search']()}\\n\"\n        )\n        return url_text\n\n\n\nAWS and Docker Swarm mode\n\nAWS network rules: Ports, ports, ports. You need to make sure that all instances in your swarm have Security Group(s) attached with the necessary inbound/outbound rules defined. When running Docker Swarm the following inbound rules are absolutely essential otherwise Swarm mode will not work:\n\nTCP port 2377 for cluster management communications.\nTCP and UDP port 7946 for communication among nodes.\nUDP port 4789 for overlay network traffic.\n\nIn addition, make sure you add any rules for other ports that are specific to your application. In our case our app was exposed on port 8000 so I needed to add an additional inbound rule for TCP port 8000.\nAdding worker nodes to swarm: When adding a worker instance to the swarm on AWS, it is essential to include the --advertise-addr argument. For example:\ndocker swarm join --token SWMTKN-1-49nj1abc... manager.node.ip:2377 --advertise-addr worker.node.ip\nConfiguring HTTPS: When you deploy your Shiny application on an AWS compute instance, e.g.¬†on port 8000, the application will be available at a url looking something like http://manager.node.ip:8000. While this works fine, it will look unusual to the average user and may be flagged by some browsers as insecure and result in warning messages being sent to the user. If it is important to have HTTPS configured for your application, there are a couple ways to approach this. Both require having a domain or sub-domain name available.\n\nOnce you have launched your application (or swarm) on AWS, configure a DNS record on your domain to forward your sub-domain to the public IP address of the server where your application is hosted. This process may vary slightly depending on where you purchased your domain name (e.g.¬†Bluehost, Namecheap).\nInstall and configure nginx to forward traffic from port 80 to whatever port your application is running on.\nConfigure nginx with SSL/TLS certificates using Let‚Äôs Encrypt.\n\nThere are many good online tutorials on exactly how to do steps 2 and 3. The main shortcoming with the method described above is that you would have to do all three steps separately every time you re-deploy your services on AWS. If you only deploy once, this may not be an issue. But if you think you might terminate and re-deploy your application multiple times, it may get tiresome. One way around this is to allocate an AWS Elastic IP address to your account and then create a DNS record on your domain pointing to the elastic IP per step 1 above. Then, every time you launch your application on a new AWS compute instance you can associate the elastic IP address with your instance, and you don‚Äôt need to re-do step 1. You will still have to do steps 2 and 3, but you can do these steps programmatically. Step 1 is by far the most time consuming and you will only have to do that once.\nPersisting data with AWS volumes: When running your application on AWS, or any cloud provider, there is always the concern that your compute resources might get terminated without any warning. As such, it is essential that all your data be backed up so that it will persist regardless of whether your application terminates or not. For our application, we created an AWS volume and mounted that volume to the local filesystem of the compute instance where our database container was running. We then used a bind mount to mount that directory on the host machine into the PostgreSQL Docker container.\nScaling the application: Py Shiny is built on uvicorn. As a direct result, a user can deploy a Shiny application by simply running the following on the command line:\nuvicorn app:app --host 0.0.0.0 --port 8000\n\n\n\n\n\n\nThe first app references the file app.py where your application is defined, and the second app references the final line in your app.py file that should look something like:\napp = App(app_ui, server)\n\n\n\nUvicorn has scaling built-in via the --workers argument. If you wanted to scale your application in a super simple way and avoid all the hassle above, it‚Äôs as easy as deploying your application on a very large AWS server and running it with something like:\nuvicorn app:app --host 0.0.0.0 --port 8000 --workers 20\nFor several reasons this approach didn‚Äôt work for our situation, but it may be a reasonable approach for many people. To see more about self-hosted deployment, see the Shiny docs or the uvicorn docs.\n\n\n\nGeneral\nIn building and deploying our application there were a bunch of small, almost unnoticeable steps that go into each larger step. For example, when I wanted to deploy our survey onto AWS, there were several preliminary steps:\n\nBuild and push the Docker images for all three services to DockerHub.\nCreate all AWS resources (e.g.¬†security group, volume, etc.)\nSpin up the Docker swarm and deploy our application.\n\nWhen you‚Äôre actively developing a project it‚Äôs easy to remember all the pre-requisite steps that go into each larger step. However, it‚Äôs really easy to quickly forget these things and to come back weeks or months later and struggle to build or deploy your application. So do your future self a favor and use a build tool like just! Not only does this remove a lot of key-strokes when you‚Äôre developing an application, but it codifies all the easy-to-forget steps for your future self."
  },
  {
    "objectID": "blog/posts/dynamic_surveys/index.html#code",
    "href": "blog/posts/dynamic_surveys/index.html#code",
    "title": "Building and Deploying Adaptive Experiments with Shiny",
    "section": "Code",
    "text": "Code\nTo browse the code that corresponds to each part of this post, check out the GitHub repo here and feel free to reach out with any questions or drop an issue on the repo!"
  },
  {
    "objectID": "blog/posts/conformal-beyond-exchangeability/index.html",
    "href": "blog/posts/conformal-beyond-exchangeability/index.html",
    "title": "Conformal Prediction Beyond Exchangeability",
    "section": "",
    "text": "Dependencies\n\nfrom great_tables import GT, md\nimport numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nfrom pathlib import Path\nimport plotnine as pn\nimport statsmodels.api as sm\nfrom tqdm import tqdm\nfrom typing import Any, Callable, List\n\nbase_dir = Path().cwd()\ngenerator = np.random.default_rng()\n\n\n\nData\n\n# Electricity data\nelectricity = pd.read_csv(base_dir / \"data\" / \"electricity-normalized.csv\")\nelectricity = (\n    electricity\n    .iloc[17760:]\n    .assign(period=lambda x: x.period*24)\n    .loc[lambda df: (df[\"period\"] &gt;= 9) & (df[\"period\"] &lt;= 12)]\n    [[\"transfer\", \"nswprice\", \"vicprice\", \"nswdemand\", \"vicdemand\"]]\n    .reset_index(drop=True)\n)\npermuted = electricity.sample(frac=1).reset_index(drop=True)\n\n# Function to generate simulated data\ndef sim_data(N: int, d: int, setting: int) -&gt; tuple[pd.DataFrame, npt.NDArray]:\n    X = np.random.multivariate_normal(mean=np.zeros(d), cov=np.eye(d), size=N)\n    if setting == 1:\n        beta = np.array([2, 1, 0, 0])\n        y = X @ beta + np.random.normal(0, 1, N)\n        X = pd.DataFrame(X, columns=[f\"feature_{i+1}\" for i in range(d)])\n    elif setting == 2:\n        beta_1 = np.array([2, 1, 0, 0])\n        beta_2 = np.array([0, -2, -1, 0])\n        beta_3 = np.array([0, 0, 2, 1])\n        y = np.zeros(N)\n        # Generate y for different segments\n        y[:500] = X[:500] @ beta_1 + np.random.normal(0, 1, 500)\n        y[500:1500] = X[500:1500] @ beta_2 + np.random.normal(0, 1, 1000)\n        y[1500:] = X[1500:] @ beta_3 + np.random.normal(0, 1, 500)\n        X = pd.DataFrame(X, columns=[f\"feature_{i+1}\" for i in range(d)])\n    else:\n        beta_start = np.array([2, 1, 0, 0])\n        beta_end = np.array([0, 0, 2, 1])\n        beta = np.linspace(beta_start, beta_end, N)\n        y = np.array([X[i] @ beta[i] + np.random.normal(0, 1) for i in range(N)])\n        X = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(d)])\n    return (X, y)\n\n\n\nFunctions\nThe nexcp_split function implements non-exchangeable split conformal prediction (CP). However, we can force it to also implement standard CP, which assumes exchangeability, by setting uniform weights. So we only need one function to replicate the results!\n\ndef normalize_weights(weights: npt.NDArray):\n    return weights / weights.sum()\n\n\ndef nexcp_split(\n    model: Callable[[npt.NDArray, pd.DataFrame, npt.NDArray], Any],\n    split_function: Callable[[int], npt.NDArray],\n    y: npt.NDArray,\n    X: pd.DataFrame,\n    tag_function: Callable[[int], npt.NDArray],\n    weight_function: Callable[[int], npt.NDArray],\n    alpha: float,\n    test_index: int\n):\n    \"\"\"Implements non-exchangeable split conformal prediction\"\"\"\n    \n    # Pull test observation from data\n    y_test = y[test_index]\n    X_test = X.iloc[[test_index]]\n    # Select all observations up to that point\n    y = y[:test_index]\n    X = X.iloc[:test_index]\n    # Generate indices for train/calibration split\n    split_indices = split_function(test_index)\n    # Split data, tags, and weights\n    X_train = X.iloc[split_indices]\n    y_train = y[split_indices]\n    X_calib = X.drop(split_indices)\n    y_calib = np.delete(y, split_indices)\n    # Generate tags and weights\n    tags = tag_function(test_index)\n    weights = weight_function(test_index)\n    # Train model\n    model_base = model(y_train, X_train, weights=tags[split_indices])\n    model_fitted = model_base.fit()\n    # Generate residuals\n    residuals = np.abs(y_calib - model_fitted.predict(X_calib))\n    # Calculate weighted quantile of residuals\n    weights_calib = normalize_weights(np.delete(weights[:test_index], split_indices))\n    q_hat = np.quantile(\n        residuals,\n        1 - alpha,\n        weights=weights_calib,\n        method=\"inverted_cdf\"\n    )\n    # Calculate predicted value\n    y_hat = model_fitted.predict(X_test).iloc[0]\n    # Generate CI\n    lb = y_hat - q_hat\n    ub = y_hat + q_hat\n    covered = lb &lt;= y_test &lt;= ub\n    return {\"ci\": np.array([lb, y_hat, ub]), \"covered\": covered, \"width\": ub-lb}\n\n\ndef plot_rolling_coverage(\n    results: List[dict],\n    alpha: float = 0.1,\n    window: int = 300,\n    rows: int = 2,\n    repeated: bool = False\n):\n    \"\"\"Plot the algorithm's mean coverage over a sliding window\"\"\"\n\n    coverage_df = pd.DataFrame(results)\n    if repeated:\n        coverage_df = (\n            coverage_df\n            .groupby([\"method\", \"dataset\", \"index\"])[\"covered\"]\n            .mean()\n            .reset_index()\n        )\n    coverage_df[\"coverage_mean\"] = (\n        coverage_df\n        .groupby([\"method\", \"dataset\"])[\"covered\"]\n        .transform(lambda x: x.rolling(window=window).mean())\n    )\n    coverage_df[\"time\"] = coverage_df.groupby([\"method\", \"dataset\"]).cumcount() + 1\n    coverage_df = coverage_df.dropna(subset=[\"coverage_mean\"])\n    coverage_plot = (\n        pn.ggplot(\n            coverage_df,\n            pn.aes(x=\"time\", y=\"coverage_mean\", color=\"method\", group=\"method\")\n        )\n        + pn.geom_line()\n        + pn.geom_hline(yintercept=1-alpha, linetype=\"solid\")\n        + pn.scale_y_continuous(limits=(0, 1))\n        + pn.facet_wrap(\"~ dataset\", nrow=rows, scales=\"free\")\n        + pn.theme_538()\n        + pn.labs(x=\"Time\", y=\"Coverage\", color=\"Method\")\n    )\n    return coverage_plot\n\n\ndef plot_rolling_width(\n    results: dict,\n    window: int = 300,\n    rows: int = 2,\n    repeated: bool = False\n):\n    \"\"\"Plot the algorithm's mean prediction interval width over a sliding window\"\"\"\n\n    width_df = pd.DataFrame(results)\n    if repeated:\n        width_df = (\n            width_df\n            .groupby([\"method\", \"dataset\", \"index\"])[\"width\"]\n            .mean()\n            .reset_index()\n        )\n    width_df[\"width_mean\"] = (\n        width_df\n        .groupby([\"method\", \"dataset\"])[\"width\"]\n        .transform(lambda x: x.rolling(window=window).mean())\n    )\n    width_df[\"time\"] = width_df.groupby([\"method\", \"dataset\"]).cumcount() + 1\n    width_df = width_df.dropna(subset=[\"width_mean\"])\n    width_plot = (\n        pn.ggplot(\n            width_df,\n            pn.aes(x=\"time\", y=\"width_mean\", color=\"method\", group=\"method\")\n        )\n        + pn.geom_line()\n        + pn.facet_wrap(\"~ dataset\", nrow=rows, scales=\"free\")\n        + pn.theme_538()\n        + pn.labs(x=\"Time\", y=\"Width\", color=\"Method\")\n    )\n    return width_plot\n\n\n\nElectricity example\nNote: I implement non-exchangeable split CP with least-squares by using WLS and setting all the tags to a uniform value. Standard CP is implemented by setting all the weights to a uniform value as mentioned above.\n\nsplit_fn = lambda x: np.sort(generator.choice(x, int(np.floor(x*0.3)), replace=False))\nresults = []\n\n# Create X and y for the normal and permuted data\nX, y = (electricity.drop(\"transfer\", axis=1), electricity[\"transfer\"].to_numpy())\nX_perm, y_perm = (permuted.drop(\"transfer\", axis=1), permuted[\"transfer\"].to_numpy())\n\n# Predict for each observation from N=100 to N=len(electricity)\nfor i in tqdm(range(100, len(electricity)), total=len(electricity)-100):\n    for method in [\"NexCP+LS\", \"NexCP+WLS\", \"CP+LS\"]:\n        for dataset in [\"Electricity\", \"Permuted\"]:\n            if dataset == \"Electricity\":\n                X_model, y_model = (X, y)\n            else:\n                X_model, y_model = (X_perm, y_perm)\n            if method == \"NexCP+LS\":\n                tag_fn = lambda x: np.array([1.]*(x + 1))\n                weight_fn = lambda x: 0.99**np.arange(x, -1, -1)\n            elif method == \"NexCP+WLS\":\n                tag_fn = lambda x: 0.99**np.arange(x, -1, -1)\n                weight_fn = tag_fn\n            else:\n                tag_fn = lambda x: np.array([1.]*(x + 1))\n                weight_fn = tag_fn\n            out = nexcp_split(\n                model=sm.WLS,\n                split_function=split_fn,\n                y=y_model,\n                X=X_model,\n                tag_function=tag_fn,\n                weight_function=weight_fn,\n                alpha=0.1,\n                test_index=i\n            )\n            out[\"method\"] = method\n            out[\"dataset\"] = dataset\n            out[\"index\"] = i\n            del out[\"ci\"]\n            results.append(out)\n\n\nPlots\ncoverage_plot = plot_rolling_coverage(results, alpha=0.1, window=300)\ncoverage_plot.show()\n\nwidth_plot = plot_rolling_width(results, window=300)\nwidth_plot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\ntable = (\n    pd\n    .DataFrame(results)\n    .groupby([\"method\", \"dataset\"])\n    .mean()\n    .reset_index()\n)\ntable = (\n    table\n    .pivot_table(\n        index='method',\n        columns='dataset',\n        values=['covered', 'width']\n    )\n)\ntable.columns = [f'{col[0]}_{col[1].lower()}' for col in table.columns]\ntable = table.reset_index()\ntable = (\n    GT(table, rowname_col=\"method\")\n    .tab_spanner(\n        label=\"Electricity data\",\n        columns=[\"covered_electricity\", \"width_electricity\"]\n    )\n    .tab_spanner(\n        label=\"Permuted electricity data\",\n        columns=[\"covered_permuted\", \"width_permuted\"]\n    )\n    .fmt_number(\n        columns = [\n            \"covered_electricity\",\n            \"width_electricity\",\n            \"covered_permuted\",\n            \"width_permuted\"\n        ],\n        decimals=3\n    )\n    .cols_label(\n        covered_electricity = \"Coverage\",\n        width_electricity = \"Width\",\n        covered_permuted = \"Coverage\",\n        width_permuted = \"Width\"\n    )\n)\ntable.show()\n\n\n\n\n\n\n\n\nElectricity data\nPermuted electricity data\n\n\nCoverage\nWidth\nCoverage\nWidth\n\n\n\n\nCP+LS\n0.859\n0.558\n0.895\n0.628\n\n\nNexCP+LS\n0.878\n0.581\n0.902\n0.638\n\n\nNexCP+WLS\n0.880\n0.497\n0.895\n0.629\n\n\n\n\n\n\n\n\n\n\n\nSimulated example\nThis demonstrates the conformal prediction algorithm in the following data settings: i.i.d. data, data generating process with changepoints, and data with distribution drift. In the paper they repeat this 200 times to smooth the estimates, but for computational purposes here I only repeated it 50 times.\n\nsplit_fn = lambda x: np.sort(generator.choice(x, int(np.floor(x*0.3)), replace=False))\nresults = []\n\n# Predict for each observation from N=100 to N=len(electricity)\nfor i in tqdm(range(100, 2000), total=2000-100):\n    for rep in range(50):\n        for method in [\"NexCP+LS\", \"NexCP+WLS\", \"CP+LS\"]:\n            for dataset in [\"setting_1\", \"setting_2\", \"setting_3\"]:\n                if dataset == \"setting_1\":\n                    X_model, y_model = sim_data(2000, 4, setting=1)\n                elif dataset == \"setting_2\":\n                    X_model, y_model = sim_data(2000, 4, setting=2)\n                else:\n                    X_model, y_model = sim_data(2000, 4, setting=3)\n                if method == \"NexCP+LS\":\n                    tag_fn = lambda x: np.array([1.]*(x + 1))\n                    weight_fn = lambda x: 0.99**np.arange(x, -1, -1)\n                elif method == \"NexCP+WLS\":\n                    tag_fn = lambda x: 0.99**np.arange(x, -1, -1)\n                    weight_fn = tag_fn\n                else:\n                    tag_fn = lambda x: np.array([1.]*(x + 1))\n                    weight_fn = tag_fn\n                out = nexcp_split(\n                    model=sm.WLS,\n                    split_function=split_fn,\n                    y=y_model,\n                    X=X_model,\n                    tag_function=tag_fn,\n                    weight_function=weight_fn,\n                    alpha=0.1,\n                    test_index=i\n                )\n                out[\"method\"] = method\n                out[\"dataset\"] = dataset\n                out[\"index\"] = i\n                del out[\"ci\"]\n                results.append(out)\n\n\nPlots\ncoverage_plot = plot_rolling_coverage(\n    results,\n    alpha=0.1,\n    window=10,\n    rows=3,\n    repeated=True\n)\ncoverage_plot.show()\n\nwidth_plot = plot_rolling_width(results, window=10, rows=3, repeated=True)\nwidth_plot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\ntable = (\n    pd\n    .DataFrame(results)\n    .groupby([\"method\", \"dataset\", \"index\"])\n    .mean()\n    .reset_index()\n    .drop(labels=[\"index\"], axis=1)\n    .groupby([\"method\", \"dataset\"])\n    .mean()\n    .reset_index()\n)\ntable = (\n    table\n    .pivot_table(\n        index=\"method\",\n        columns=\"dataset\",\n        values=[\"covered\", \"width\"]\n    )\n)\ntable.columns = [f\"{col[0]}_{col[1].lower()}\" for col in table.columns]\ntable = table.reset_index()\ntable = (\n    GT(table, rowname_col=\"method\")\n    .tab_spanner(\n        label=\"Setting 1 (i.i.d. data)\",\n        columns=[\"covered_setting_1\", \"width_setting_1\"]\n    )\n    .tab_spanner(\n        label=\"Setting 2 (changepoints)\",\n        columns=[\"covered_setting_2\", \"width_setting_2\"]\n    )\n    .tab_spanner(\n        label=\"Setting 3 (drift)\",\n        columns=[\"covered_setting_3\", \"width_setting_3\"]\n    )\n    .fmt_number(\n        columns = [\n            \"covered_setting_1\",\n            \"width_setting_1\",\n            \"covered_setting_2\",\n            \"width_setting_2\",\n            \"covered_setting_3\",\n            \"width_setting_3\"\n        ],\n        decimals=3\n    )\n    .cols_label(\n        covered_setting_1 = \"Coverage\",\n        width_setting_1 = \"Width\",\n        covered_setting_2 = \"Coverage\",\n        width_setting_2 = \"Width\",\n        covered_setting_3 = \"Coverage\",\n        width_setting_3 = \"Width\"\n    )\n)\ntable.show()\n\n\n\n\n\n\n\n\nSetting 1 (i.i.d. data)\nSetting 2 (changepoints)\nSetting 3 (drift)\n\n\nCoverage\nWidth\nCoverage\nWidth\nCoverage\nWidth\n\n\n\n\nCP+LS\n0.899\n3.325\n0.832\n6.022\n0.836\n3.754\n\n\nNexCP+LS\n0.898\n3.324\n0.874\n6.692\n0.880\n4.208\n\n\nNexCP+WLS\n0.897\n3.403\n0.896\n4.114\n0.897\n3.440"
  },
  {
    "objectID": "blog/posts/conjoint_analysis/index.html",
    "href": "blog/posts/conjoint_analysis/index.html",
    "title": "Anytime-valid Inference on the AMCE in Conjoint Experiments",
    "section": "",
    "text": "In this post, I demonstrate how to estimate Average Marginal Component Effects (AMCE) in a conjoint experiment using both a fixed‚Äën approach and a sequential, anytime‚Äëvalid approach.\nFor background, see this super-detailed discussion of conjoint analysis and this guide on marginal effects by Andrew Heiss."
  },
  {
    "objectID": "blog/posts/conjoint_analysis/index.html#candidate-experiment-setup",
    "href": "blog/posts/conjoint_analysis/index.html#candidate-experiment-setup",
    "title": "Anytime-valid Inference on the AMCE in Conjoint Experiments",
    "section": "Candidate Experiment ‚Äì Setup",
    "text": "Candidate Experiment ‚Äì Setup\nOur analysis is based on the candidate experiment from Hainmueller et al.‚Äôs Causal Inference in Conjoint Analysis. Below is the paper‚Äôs description:\n\nThe choice between competing candidates for elected office is central to democracy. Candidates typically differ on a variety of dimensions, including their personal background and demographic characteristics, issue positions, and prior experience. The centrality of partisanship to voter decision making is amply documented [ref], so we focus here on the less-examined role of candidates‚Äô personal traits [ref]. Within the United States, there is constant speculation about the role of candidates‚Äô personal backgrounds in generating support or opposition; here, we harness conjoint analysis to examine those claims.  ¬†¬†¬†¬†¬†¬†We focus on eight attributes of would-be presidential candidates, all of which have emerged in recent campaigns. Six of these attributes can take on one of six values, including the candidates‚Äô religion (Catholic, Evangelical Protestant, Mainline Protestant, Mormon, Jewish, or None), college education (no college, state university, community college, Baptist college, Ivy League college, or small college), profession (lawyer, high school teacher, business owner, farmer, doctor, or car dealer), annual income ($32K, $54K, $65K, $92K, $210K, and $5.1M), racial/ethnic background (Hispanic, White, Caucasian, Black, Asian American, and Native American), and age (36, 45, 52, 60, 68, and 75). 3 Two other attributes take on only two values: military service (served or not) and gender (male or female). Each respondent to our online survey‚Äîadministered through Amazon‚Äôs Mechanical Turk [ref]‚Äîsaw six pairs of profiles that were generated using the fully randomized approach described below. Figure A.1 in the Supplemental Information (SI) illustrates one choice presented to one respondent. The profiles were presented side-by-side, with each pair of profiles on a separate screen. To ease the cognitive burden for respondents while also minimizing primacy and recency effects, the attributes were presented in a randomized order that was fixed across the six pairings for each respondent.  ¬†¬†¬†¬†¬†¬†On the same screen as each candidate pairing, respondents were asked multiple questions which serve as dependent variables. First, they were asked to choose between the two candidates, a ‚Äúforced-choice‚Äù design that enables us to evaluate the role of each attribute value in the assessment of one profile relative to another. This question closely resembles real-world voter decision making, in which respondents must cast a single ballot between competing candidates who vary on multiple dimensions. In the second and third questions following the profiles, the respondents rated each candidate on a one to seven scale, enabling evaluations of the levels of absolute support or opposition to each profile separately."
  },
  {
    "objectID": "blog/posts/conjoint_analysis/index.html#candidate-experiment-fixed-n-analysis",
    "href": "blog/posts/conjoint_analysis/index.html#candidate-experiment-fixed-n-analysis",
    "title": "Anytime-valid Inference on the AMCE in Conjoint Experiments",
    "section": "Candidate Experiment ‚Äì Fixed-N Analysis",
    "text": "Candidate Experiment ‚Äì Fixed-N Analysis\nThe data for the candidate experiment is publically available here or here. Let‚Äôs begin by loading the necessary packages and the data:\n\nData Prep\n\n\nCode\nlibrary(dplyr)\nlibrary(fixest)\nlibrary(forcats)\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(haven)\nlibrary(marginaleffects)\nlibrary(scales)\nlibrary(tibble)\nlibrary(tidyr)\n\ncandidate &lt;- read_dta(\"/Users/dmolitor/Downloads/candidate.dta\")\n\n# Make the variable names nice and readable\nvariable_lookup &lt;- tribble(\n  ~variable,    ~variable_nice,\n  \"atmilitary\", \"Military\",\n  \"atreligion\", \"Religion\",\n  \"ated\",       \"Education\",\n  \"atprof\",     \"Profession\",\n  \"atmale\",     \"Gender\",\n  \"atinc\",      \"Income\",\n  \"atrace\",     \"Race\",\n  \"atage\",      \"Age\"\n)\n\ncandidate &lt;- as_factor(candidate) |&gt;\n  rename(respondent = resID) |&gt;\n  rename_with(\n    .cols = atmilitary:atmale,\n    .fn = \\(x) vapply(\n      x,\n      \\(y) filter(variable_lookup, variable == y)$variable_nice,\n      character(1)\n    )\n  )\n\nglimpse(candidate)\n\n\nEach row in the dataset represents one candidate profile out of 1,733 candidate comparisons made by 311 survey respondents. Each of the candidate comparisons has 2 candidate profiles to choose between, so the total number of profiles in the data is 3,466 (1,733 x 2). The selected column is a binary outcome indicating whether the survey respondent (identified in the respondent column) selected that profile as the preferred candidate.\n\n\nEstimating AMCEs\nThe primary estimand of interest is the Average Marginal Component Effect (AMCE). The AMCE measures the average change in the outcome when an attribute shifts from a baseline level to an alternative, averaging over the joint distribution other attributes. In potential outcomes notation:\n\n\\(Y_i(X_j=x_j,X_{‚àíj})\\) is the potential outcome for respondent \\(i\\) when attribute \\(j\\) is set to \\(X_j=x_j\\)‚Äã and other attributes \\(X_{-j}\\)‚Äã‚Äã vary.\n\\(X_j^*\\) is the baseline (or reference) level for attribute \\(j\\).\n\nThen, \\[\\text{AMCE}(X_j)=E_{X_{‚àíj}}[Y_i(X_j=x_j,X_{‚àíj})‚àíY_i(X_j=X_j^‚àó, X_{‚àíj})],\\] where expectation \\(E_{X_{-j}}\\)‚Äã‚Äã is taken over the joint distribution of the other attributes.\nWe can estimate the AMCEs for all attributes simultaneously via a regression model that includes all attribute dummy variables. Note: Cluster standard errors at the respondent level since each respondent provides multiple observations.\n\n\nCode\n# Function to add the reference level for each of our attributes\nadd_reference &lt;- function(data, low = \"conf.low\", high = \"conf.high\") {\n  data &lt;- data |&gt;\n    separate(col = \"contrast\", into = c(\"contrast\", \"reference\"), sep = \" - \")\n  data &lt;- bind_rows(\n    data,\n    data |&gt;\n      mutate(contrast = reference) |&gt;\n      distinct(term, contrast, .keep_all = TRUE) |&gt;\n      mutate(across(c(estimate, {{low}}, {{high}}), ~ 0))\n  )\n  return(data)\n}\n\n# Estimate AMCEs with a logistic regression model\nmodel &lt;- feglm(\n  selected ~ Military + Religion + Education + Profession + Income + Race + Age\n    + Gender,\n  data = candidate,\n  family = \"binomial\",\n  cluster = ~ respondent\n)\n\n# Calculate the marginal effects (AMCEs) as probabilities\nmarginal_effects &lt;- avg_slopes(model, newdata = \"mean\")\n\nggplot(\n  add_reference(marginal_effects),\n  aes(\n    x = estimate,\n    y = contrast,\n    xmin = conf.low,\n    xmax = conf.high,\n    color = term\n  )\n) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nEstimating Marginal Means\nAnother estimand of interest is a purely descriptive quantity‚Äîmarginal means. The marginal mean for a particular value \\(x_j\\)‚Äã of attribute \\(j\\) is: \\[\\mu(x_j) := E_{X_{-j}}[Y | X_j=x_j],\\] where \\(Y\\) are the observed outcomes, \\(X_j\\)‚Äã is the attribute of interest, and the expectation \\(E\\) is taken over the joint distribution of all other attributes \\(X_{-j}\\)‚Äã.\nWe could estimate this non-parametrically (using group_by() and summarize()) or via regression which will give us corresponding CIs, p-values, etc. in one fell swoop.\n\n\nCode\n# A function to calculate the marginal mean for a single attribute\nmarginal_mean &lt;- function(model, attribute) {\n  mms &lt;- avg_predictions(model, \"balanced\", by = attribute)\n  mms &lt;- mms |&gt;\n    rename(\"level\" = {{attribute}}) |&gt;\n    mutate(\"term\" = {{attribute}})\n  return(mms)\n}\n\nmarginal_means &lt;- bind_rows(\n  lapply(\n    c(\n      \"Military\",\n      \"Religion\",\n      \"Education\",\n      \"Profession\",\n      \"Income\",\n      \"Race\",\n      \"Age\",\n      \"Gender\"\n    ),\n    function(attribute) marginal_mean(model, attribute)\n  )\n)\n\nggplot(\n  marginal_means,\n  aes(\n    x = estimate,\n    y = level,\n    xmin = conf.low,\n    xmax = conf.high,\n    color = term\n  )\n) +\n  geom_vline(xintercept = 0.5, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(x = \"Marginal Means\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/posts/conjoint_analysis/index.html#candidate-experiment-sequential-analysis",
    "href": "blog/posts/conjoint_analysis/index.html#candidate-experiment-sequential-analysis",
    "title": "Anytime-valid Inference on the AMCE in Conjoint Experiments",
    "section": "Candidate Experiment ‚Äì Sequential Analysis",
    "text": "Candidate Experiment ‚Äì Sequential Analysis\nFixed‚Äë\\(N\\) estimators provide valid inference at a single predetermined sample size \\(N\\) (e.g., 95% confidence intervals are valid only at a single \\(N\\)). Typically, researchers will utilize power calculations to determine sufficient sample sizes to detect treatment effects of interest prior to running an experiment. However, power calculations rely on unverifiable assumptions which can cause them to be inaccurate and difficult to perform, particularly in the context of conjoint experiments. Instead of relying on power calculations, researchers often would like to monitor statistical results as data accumulate and stop once their estimates become statistically significant. However, if we repeatedly calculate our AMCE estimates and fixed-\\(N\\) 95% CIs and collect more data until they are statistically significant, our probability of committing a Type 1 error will far exceed the nominal significance level (\\(\\alpha = 0.05\\)).\nAnytime‚Äëvalid inference offers a solution by yielding p‚Äëvalues and confidence sequences (CSs) that remain valid even if we ‚Äúpeek‚Äù at the data and stop the experiment once our estimates are statistically significant. In this blog post I explore methods by Lindon et al.¬†that develop anytime-valid p-values and CSs for linear regression models. These methods adapt standard regression outputs (e.g., from lm() in R) to produce anytime‚Äëvalid p‚Äëvalues and CSs.\n\nEstimating Anytime-Valid p-values and Confidence Sequences\nBelow I will demonstrate how we can adapt our code for estimating AMCES to output anytime-valid CSs and p-values instead of the usual fixed-n CIs and p-values. Check the full code for calculating anytime-valid CSs here.\n\n## Same code as before\n\n# Estimate AMCEs with a logistic regression model\nmodel &lt;- feglm(\n  selected ~ Military + Religion + Education + Profession + Income + Race + Age\n    + Gender,\n  data = candidate,\n  family = \"binomial\",\n  cluster = ~ respondent\n)\n\n# Calculate the marginal effects (AMCEs) as probabilities\nmarginal_effects &lt;- avg_slopes(model, newdata = \"mean\")\n\n## Calculate anytime-valid p-values and CSs\n\nmarginal_effects_sequential &lt;- sequential_f_cs(\n  delta = marginal_effects$estimate,\n  se = marginal_effects$std.error,\n  n = model$nobs,\n  n_params = model$nparams,\n  Z = solve(get_vcov(marginal_effects)),\n  term = marginal_effects$term,\n  contrast = marginal_effects$contrast\n)\n\n\n\nEstimating sequential AMCEs\nWe begin by writing a function that will simulate a random respondent from a conjoint experiment with specified AMCEs. Our simulated conjoint experiment will have a binary outcome and two attributes (Attribute 1 with 3 levels, Attribute 2 with 5 levels). The true AMCEs are:\n\n\n\nAttribute\nLevel\nTrue AMCE\n\n\n\n\n1\n2\n0.2\n\n\n1\n3\n0.1\n\n\n2\n2\n-0.1\n\n\n2\n3\n0.15\n\n\n2\n4\n0.3\n\n\n2\n5\n-0.2\n\n\n\n\n\nCode\n# Function to simulate a random survey participant\nsimulate_profile &lt;- function(amce_attr1, amce_attr2, intercept = 0.5) {\n  attr1_levels &lt;- c(\"Level 1\", names(amce_attr1))\n  attr2_levels &lt;- c(\"Level 1\", names(amce_attr2))\n  # Randomly sample a level for each attribute (uniformly)\n  attr1_val &lt;- sample(attr1_levels, 1)\n  attr2_val &lt;- sample(attr2_levels, 1)\n  # Determine the effect for attribute 1: baseline gets effect 0\n  effect1 &lt;- if (attr1_val == \"Level 1\") 0 else amce_attr1[attr1_val]\n  # Determine the effect for attribute 2: baseline gets effect 0\n  effect2 &lt;- if (attr2_val == \"Level 1\") 0 else amce_attr2[attr2_val]\n  # Compute the latent probability p\n  p &lt;- intercept + effect1 + effect2\n  # Ensure that p is within [0, 1]\n  stopifnot(p &gt;= 0 && p &lt;= 1)\n  # Simulate the binary outcome using p as the success probability.\n  outcome &lt;- rbinom(1, size = 1, prob = p)\n  return(tibble(\n    attr1 = attr1_val,\n    attr2 = attr2_val,\n    outcome = outcome,\n    p = p\n  ))\n}\n\n# Specify the attribute-level AMCEs for attribute 1 & 2\namce_attr1 &lt;- c(\"Level 2\" = 0.2, \"Level 3\" = 0.1)\namce_attr2 &lt;- c(\"Level 2\" = -0.1, \"Level 3\" = 0.15, \"Level 4\" = 0.3, \"Level 5\" = -0.2)\n\n\nWe simulate 100 survey participants initially and update our AMCE estimates and anytime-valid 95% CSs with each additional participant until reaching \\(N=1000\\).\n\n\nCode\n# \"Gather\" 100 survey responses before estimating\nsim_data &lt;- bind_rows(lapply(1:100, \\(i) simulate_profile(amce_attr1, amce_attr2, 0.3)))\n# Initialize tibble to collect our AMCE estimates\nsim_estimates &lt;- tibble()\n\n# Simulate a sequential conjoint with total N=1000\npb &lt;- txtProgressBar(min = 1, max = 1000, style = 3)\nfor (i in 1:1000) {\n  # Randomly simulate a survey participant\n  sim_data &lt;- bind_rows(\n    sim_data,\n   simulate_profile(amce_attr1, amce_attr2, 0.3)\n  )\n  # Estimate our model\n  sim_model &lt;- feglm(\n    outcome ~ attr1 + attr2,\n    data = sim_data,\n    family = \"binomial\"\n  )\n  # Calculate marginal effects\n  marginal_eff_sim &lt;- avg_slopes(sim_model, newdata = \"mean\")\n  # Calculate sequential p-values and CSs\n  marginal_eff_sim_seq &lt;- sequential_f_cs(\n    delta = marginal_eff_sim$estimate,\n    se = marginal_eff_sim$std.error,\n    n = sim_model$nobs,\n    n_params = sim_model$nparams,\n    Z = solve(get_vcov(marginal_eff_sim)),\n    term = marginal_eff_sim$term,\n    contrast = marginal_eff_sim$contrast\n  ) |&gt;\n    add_reference(low = \"cs_lower\", high = \"cs_upper\") |&gt;\n    mutate(iter = i)\n  # Append data\n  sim_estimates &lt;- bind_rows(sim_estimates, marginal_eff_sim_seq)\n  setTxtProgressBar(pb, i)\n}\nclose(pb)\n\n\nFinally, we plot the progression of the AMCE estimates and CSs across the entire experiment.\n\n\nCode\ntruth &lt;- tribble(\n  ~term, ~contrast, ~estimate,\n  \"attr1\", \"Level 2\", 0.2,\n  \"attr1\", \"Level 3\", 0.1,\n  \"attr2\", \"Level 2\", -0.1,\n  \"attr2\", \"Level 3\", 0.15,\n  \"attr2\", \"Level 4\", 0.3,\n  \"attr2\", \"Level 5\", -0.2\n)\n\n# Plot the CSs over time\nplot_anim &lt;- ggplot(\n  sim_estimates,\n    aes(\n      x = estimate,\n      y = contrast,\n      xmin = cs_lower,\n      xmax = cs_upper,\n      color = term\n    )\n  ) +\n  geom_point(\n    data = truth,\n    aes(x = estimate, y = contrast),\n    inherit.aes = FALSE,\n    shape = 17,\n    size = 2\n  ) +\n  geom_vline(xintercept = 0, color = \"gray70\") +\n  geom_point() +\n  geom_linerange() +\n  facet_wrap(~ term, ncol = 1, scales = \"free_y\", drop = TRUE) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(xlim = c(-.5, .5)) +\n  labs(x = \"Estimated AMCE (percentage points)\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  transition_states(\n    iter,\n    transition_length = 3,\n    state_length = 0\n  ) +\n  ease_aes()\n\n# Turn this into a gif\nanimation_gif &lt;- animate(\n  plot_anim,\n  renderer = gifski_renderer(),\n  nframes = 300,\n  height = 4,\n  width = 4,\n  units = \"in\",\n  res = 200\n)\n\n\n\n\n\nWe see our estimates and CSs converge towards the true AMCEs (marked by diamond-shaped points) over time, maintaining valid coverage despite estimating AMCEs and corresponding p-values and CSs at each step!"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Daniel Molitor",
    "section": "",
    "text": "Delivering Unemployment Assistance in Times of Crisis: Scalable Cloud Solutions Can Keep Essential Government Programs Running and Supporting Those in Need. Mintaka Angell, et al.¬†(2020). Digital Government: Research and Practice.\n\n\nAbstract\n\n\nThe COVID-19 public health emergency caused widespread economic shutdown and unemployment. The resulting surge in Unemployment Insurance claims threatened to overwhelm the legacy systems state workforce agencies rely on to collect, process, and pay claims. In Rhode Island, we developed a scalable cloud solution to collect Pandemic Unemployment Assistance claims as part of a new program created under the Coronavirus Aid, Relief and Economic Security Act to extend unemployment benefits to independent contractors and gig-economy workers not covered by traditional Unemployment Insurance. Our new system was developed, tested, and deployed within 10 days following the passage of the Coronavirus Aid, Relief and Economic Security Act, making Rhode Island the first state in the nation to collect, validate, and pay Pandemic Unemployment Assistance claims. A cloud-enhanced interactive voice response system was deployed a week later to handle the corresponding surge in weekly certifications for continuing unemployment benefits. Cloud solutions can augment legacy systems by offloading processes that are more efficiently handled in modern scalable systems, reserving the limited resources of legacy systems for what they were originally designed. This agile use of combined technologies allowed Rhode Island to deliver timely Pandemic Unemployment Assistance benefits with an estimated cost savings of $502,000 (representing a 411% return on investment)."
  },
  {
    "objectID": "research/index.html#publications",
    "href": "research/index.html#publications",
    "title": "Daniel Molitor",
    "section": "",
    "text": "Delivering Unemployment Assistance in Times of Crisis: Scalable Cloud Solutions Can Keep Essential Government Programs Running and Supporting Those in Need. Mintaka Angell, et al.¬†(2020). Digital Government: Research and Practice.\n\n\nAbstract\n\n\nThe COVID-19 public health emergency caused widespread economic shutdown and unemployment. The resulting surge in Unemployment Insurance claims threatened to overwhelm the legacy systems state workforce agencies rely on to collect, process, and pay claims. In Rhode Island, we developed a scalable cloud solution to collect Pandemic Unemployment Assistance claims as part of a new program created under the Coronavirus Aid, Relief and Economic Security Act to extend unemployment benefits to independent contractors and gig-economy workers not covered by traditional Unemployment Insurance. Our new system was developed, tested, and deployed within 10 days following the passage of the Coronavirus Aid, Relief and Economic Security Act, making Rhode Island the first state in the nation to collect, validate, and pay Pandemic Unemployment Assistance claims. A cloud-enhanced interactive voice response system was deployed a week later to handle the corresponding surge in weekly certifications for continuing unemployment benefits. Cloud solutions can augment legacy systems by offloading processes that are more efficiently handled in modern scalable systems, reserving the limited resources of legacy systems for what they were originally designed. This agile use of combined technologies allowed Rhode Island to deliver timely Pandemic Unemployment Assistance benefits with an estimated cost savings of $502,000 (representing a 411% return on investment)."
  },
  {
    "objectID": "research/index.html#pre-prints",
    "href": "research/index.html#pre-prints",
    "title": "Daniel Molitor",
    "section": "Pre-prints",
    "text": "Pre-prints\nAdaptive Randomization in Conjoint Survey Experiments. Jennah Gosciak, Daniel Molitor, Ian Lundberg. SocArXiv.\n\n\nAbstract\n\n\nHuman choices are often multi-dimensional. For example, a person deciding which of two immigrants is more worthy of admission to a country might weigh the prospective immigrants‚Äô education, age, country of origin, and employment history. Conjoint experiments have rapidly generated new insight into these multidimensional choices. By independently randomizing the attributes of a pair of fictitious profiles, researchers summarize the average contribution that each attribute makes to an overall choice. But what if the effect of one attribute depends on the values of other attributes? We present a method that uses data-adaptive experimentation to search for heterogeneity in the effect of one focal attribute as a function of all other attributes. Our empirical application of this method shows that U.S. adults weigh the education of an immigrant much more heavily for certain immigrants than for others. By targeting the heterogeneous effects of a focal attribute, our approach complements conjoint designs that target the average effects of all attributes.\n\n\nThe Causal Effect of Parent Occupation on Child Occupation: A Multivalued Treatment with Positivity Constraints. Ian Lundberg, Jennie Brand, Daniel Molitor. Conditionally accepted, Sociological Methods and Research.\n\n\nAbstract\n\n\nContemporary social mobility research often adopts an ostensibly descriptive goal: to document associations between parent and child socioeconomic outcomes and their variation over time and place. To complement descriptive research, we adopt a causal goal: to estimate the degree to which parent occupation causes child occupation. We formalize this causal goal in the potential outcomes framework to define precise counterfactuals. We highlight a difficulty connected to the positivity assumption for causal inference: when the treatment is parent occupation, many counterfactuals never happen in observed data. Parents without college degrees are never employed as physicians, for instance. We show how to select causal estimands involving only the counterfactuals that can be studied with data. We demonstrate our approach using the National Longitudinal Survey of Youth 1979. Our causal approach points to open questions about how specific aspects of family background, such as parent occupation, causally shape the life chances of children.\n\n\nEstimating Value-added Returns to Labor Training Programs with Causal Machine Learning. Mintaka Angell, et al.¬†OSF Pre-prints.\n\n\nAbstract\n\n\nThe mismatch between the skills that employers seek and the skills that workers possess will increase substantially as demand for technically skilled workers accelerates. Skill mismatches disproportionately affect low-income workers and those within industries where relative demand growth for technical skills is strongest. As a result, much emphasis is placed on reskilling workers to ease transitions into new careers. However, utilization of training programs may be sub-optimal if workers are uncertain about the returns to their investment in training. While the U.S. spends billions of dollars annually on reskilling programs and unemployment insurance, there are few measures of program effectiveness that workers or government can use to guide training investment and ensure valuable reskilling outcomes. We demonstrate a causal machine learning method for estimating the value-added returns to training programs in Rhode Island, where enrollment increases future quarterly earnings by $605 on average, ranging from -$1,570 to $3,470 for individual programs. In a nationwide survey (N=2,014), workers prefer information on the value-added returns to earnings following training enrollment, establishing the importance of our estimates for guiding training decisions. For every 10% increase in expected earnings, workers are 17.4% more likely to express interest in training. State and local governments can provide this preferred information on value-added returns using our method and existing administrative data."
  },
  {
    "objectID": "research/index.html#works-in-progress",
    "href": "research/index.html#works-in-progress",
    "title": "Daniel Molitor",
    "section": "Works in Progress",
    "text": "Works in Progress\nAnytime-Valid Inference in Conjoint Experiments. Daniel Molitor.\n\n\nAbstract\n\n\nConjoint experiments have become increasingly popular for studying how multiple attributes influence decision-making. However, determining the optimal sample size required to achieve adequate statistical power in conjoint experiments is challenging; conventional power analysis requires many assumptions to hold simultaneously, and can easily under- or over-estimate the necessary sample size. To overcome these limitations, I propose an alternative approach grounded in recent advances in anytime-valid inference. Rather than relying on conventional power analysis, this approach introduces anytime-valid confidence sequences (CSs) and corresponding p-values for key conjoint estimands, including the AMCE, ACIE, and marginal means. These procedures are computationally simple‚Äîbuilding on standard regression outputs, guarantee valid Type I error control at any stopping point, and enable practitioners to continuously monitor their empirical estimates and implement data-driven stopping rules once their estimates of interest achieve sufficient statistical power or precision. In simulations calibrated to real-world conjoint studies, I show that this approach preserves nominal coverage, achieves comparable power to standard fixed-n approaches, and yields average sample savings of 10‚Äì40% across a broad range of effect sizes, sample sizes, and attribute levels. This approach gives practitioners a principled, efficient way to determine when to stop data collection without relying on pre-specified power analyses.\n\n\nBalancing Power and Efficiency in Adaptive Experiments. Daniel Molitor, Samantha Gold.\n\n\nAbstract\n\n\nThis paper introduces the Modified Mixture Adaptive Design (MADMod), an experimental design for conducting adaptive experiments with multiple treatment arms that addresses both efficiency and robust inference. Building on the Mixture Adaptive Design (MAD) framework, MADMod retains anytime-valid confidence sequences‚Äîallowing experiments to conclude dynamically‚Äîwhile systematically reallocating samples to ensure each arm achieves sufficient statistical power. By integrating importance weights that decay once an arm‚Äôs average treatment effect is detected as significant, MADMod prevents under-allocation to suboptimal arms without sacrificing the efficiency gains of multi-armed bandits. Simulation results demonstrate that, compared to standard adaptive designs, MADMod substantially reduces Type 2 error rates and offers more precise inference across all treatments. This allows researchers to harness the benefits of adaptive sampling and early stopping while maintaining well-powered evaluations of every treatment arm."
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html",
    "href": "blog/posts/robust_adaptive_exp/index.html",
    "title": "Robust Adaptive Experiments",
    "section": "",
    "text": "Recently I‚Äôve been thinking about how to design adaptive experiments that enable valid inference on treatment effects while maintaining sufficient power to detect nonzero effects across treatment arms (including sub-optimal arms). To explore this, I will run simulations demonstrating how we can achieve these goals. Specifically, I extend the Mixture Adaptive Design (MAD) (Liang & Bojinov, 2024) to produce an adaptive experiment with the following properties:"
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#introducing-the-mad",
    "href": "blog/posts/robust_adaptive_exp/index.html#introducing-the-mad",
    "title": "Robust Adaptive Experiments",
    "section": "Introducing the MAD",
    "text": "Introducing the MAD\nThe MAD combines Bernoulli randomization with arbitrary multi-armed bandit (MAB) algorithms, enabling unbiased ATE estimation with anytime-valid confidence sequences (CSs).\nTo illustrate its usefulness, consider a simple experiment with one control and one treatment arm. Outcomes are sampled as follows:\n\nControl arm: Y ‚àº Bernoulli(\\(\\theta\\)=0.5)\nTreatment arm: Y‚àºBernoulli(\\(\\theta\\)=0.6)\nTrue ATE: 0.1\n\nWe use Thompson Sampling (TS) as the bandit algorithm and stop the experiment as soon as the ATE reaches statistical significance.\n\n\nShow the code\ngenerator = np.random.default_rng(seed=123)\n\ndef reward_fn(arm: int) -&gt; float:\n    values = {\n        0: generator.binomial(1, 0.5),\n        1: generator.binomial(1, 0.6)  # ATE = 0.1\n    }\n    return values[arm]\n\nexp_simple = MAD(\n    bandit=TSBernoulli(k=2, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(30e3)\n)\nexp_simple.fit(cs_precision=0, verbose=False)\n\n\nFinally, we plot the MAD-estimated ATE over time, showing convergence to the true effect and demonstrating that the corresponding 95% CSs maintain valid coverage.\n\n\nShow the code\n(\n    exp_simple.plot_ate_path()\n    + pn.coord_cartesian(ylim=(-.5, 1.5))\n    + pn.geom_hline(\n        mapping=pn.aes(yintercept=\"ate\", color=\"factor(arm)\"),\n        data=pd.DataFrame({\"arm\": list(range(1, 2)), \"ate\": [0.1]}),\n        linetype=\"dotted\"\n    )\n    + pn.theme(strip_text=pn.element_blank()) \n)\n\n\n\n\n\n\n\n\n\n\nBandit benefits\nThe underlying bandit algorithm provides additional benefits. Below, we show the total sample size assigned to both arms of the experiment:\n\n\nShow the code\nexp_simple.plot_n()\n\n\n\n\n\n\n\n\n\nand the arm assignment probability over time:\n\n\nShow the code\nexp_simple.plot_probabilities()\n\n\n\n\n\n\n\n\n\nThe TS algorithm assigns the majority of the sample to the optimal arm (Arm 1 is the treatment). This demonstrates how we can achieve both valid ATE inference and reward maximization with the bandit algorithm.\n\n\nLimitations\nIn adaptive experiments with multiple treatment arms, a common issue is being under-powered to detect non-zero ATEs in sub-optimal arms. This happens because the bandit algorithm allocates most of the sample to the optimal arm(s), neglecting the others.\nWe demonstrate this with an experiment simulating a control arm and four treatment arms with ATEs of 0.1, 0.12, 0.3, and 0.32, respectively, over a fixed sample size of 20,000. We expect the bandit algorithm to allocate most of the sample to arms 3 and 4, leaving arms 1 and 2 under-powered.\n\n\nShow the code\ndef reward_fn(arm: int) -&gt; float:\n    values = {\n        0: generator.binomial(1, 0.5),  # Control arm\n        1: generator.binomial(1, 0.6),  # ATE = 0.1\n        2: generator.binomial(1, 0.62), # ATE = 0.12\n        3: generator.binomial(1, 0.8),  # ATE = 0.3\n        4: generator.binomial(1, 0.82)  # ATE = 0.32\n    }\n    return values[arm]\n\nexp_complex = MAD(\n    bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(20e3)\n)\nexp_complex.fit(early_stopping=False, verbose=False)\n\nates = pd.concat(\n    [\n        exp_complex.estimates().assign(which=\"mad\"),\n        pd.DataFrame({\n            \"arm\": list(range(1, 5)),\n            \"ate\": [0.1, 0.12, 0.3, 0.32],\n            \"which\": [\"truth\"]*(4)\n        })\n    ],\n    axis=0\n)\n(\n    pn.ggplot(\n        ates,\n        mapping=pn.aes(\n            x=\"factor(arm)\",\n            y=\"ate\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"which\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.3))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.3), width=0.001)\n    + pn.geom_hline(yintercept=0, linetype=\"dashed\", color=\"black\")\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"ATE\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nAs anticipated, we observe strong ATE estimates for arms 3 and 4 but under-powered estimates for arms 1 and 2 (CSs include 0). We can confirm that, indeed, TS focuses the majority of the sample on arms 3 and 4 to the detriment of power in our experiment.\n\n\nShow the code\nexp_complex.plot_n()"
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#mad-modified",
    "href": "blog/posts/robust_adaptive_exp/index.html#mad-modified",
    "title": "Robust Adaptive Experiments",
    "section": "MAD modified",
    "text": "MAD modified\nI propose an extension of the MAD algorithm to address the challenge of inadequate power in sub-optimal arms. For each treatment arm \\(k \\in K\\) and time period \\(t\\), I introduce importance weights \\(w_{tk} \\in [0, 1]\\). Once the estimated ATE for arm \\(k\\) becomes statistically significant, \\(w_{tk}\\) begins to shrink toward zero according to a user-defined function of \\(t\\).\nIn the notation of Liang and Bojinov, let \\(A\\) represent an arbitrary adaptive algorithm. They define \\(p_t^A(k)\\) as the assignment probability for arm \\(k\\) at time \\(t\\) under \\(A\\). By construction, the set \\(p_t^A(k)\\) of adaptive assignment probabilities for all \\(k \\in K\\) forms a valid probability distribution over \\(K\\), meaning \\(\\sum_{k \\in K}{p_t^A(k)}=1\\). I modify these probabilities to \\(g(p_t^A(k))\\) where \\(g\\) re-weights \\(p_t^A(k)\\) based on the importance weight \\(w_{tk}\\).\nFor each treatment arm \\(k \\in K\\) at time \\(t\\), the re-weighted probability \\(g(p_t^A(k))\\) is computed as follows:\n1.) Apply Importance Weights: Each probability is first scaled by its importance weight: \\[p_t^*(k)=w_{tk}*p_t^A(k).\\]\n2.) Compute Lost Probability Mass: The probability mass lost due to down-weighting is: \\[L_t = \\sum_{k \\in K}{p_t^A(k)*(1 - w_{tk})}.\\]\n3.) Compute Relative Redistribution Weights: The total weight sum is: \\[W_t = \\sum_{k \\in K}{w_{tk}}.\\] Each arm‚Äôs share of the remaining mass is: \\[r_{tk} = \\frac{w_{tk}}{W_t}.\\]\n4.) Redistribute Lost Mass: Redistribute the lost mass proportionally to the relative weights: \\[p_t^g(k) = p_t^*(k) + (r_{tk} * L_t).\\]\n5.) Normalization Check: Since \\(p_t^g(k)\\) for all \\(k \\in K\\) forms a valid probability distribution over \\(K\\), it satisfies: \\[\\sum_{k \\in K}p_t^g(k)=1.\\]\nThus, the function \\(g\\) modifies the original assignment probabilities by scaling each by its importance weight and redistributing the lost probability mass in a manner that preserves the total probability sum.\n\nUser-Specified Decay of Importance Weights\nThe importance weight function \\(w_{tk}\\)‚Äã controls how quickly the assignment probability for arm \\(k\\) shrinks once its estimated ATE becomes statistically significant. This user-defined function balances two extremes:\n\n\\(w_{tk}=1\\) for all \\(t\\), which keeps \\(g(p_t^A(k))=p_t^A(k)\\), making the algorithm identical to the original MAD design.\n\\(w_{tk}=0\\) after arm \\(k\\) reaches statistical significance, redirecting all future probability mass away from arm \\(k\\) and prioritizing underpowered arms.\nMore generally, the user defines \\(w_{tk}\\) somewhere in between, where:\n\nA slower decay of \\(w_{tk}\\) (closer to 1) retains more influence from the adaptive algorithm‚Äôs assignment probabilities.\nA faster decay (closer to 0) shifts the algorithm toward prioritizing underpowered arms at the expense of bandit goals (e.g.¬†reward maximization).\n\n\nReasonable choices for \\(w_{tk}\\) include polynomial or exponential decay, providing flexibility in tuning sample reallocation."
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#algorithm-comparison",
    "href": "blog/posts/robust_adaptive_exp/index.html#algorithm-comparison",
    "title": "Robust Adaptive Experiments",
    "section": "Algorithm comparison",
    "text": "Algorithm comparison\nI compare the two algorithms to highlight the benefits of the modified approach. The modified algorithm significantly improves power to detect non-zero ATEs in all treatment arms and provides more precise ATE estimates than the original MAD algorithm with the same sample size. However, this comes at the cost of assigning more sample to sub-optimal arms, where ‚Äúoptimal‚Äù is defined by the underlying bandit algorithm.\n\nImproved power and precision\nThe following plots demonstrate the increased power and precision of the modified MAD algorithm.\n\n\nShow the code\n# Run the modified algorithm\nmad_modified = MADModified(\n    bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(20e3),\n    decay=lambda x: 1./(x**(1./8.))\n)\nmad_modified.fit(cs_precision=0.1, verbose=False, early_stopping=True)\n\n# Run the vanilla algorithm\nmad_vanilla = MAD(\n    bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=mad_modified._bandit._t\n)\nmad_vanilla.fit(verbose=False, early_stopping=False)\n\n# Compare the ATEs and CSs\nates = pd.concat(\n    [\n        mad_modified.estimates().assign(which=\"MADMod\"),\n        mad_vanilla.estimates().assign(which=\"MAD\"),\n        pd.DataFrame({\n            \"arm\": list(range(1, 5)),\n            \"ate\": [0.1, 0.12, 0.3, 0.32],\n            \"which\": [\"Truth\"]*(4)\n        })\n    ],\n    axis=0\n)\n(\n    pn.ggplot(\n        ates,\n        mapping=pn.aes(\n            x=\"factor(arm)\",\n            y=\"ate\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"which\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.3))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.3), width=0.001)\n    + pn.geom_hline(yintercept=0, linetype=\"dashed\", color=\"black\")\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"ATE\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nAnd the following plot compares the sample assignment to the treatment arms of the two algorithms:\n\n\nShow the code\nsample_sizes = pd.concat([\n    pd.DataFrame(x) for x in\n    [\n        {\n            \"arm\": [k for k in range(len(mad_modified._ate))],\n            \"n\": [last(n) for n in mad_modified._n],\n            \"which\": [\"MADMod\"]*len(mad_modified._ate)\n        },\n        {\n            \"arm\": [k for k in range(len(mad_vanilla._ate))],\n            \"n\": [last(n) for n in mad_vanilla._n],\n            \"which\": [\"MAD\"]*len(mad_vanilla._ate)\n        }\n    ]\n])\n(\n    pn.ggplot(sample_sizes, pn.aes(x=\"factor(arm)\", y=\"n\", fill=\"which\", color=\"which\"))\n    + pn.geom_bar(stat=\"identity\", position=pn.position_dodge(width=0.75), width=0.7)\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"N\", color=\"Method\", fill=\"Method\")\n)\n\n\n\n\n\n\n\n\n\n\n\nSimulation results over 1,0000 runs\nWe can more precisely quantify the improvements by running 1,000 simulations, comparing Type 2 error and confidence band width between the vanilla MAD algorithm and the modified algorithm. Each simulation runs for 20,000 iterations with early stopping. If the modified algorithm stops early, the vanilla algorithm will also stop early to maintain equal sample sizes in each simulation.\n\n\nShow the code\ndef delta_fn(x):\n    return 1. / (x ** 0.24)\n\ndef decay_fn(x):\n    return 1. / (x ** (1. / 8.))\n\ndef compare(i):\n    mad_modified = MADModified(\n        bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n        alpha=0.05,\n        delta=delta_fn,\n        t_star=int(2e4),\n        decay=decay_fn\n    )\n    mad_modified.fit(cs_precision=0.1, verbose=False, early_stopping=True)\n\n    # Run the vanilla algorithm\n    mad_vanilla = MAD(\n        bandit=TSBernoulli(k=5, control=0, reward=reward_fn),\n        alpha=0.05,\n        delta=delta_fn,\n        t_star=mad_modified._bandit._t\n    )\n    mad_vanilla.fit(verbose=False, early_stopping=False)\n\n    # Calculate the Type 2 error and the Confidence Sequence width\n\n    ## For modified algorithm\n    mad_mod_n = (\n        pd\n        .DataFrame([\n            {\"arm\": k, \"n\": last(mad_modified._n[k])}\n            for k in range(mad_modified._bandit.k())\n            if k != mad_modified._bandit.control()\n        ])\n        .assign(\n            n_pct=lambda x: x[\"n\"].apply(lambda y: y/np.sum(x[\"n\"]))\n        )\n    )\n    mad_mod_df = (\n        mad_modified\n        .estimates()\n        .assign(\n            idx=i,\n            method=\"modified\",\n            width=lambda x: x[\"ub\"] - x[\"lb\"],\n            error=lambda x: ((0 &gt; x[\"lb\"]) & (0 &lt; x[\"ub\"]))\n        )\n        .merge(mad_mod_n, on=\"arm\", how=\"left\")\n    )\n\n    ## For vanilla algorithm\n    mad_van_n = (\n        pd\n        .DataFrame([\n            {\"arm\": k, \"n\": last(mad_vanilla._n[k])}\n            for k in range(mad_vanilla._bandit.k())\n            if k != mad_vanilla._bandit.control()\n        ])\n        .assign(\n            n_pct=lambda x: x[\"n\"].apply(lambda y: y/np.sum(x[\"n\"]))\n        )\n    )\n    mad_van_df = (\n        mad_vanilla\n        .estimates()\n        .assign(\n            idx=i,\n            method=\"mad\",\n            width=lambda x: x[\"ub\"] - x[\"lb\"],\n            error=lambda x: ((0 &gt; x[\"lb\"]) & (0 &lt; x[\"ub\"]))\n        )\n        .merge(mad_van_n, on=\"arm\", how=\"left\")\n    )\n\n    out = {\n        \"metrics\": pd.concat([mad_mod_df, mad_van_df]),\n        \"reward\": {\n            \"modified\": np.sum(mad_modified._rewards),\n            \"mad\": np.sum(mad_vanilla._rewards)\n        }\n    }\n    return out\n\n# Execute in parallel with joblib\ncomparison_results_list = [\n    x for x in\n    joblib.Parallel(return_as=\"generator\", n_jobs=-1)(\n        joblib.delayed(compare)(i) for i in range(100)\n    )\n]\n\n# Compare performance on key metrics across simulations\nmetrics_df = pd.melt(\n    (\n        pd\n        .concat([x[\"metrics\"] for x in comparison_results_list])\n        .reset_index(drop=True)\n        .assign(error=lambda x: x[\"error\"].apply(lambda y: int(y)))\n    ),\n    id_vars=[\"arm\", \"method\"],\n    value_vars=[\"width\", \"error\", \"n\", \"n_pct\"],\n    var_name=\"meas\",\n    value_name=\"value\"\n)\n\n# Compare reward accumulation across simulations\nreward_df = pd.melt(\n    pd.DataFrame([x[\"reward\"] for x in comparison_results_list]),\n    value_vars=[\"modified\", \"mad\"],\n    var_name=\"method\",\n    value_name=\"reward\"\n)\n\nmetrics_summary = (\n    metrics_df\n    .groupby([\"arm\", \"method\", \"meas\"], as_index=False).agg(\n        mean=(\"value\", \"mean\"),\n        std=(\"value\", \"std\"),\n        n=(\"value\", \"count\")\n    )\n    .assign(\n        se=lambda x: x[\"std\"] / np.sqrt(x[\"n\"]),\n        t_val=lambda x: t.ppf(0.975, x[\"n\"] - 1),\n        ub=lambda x: x[\"mean\"] + x[\"t_val\"] * x[\"se\"],\n        lb=lambda x: x[\"mean\"] - x[\"t_val\"] * x[\"se\"]\n    )\n    .drop(columns=[\"se\", \"t_val\"])\n)\n\n\nThe following plot shows the mean (and 95% confidence intervals) of the Type 2 error and CS width for both algorithms.\n\n\nShow the code\nfacet_labels = {\n    \"error\": \"Type 2 error\",\n    \"width\": \"Interval width\",\n    \"n\": \"Sample size\",\n    \"n_pct\": \"Sample size %\"\n}\n(\n    pn.ggplot(\n        metrics_summary[metrics_summary[\"meas\"].isin([\"error\", \"width\"])],\n        pn.aes(\n            x=\"factor(arm)\",\n            y=\"mean\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"method\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.2))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.2), width=0.01)\n    + pn.facet_wrap(\n        \"~ meas\",\n        labeller=lambda x: facet_labels[x],\n        scales=\"free\"\n    )\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nThe modified MAD algorithm achieves far lower Type 2 error and improved ATE precision in all treatment arms.\n\n\nTradeoffs\nThese plots illustrate the tradeoffs of the modified algorithm. On average, it allocates significantly more sample to sub-optimal arms compared to the standard MAD algorithm.\n\n\nShow the code\n(\n    pn.ggplot(\n        metrics_summary[metrics_summary[\"meas\"].isin([\"n\", \"n_pct\"])],\n        pn.aes(\n            x=\"factor(arm)\",\n            y=\"mean\",\n            ymin=\"lb\",\n            ymax=\"ub\",\n            color=\"method\"\n        )\n    )\n    + pn.geom_point(position=pn.position_dodge(width=0.2))\n    + pn.geom_errorbar(position=pn.position_dodge(width=0.2), width=0.01)\n    + pn.facet_wrap(\n        \"~ meas\",\n        labeller=lambda x: facet_labels[x],\n        scales=\"free\"\n    )\n    + pn.theme_538()\n    + pn.labs(x=\"Arm\", y=\"\", color=\"Method\")\n)\n\n\n\n\n\n\n\n\n\nAs a result, this reallocation reduces total reward accumulation. The difference in accumulated reward across the 1,000 simulations is shown below:\n\n\nShow the code\n(\n    pn.ggplot(reward_df, pn.aes(x=\"method\", y=\"reward\"))\n    + pn.geom_boxplot()\n    + pn.theme_538()\n    + pn.labs(x=\"Method\", y=\"Cumulative reward\")\n)"
  },
  {
    "objectID": "blog/posts/robust_adaptive_exp/index.html#summary",
    "href": "blog/posts/robust_adaptive_exp/index.html#summary",
    "title": "Robust Adaptive Experiments",
    "section": "Summary",
    "text": "Summary\nIn summary, this approach allows us to achieve anytime-valid inference on the ATE, enabling early stopping for greater flexibility and efficiency. It also allows us to ensure dynamic sample allocation, guaranteeing sufficient power for all (or the top n) treatment arms."
  },
  {
    "objectID": "blog/posts/anytime_valid_linear_models/index.html",
    "href": "blog/posts/anytime_valid_linear_models/index.html",
    "title": "Anytime-Valid Regression Adjusted Causal Inference",
    "section": "",
    "text": "Randomized experiments (A/B tests) are ubiquitous in both academia and the tech sector. They enable robust causal inference regarding the impact of interventions on user or participant outcomes. Under randomization, various estimators provide valid estimates of the average treatment effect (ATE), defined as \\(\\tau = E[Y_i(1) - Y_i(0)].\\) These estimators typically yield both point estimates and confidence intervals (CIs) such that \\[P(\\tau \\in \\text{CI}) &lt;= \\alpha,\\] where \\(\\alpha\\) is the significance level and \\(\\text{CI}\\) is a random interval constructed from our data.\nOne major issue with common estimators like ordinary least squares (OLS) is that the statistical guarantees for their confidence intervals only hold for a pre-specified sample size \\(N\\). A surprisingly common practice is to collect some data, estimate \\(\\tau\\) and its corresponding 95% CI, and then, if the effect is large but not statistically significant, collect more data and re-estimate \\(\\tau\\). This process of ‚Äúdata-peeking‚Äù may be repeated multiple times. However, such practices violate the statistical guarantees of standard confidence intervals, causing the probability of a Type I error to far exceed the nominal significance level \\(\\alpha\\).\nTo address this issue, researchers at Netflix have developed anytime-valid Confidence Sequences (CSs) for linear regression parameters. These CSs have the property that \\[P(\\forall n, \\delta \\in \\text{CS}_{\\delta, n}) \\geq 1 - \\alpha,\\] where \\(\\delta\\) represents the regression parameter. Essentially, this means that you can estimate the parameter and update the CS as often as you like, and the probability of committing a Type I error will always remain below \\(\\alpha\\).\nThis method is not only practically significant but also easy to implement using standard linear regression outputs in R. The simulations that follow will demonstrate the usefulness of this approach."
  },
  {
    "objectID": "blog/posts/anytime_valid_linear_models/index.html#simulate-rct",
    "href": "blog/posts/anytime_valid_linear_models/index.html#simulate-rct",
    "title": "Anytime-Valid Regression Adjusted Causal Inference",
    "section": "Simulate RCT",
    "text": "Simulate RCT\nThese simulations will rely on the following packages:\n\nDependencies\n\n\nShow the code\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nData generation and helper functions\nWe simulate a simple randomized controlled trial (RCT) where binary treatment is assigned randomly. The potential outcomes are defined by \\[Y_i(W_i) = 0.5 + 0.5*W_i + 2*X_{i1} + 1.2*X_{i2} + 0.4*X_{i3} + \\epsilon_i\\]\nwhere \\(W_i\\)‚Äã indicates the treatment status of unit \\(i\\), the true average treatment effect (ATE) is \\(\\tau = 0.5\\), and \\(\\epsilon_i \\sim \\text{Normal}(0, 1)\\).\nIn this setting, a simple regression of \\(Y_i\\)‚Äã on an intercept and \\(W_i\\)‚Äã, i.e.¬†\\(Y_i \\sim \\alpha + \\tau * W_i\\)‚Äã, is equivalent to the difference-in-means estimator and produces an unbiased estimate \\(\\hat{\\tau}_{DM}\\) of \\(\\tau\\). Similarly, a covariate-adjusted regression, given by \\(Y_i \\sim \\alpha + \\tau * W_i + \\beta^T*X_i\\), also provides an unbiased estimate \\(\\hat{\\tau}_{CA}\\), but with lower variance than \\(\\hat{\\tau}_{DM}\\).\nFirst, we define a function to simulate a random tuple \\((X_i, W_i, Y_i)\\) according to the RCT setup described above:\n\n\nShow the code\ndraw &lt;- function(arm, ate, sigma = 1.0) {\n  covar_coef &lt;- c(2, 1.2, 0.4)\n  covar &lt;- rbinom(3, size = 1, prob = c(0.1, 0.5, 0.9))\n  y &lt;- 0.5 + ate*arm + drop(covar_coef %*% covar) + rnorm(1, 0, sigma)\n  cbind(data.frame(\"y\" = y, \"t\" = arm), data.frame(t(covar)))\n}\n\n\nNext, for a dataset comprising these units \\(\\{(X_i, W_i, Y_i)\\}_{i=1}^N\\)‚Äã, we construct a function to estimate both \\(\\hat{\\tau}_{DM}\\) and \\(\\hat{\\tau}_{CA}\\)‚Äã, along with their corresponding exact and asymptotic confidence sequences (CSs):\n\n\nShow the code\ncompare &lt;- function(model_ca, model_dm, data, iter, ate = 0.5) {\n  seq_f &lt;- sequential_f_cs(model_ca, phi = 10)\n  seq_f_dm &lt;- sequential_f_cs(model_dm, phi = 10)\n  seq_asymp &lt;- ate_cs_asymp(model_ca, treat_name = \"t\", lambda = 100)\n  seq_asymp_dm &lt;- ate_cs_asymp(\n    model_dm,\n    lambda = 100,\n    treat_name = \"t\"\n  )\n  comparison_df &lt;- data.frame(\n    \"i\" = iter,\n    \"method\" = c(\n      \"f_test\",\n      \"f_test_unadj\",\n      \"asymp\",\n      \"asymp_unadj\"\n    ),\n    \"estimate\" = c(\n      subset(seq_f, covariate == \"t\")$estimate,\n      subset(seq_f_dm, covariate == \"t\")$estimate,\n      seq_asymp$estimate,\n      seq_asymp_dm$estimate\n    ),\n    \"lower\" = c(\n      subset(seq_f, covariate == \"t\")$cs_lower,\n      subset(seq_f_dm, covariate == \"t\")$cs_lower,\n      seq_asymp$cs_lower,\n      seq_asymp_dm$cs_lower\n    ),\n    \"upper\" = c(\n      subset(seq_f, covariate == \"t\")$cs_upper,\n      subset(seq_f_dm, covariate == \"t\")$cs_upper,\n      seq_asymp$cs_upper,\n      seq_asymp_dm$cs_upper\n    )\n  )\n  comparison_df$covered &lt;- (\n    comparison_df$lower &lt;= ate & ate &lt;= comparison_df$upper\n  )\n  return(comparison_df)\n}\n\n\nFinally, we create a function that simulates an experiment of size \\(N\\) where each tuple \\((X_i, W_i, Y_i)\\) is received sequentially. At each sample size \\(n \\leq N\\), we re-estimate \\(\\hat{\\tau}_{DM}\\), \\(\\hat{\\tau}_{CA}\\)‚Äã, and their CSs:\n\n\nShow the code\nsimulate &lt;- function(model_ca_fn, model_dm_fn, draw_fn, n, ate) {\n  # Warm-start with 20 observations so that no regression coefs are NA\n  # at any point\n  df &lt;- do.call(rbind, lapply(1:20, function(x) draw_fn(ate)))\n  estimates &lt;- data.frame()\n  for (i in 1:n) {\n    observation &lt;- draw_fn(ate)\n    df &lt;- rbind(df, observation)\n    model &lt;- model_ca_fn(df)\n    model_dm &lt;- model_dm_fn(df)\n    estimates &lt;- rbind(\n      estimates,\n      compare(model, model_dm, df, iter = i, ate = ate)\n    )\n  }\n  estimates &lt;- estimates |&gt; \n    mutate(\n      stat_sig = 0 &lt; lower | 0 &gt; upper,\n      method = case_when(\n        method == \"asymp\" ~ \"Sequential asymptotic CS\",\n        method == \"asymp_unadj\" ~ \"Sequential asymptotic CS w/o covariates\",\n        method == \"f_test\" ~ \"Sequential CS\",\n        method == \"f_test_unadj\" ~ \"Sequential CS w/o covariates\",\n        method == \"lm\" ~ \"Fixed-N CS\",\n        method == \"lm_unadj\" ~ \"Fixed-N CS w/o covariates\"\n      )\n    ) |&gt;\n    group_by(method) |&gt;\n    mutate(transition = (!lag(stat_sig, default = FALSE)) & stat_sig) |&gt;\n    mutate(\n      stat_sig_i_min = if_else(\n        any(stat_sig & !lag(stat_sig, default = FALSE)),\n        min(i[stat_sig & !lag(stat_sig, default = FALSE)]),\n        NA_integer_\n      ),\n      stat_sig_i_max = if_else(\n        any(transition),\n        max(i[transition]),\n        NA_integer_\n      )  \n    ) |&gt;\n    ungroup()\n  return(estimates)\n}\n\n\n\n\nRun the simulation\nWe then run our simulation for \\(N = 500\\) and plot the estimates \\(\\hat{\\tau}_{DM}\\) and \\(\\hat{\\tau}_{CA}\\)‚Äã along with their CSs at each step. The red dotted line is the true value of \\(\\tau\\) and the vertical blue line indicates the sample size when the corresponding estimator becomes statistically significant.\n\n\nShow the code\nestimates &lt;- simulate(\n  model_ca_fn = function(data) lm(y ~ ., data),\n  model_dm_fn = function(data) lm(y ~ t, data),\n  draw_fn = function(ate) draw(rbinom(1, 1, 0.5), ate = ate),\n  n = 500,\n  ate = 0.5\n)\n\nggplot(estimates, aes(x = i, y = estimate, ymin = lower, ymax = upper)) +\n  geom_line(linewidth = 0.2) +\n  geom_ribbon(alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_vline(aes(xintercept = stat_sig_i_max), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(\n    ~ method,\n    ncol = 2,\n    scales = \"free\",\n  ) +\n  coord_cartesian(ylim = c(-0.5, 2)) +\n  labs(y = \"ATE\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/posts/anytime_valid_linear_models/index.html#simulate-non-fixed-probability-randomized-data-but-known-propensity-scores",
    "href": "blog/posts/anytime_valid_linear_models/index.html#simulate-non-fixed-probability-randomized-data-but-known-propensity-scores",
    "title": "Anytime-Valid Regression Adjusted Causal Inference",
    "section": "Simulate non-fixed-probability randomized data (but known propensity scores)",
    "text": "Simulate non-fixed-probability randomized data (but known propensity scores)\nNext, we simulate a randomized experiment where propensity scores are generated as a known function of covariates. Specifically, the propensity score for unit \\(i\\) is given by \\[e_i = 0.05 + 0.5*Z_{i1} + 0.3*Z_{i2} + 0.1*Z_{i3},\\]‚Äã where \\(\\{Z_{i1}, Z_{i2}, Z_{i3}\\}\\) are binary covariates. The potential outcomes are defined as \\[Y_i(W_i) = 0.5 + 0.5*W_i + 3*X_{i1} + 3*X_{i2} + 1*X_{i3} + 0.5*Z_{i1} + 0.3*Z_{i2} + 0.1*Z_{i3} + \\epsilon_i,\\] with \\(W_i\\)‚Äã representing the treatment status, the true average treatment effect (ATE) \\(\\tau = 0.5\\), and \\(\\epsilon_i \\sim \\text{Normal}(0, 1)\\). In this setup, a simple regression of \\(Y_i\\)‚Äã on an intercept and \\(W_i\\), weighted by the inverse propensity scores \\(\\frac{1}{e_i}\\), yields an unbiased estimate \\(\\hat{\\tau}_{IPW}\\)‚Äã of \\(\\tau\\). Similarly, the covariate-adjusted regression \\(Y_i \\sim \\alpha + \\tau * W_i + \\beta^T*X_i\\), when weighted by the inverse propensity scores, produces an unbiased estimate \\(\\hat{\\tau}_{CAIPW}\\) that typically has lower variance.\nWe then create a function to generate data based on this experimental setup and run our simulation for \\(N = 2000\\). As before, we plot \\(\\hat{\\tau}_{IPW}\\), \\(\\hat{\\tau}_{CAIPW}\\) and their corresponding confidence sequences (CSs) at each step.\n\n\nShow the code\ndraw_prop &lt;- function(ate = 2.0, sigma = 1.0) {\n  prop_covar_coef &lt;- c(0.5, 0.3, 0.1)\n  prop_covar &lt;- rbinom(3, size = 1, prob = c(0.1, 0.4, 0.7))\n  prop &lt;- drop(prop_covar_coef %*% prop_covar) + 0.05\n  covar_coef &lt;- c(3, 2, 1)\n  covar &lt;- rbinom(3, size = 1, prob = c(0.25, 0.5, 0.75))\n  arm &lt;- rbinom(1, 1, prop)\n  y &lt;- (\n    0.5\n    + ate*arm\n    + drop(covar_coef %*% covar)\n    + drop(prop_covar_coef %*% prop_covar)\n    + rnorm(1, 0, sigma)\n  )\n  cbind(data.frame(\"y\" = y, \"t\" = arm, \"p\" = ifelse(arm, prop, 1 - prop)), data.frame(t(covar)))\n}\n\n# Simulation estimates\nestimates &lt;- simulate(\n  function(data) lm(y ~ . - p, data, weights = 1/data$p),\n  function(data) lm(y ~ t, data, weights = 1/data$p),\n  draw_fn = function(ate) draw_prop(ate = ate),\n  n = 2000,\n  ate = 0.5\n)\n\nggplot(\n  estimates,\n  aes(x = i, y = estimate, ymin = lower, ymax = upper)\n) +\n  geom_line(linewidth = 0.2) +\n  geom_ribbon(alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_vline(aes(xintercept = stat_sig_i_max), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(\n    ~ method,\n    ncol = 2,\n    scales = \"free\",\n  ) +\n  coord_cartesian(ylim = c(-0.5, 2)) +\n  labs(y = \"ATE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs expected, including covariates significantly improves the precision of our ATE estimates."
  },
  {
    "objectID": "blog/posts/anytime_valid_linear_models/index.html#type-1-error-control",
    "href": "blog/posts/anytime_valid_linear_models/index.html#type-1-error-control",
    "title": "Anytime-Valid Regression Adjusted Causal Inference",
    "section": "Type 1 error control",
    "text": "Type 1 error control\nFinally, we assess the empirical Type I error control by running multiple simulations under the null hypothesis (i.e., with \\(\\tau = 0\\)). For each simulation, we record whether a Type 1 error is ever committed; that is, whether the CS ever fails to cover \\(\\tau\\). The overall empirical Type I error is then the fraction of simulations in which the CS commits a Type I error. This global guarantee is analogous to the property of a standard CI: before data is observed, we know that the probability of a Type I error is \\(\\leq \\alpha\\).\n\n\nShow the code\nestimates &lt;- lapply(\n  1:100,\n  function(index) {\n    simulate(\n      model_ca_fn = function(data) lm(y ~ ., data),\n      model_dm_fn = function(data) lm(y ~ t, data),\n      draw_fn = function(ate) draw(rbinom(1, 1, 0.5), ate = ate),\n      n = 1000,\n      ate = 0\n    ) |&gt;\n    mutate(sim_i = index)\n  }\n)\n\nerror_rate &lt;- estimates |&gt;\n  bind_rows() |&gt;\n  group_by(sim_i, method) |&gt;\n  summarize(any_error = !all(covered)) |&gt;\n  ungroup() |&gt;\n  group_by(Method = method) |&gt;\n  summarize(`Type 1 error` = mean(any_error))\n\nerror_rate\n\n\n# A tibble: 4 √ó 2\n  Method                                  `Type 1 error`\n  &lt;chr&gt;                                            &lt;dbl&gt;\n1 Sequential CS                                     0.04\n2 Sequential CS w/o covariates                      0.04\n3 Sequential asymptotic CS                          0.04\n4 Sequential asymptotic CS w/o covariates           0.03\n\n\nWe see that, indeed, Type 1 error is \\(\\leq \\alpha = 0.05\\) for all estimators.\n\nCredits\nA lot of the code for this was based on the paper‚Äôs official repository and was crafted in tandem with my trusty AI assistant (ChatGPT)."
  }
]