---
title: "Housing Prices Prediction"
summary: "The goal of this short project is to document a fairly realistic ML pipeline, including data cleaning, data visualization, and model development, in both R and Python."
author: "Daniel Molitor"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: breezedark
    theme: journal
    toc: yes
    toc_depth: 4
    toc_float: yes
categories: ["R", "Python", "ML"]
image:
  caption: ""
  focal_point: ""
  placement: 2
  preview_only: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE,
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
set.seed(123) # for replicating the results
options(scipen = 999)

# Load helper functions
source("./utils.R")

# Set this venv's python .exe as the one to use
use_python_virtualenv(envdir = "~/.venv/housing")
```

The goal of this short project is to document a fairly realistic ML pipeline, including data cleaning, data visualization, and model development, in both R and Python.

# Python

## Requisite modules
```{python Requisite Modules}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedShuffleSplit
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.tree import DecisionTreeRegressor
```

## Import data
```{python Import Data}
housing = pd.read_csv(
  "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv"
)

# Get data structure
housing.info()
```

We can also quickly summarize the housing data.
```{python Describe Housing}
housing.describe()
```

Now that we have a quick overview of the data, we can plot a histogram of all
numeric values.
```{python Numeric Hist}
housing.hist(bins = 50, figsize = (30, 15));
plt.show()
```

We're going to bin the `median_income` variable to allow for stratified sampling
within income bins.
```{python Cut Income}
housing["median_income_bin"] = pd.cut(
  housing["median_income"],
  bins = [0, 1.5, 3, 4.5, 6, np.inf],
  labels = [1, 2, 3, 4, 5]
)

# Plot histogram of counts
housing["median_income_bin"].hist();
plt.show()
```

## Visualize data
Now, let's visualize the median house prices by plotting them geographically.
```{python Prices Geo}
(
  housing.
  rename(columns = {"median_house_value": "Median House Value"}).
  plot(
    kind = "scatter",
    x = "longitude",
    y = "latitude",
    alpha = 0.1,
    s = housing["population"]/100,
    c = "Median House Value",
    colormap = plt.get_cmap("jet"),
    colorbar = True,
    title = "Median House Prices by Population",
    xlabel = "Longitude",
    ylabel = "Latitude"
  )
)

plt.show()
```

Let's also look at the correlation between a few of our numeric variables.
```{python Cor Plot}
pd.plotting.scatter_matrix(
  housing[
    [
     "median_house_value",
     "median_income",
     "total_rooms",
     "housing_median_age"
    ]
  ],
  alpha = 0.1,
  figsize = (15, 8)
);

plt.show()
```

Let's specifically take a look at the relationship between `median_income` and
`median_house_value`.
```{python Income House Value}
(
  housing.
  rename(
    columns = {
      "median_income": "Median Income",
      "median_house_value": "Median House Value"
    }
  ).
  plot(
    kind = "scatter",
    x = "Median Income",
    y = "Median House Value",
    alpha = 0.1
  )
)

plt.show()
```

## Feature engineering
We want to create a couple new features: `rooms_per_household` and
`bedrooms_per_room`.
```{python Feature Engineering}
housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["pop_per_household"] = housing["population"]/housing["households"]
```

## Modeling prep
Let's do an 70-30 split of the data initially.
```{python Training Data}
split = StratifiedShuffleSplit(
  n_splits = 1,
  test_size = 0.3,
  random_state = 123
)

for train_idx, test_idx in split.split(housing, housing["median_income_bin"]):
  train = housing.loc[train_idx].drop(
    ["median_income_bin", "median_house_value"],
    axis = 1
  )
  train_labels = housing.loc[train_idx, "median_house_value"]
  test = housing.loc[test_idx].drop(
    ["median_income_bin", "median_house_value"],
    axis = 1
  )
  test_labels = housing.loc[test_idx, "median_house_value"]
```

Now, let's prep the data for modeling.
```{python Modeling Prep}
# Numeric transformations
num_transform = Pipeline(
  [
    ("imputer", SimpleImputer(strategy = "median")),
    ("scaler", StandardScaler())
  ]
)

full_transform = ColumnTransformer(
  [
    (
     "numeric",
     num_transform,
     list(train.drop("ocean_proximity", axis = 1).columns)
    ),
    ("categorical", OneHotEncoder(), ["ocean_proximity"])
  ]
)

train = full_transform.fit_transform(train)
test = full_transform.transform(test)
```

## Train models
Let's quick chalk up a function to print CV metrics.
```{python Metrics Fn}
def metrics_summary(scores):
  print("Mean: ", scores.mean().round(2))
  print("Standard Dev.: ", scores.std().round(2))

```

### Linear Regression
The first model is a simple linear regression model.
```{python OLS}
# Fit model
lin_reg_scores = np.sqrt(
  -cross_val_score(
     LinearRegression(),
     train,
     train_labels,
     scoring = "neg_mean_squared_error",
     cv = 10
   )
)

# Summarize score info
metrics_summary(lin_reg_scores)
```

### Decision Tree

The next model is a single decision tree model.
```{python Decision Tree}
# Fit model
tree_reg = np.sqrt(
  -cross_val_score(
     DecisionTreeRegressor(),
     train,
     train_labels,
     scoring = "neg_mean_squared_error",
     cv = 10
   )
)

# Summarize score info
metrics_summary(tree_reg)
```

### Random Forest

Now, we will run a random forest model.
```{python Random Forest}
# Fit model
forest_reg = np.sqrt(
  -cross_val_score(
     RandomForestRegressor(n_estimators = 30),
     train,
     train_labels,
     scoring = "neg_mean_squared_error",
     cv = 10
   )
)

# Summarize score info
metrics_summary(forest_reg)
```

And finally, we will tune our random forest model via grid search.
```{python Random Forest Grid}
# Fit model
forest_reg_grid = GridSearchCV(
  RandomForestRegressor(),
  param_grid = {
    "n_estimators": [30],
    "min_samples_leaf": [5],
    "max_features": ["sqrt"]
  },
  scoring = "neg_mean_squared_error",
  cv = 5,
  return_train_score = True
)

forest_reg_grid_fit = forest_reg_grid.fit(train, train_labels)

# Get CV results
cv_results = forest_reg_grid_fit.cv_results_

# Summarize score info
for mean_sc, param in zip(cv_results["mean_test_score"], cv_results["params"]):
  print("Mean: ", np.sqrt(-mean_sc).round(2))
  print("Parameters: ", param)

```

## Final predictions
```{python Final Predictions}
# Extract model
final_model = forest_reg_grid_fit.best_estimator_

# Make predictions
final_preds = final_model.predict(test)

# Final RMSE
print(
  "Test RMSE: ", np.sqrt(mean_squared_error(test_labels, final_preds)).round(2)
)
```

# R

## Requisite packages
```{r Requisite Packages}
library(dplyr)
library(ggplot2)
library(purrr)
library(ranger)
library(readr)
library(rpart)
library(rsample)
library(sf)
library(tidyr)
library(viridis)
library(yardstick)
```

## Import data
```{r Import Data 2}
housing <- read_csv(
  "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv",
  show_col_types = FALSE
)

# Get data structure
glimpse(housing)
```

Now, to get a quick summary of the data.
```{r Describe Housing 2}
summary(housing)
```

Now that we have a quick overview of the data, we can plot a histogram of all
numeric values.
```{r Numeric Hist 2, fig.height = 8, fig.width = 15}
housing |>
  select(where(is.numeric)) |>
  pivot_longer(
    cols = everything(),
    names_to = "variable",
    values_to = "values"
  ) |>
  ggplot(aes(x = values)) +
  geom_histogram(fill = "lightblue", bins = 50) +
  facet_wrap(
    ~ variable,
    nrow = 3,
    scales = "free"
  ) +
  labs(
    x = "",
    y = ""
  ) +
  theme_minimal()
```

We're going to bin the `median_income` variable to allow for stratified sampling
within income bins.
```{r Cut Income 2}
housing <- housing |>
  mutate(
    median_income_bin = cut(
      median_income,
      breaks = c(0, 1.5, 3, 4.5, 6, Inf),
      labels = c(1, 2, 3, 4, 5)
    )
  )

# Plot histogram of counts
hist(
  as.numeric(housing$median_income_bin),
  main = "",
  xlab = "Median Income Bin",
  ylab = "Count",
  ylim = c(0, 8000)
)
```

## Visualize data
Now, let's visualize the median house prices by plotting them geographically.
```{r Prices Geo 2, fig.height = 7, fig.width = 12}
housing |>
  rename(
    "Population" = "population",
    "Median House Value" = "median_house_value"
  ) |>
  st_as_sf(coords = c("longitude", "latitude")) |>
  st_set_crs(value = 4326) |>
  ggplot(
    aes(
      size = Population,
      color = `Median House Value`
    )
  ) +
  geom_sf(alpha = 0.1) +
  theme_minimal() +
  theme(
    axis.text = element_text(face = "bold"),
    legend.title = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5, face = "bold")
  ) +
  scale_color_viridis(option = "B") +
  guides(size = "none") +
  labs(
    title = "Median House Prices by Population",
    x = "Longitude",
    y = "Latitude"
  )
```

Let's also look at the correlation between a few of our numeric variables.
```{r Cor Plot 2, fig.height = 7, fig.width = 12}
plot(
  select(
    housing,
    c(
      "median_house_value",
      "median_income",
      "total_rooms",
      "housing_median_age"
    )
  ),
  col = rgb(red = 0, green = 0, blue = 0, alpha = 0.1)
)
```

Let's specifically take a look at the relationship between `median_income` and
`median_house_value`.
```{r Income House Value 2}
housing |>
  ggplot(aes(x = median_income, y = median_house_value)) +
  geom_point(alpha = 0.1) +
  theme_minimal() +
  theme(
    axis.text = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5, face = "bold")
  ) +
  labs(x = "Median Income", y = "Median House Value")
```

## Feature engineering
We want to create a couple new features: `rooms_per_household` and
`bedrooms_per_room`.
```{r Feature Engineering 2}
housing <- housing |>
  mutate(
    rooms_per_household = total_rooms/households,
    bedrooms_per_room = total_bedrooms/total_rooms,
    pop_per_household = population/households
  )
```

## Modeling prep
Let's do an 70-30 split of the data initially.
```{r Training Data 2}
set.seed(123)
split <- initial_split(
  data = housing,
  prop = 0.7,
  strata = "median_income_bin"
)

train <- training(split) |> select(-median_income_bin)
test <- testing(split) |> select(-median_income_bin)
```

Now, let's prep the data for modeling.
```{r Modeling Prep 2}
# Get median values from training data
medians <- suppressWarnings(
  lapply(train, median, na.rm = TRUE)
)

# Median impute
train <- map2_df(
    .x = train,
    .y = medians,
    .f = ~ case_when(
      is.numeric(.x) ~ replace_na(.x, .y),
      TRUE ~ .x
    )
) |>
  mutate(ocean_proximity = sort_factor(ocean_proximity))

test <- map2_df(
    .x = test,
    .y = medians,
    .f = ~ case_when(
      is.numeric(.x) ~ replace_na(.x, .y),
      TRUE ~ .x
    )
) |>
  mutate(ocean_proximity = sort_factor(ocean_proximity))
```

## Train models
First, let's split the training data into a 10-fold CV split and make a quick
function to print CV metrics.
```{r 10-fold CV Split}
train_cv <- vfold_cv(train, v = 10)

# Summarise CV metrics
metrics_summary <- function(scores) {
  print(paste0("Mean: ", round(mean(scores), 2)))
  print(paste0("Standard Dev.: ", round(sd(scores), 2)))
}
```

### Linear Regression
The first model is a simple linear regression model.
```{r OLS 2}
# Fit model
lin_reg_scores = map_dbl(
  train_cv$splits,
  function(i) {
    lin_mod <- lm(median_house_value ~ ., data = i$data[i$in_id, ])
    lin_preds <- predict(lin_mod, i$data[-i$in_id, ])
    rmse_vec(i$data[-i$in_id, ][["median_house_value"]], lin_preds)
  }
)

# Summarize score info
metrics_summary(lin_reg_scores)
```

### Decision Tree

The next model is a single decision tree model.
```{r Decision Tree 2}
# Fit model
tree_reg_scores = map_dbl(
  train_cv$splits,
  function(i) {
    tree_mod <- rpart(median_house_value ~ ., data = i$data[i$in_id, ])
    tree_preds <- predict(tree_mod, i$data[-i$in_id, ])
    rmse_vec(i$data[-i$in_id, ][["median_house_value"]], tree_preds)
  }
)

# Summarize score info
metrics_summary(tree_reg_scores)
```

### Random Forest

Now, we will run a random forest model.
```{r Random Forest 2}
# Fit model
forest_reg_scores = map_dbl(
  train_cv$splits,
  function(i) {
    forest_mod <- ranger(median_house_value ~ ., data = i$data[i$in_id, ], 100)
    forest_preds <- predict(forest_mod, i$data[-i$in_id, ])$predictions
    rmse_vec(i$data[-i$in_id, ][["median_house_value"]], forest_preds)
  }
)

# Summarize score info
metrics_summary(tree_reg_scores)
```

And finally, we will tune our random forest model via grid search.
```{r Random Forest Grid 2}
# Create grid
param_grid <- expand.grid(num.trees = 100, mtry = 5, min.node.size = 3)

# Fit model
forest_reg_grid <- map(
  1:nrow(param_grid),
  function(grid_idx) {
    map_dbl(
      train_cv$splits,
      function(cv_fold) {
        # LOO CV Model
        forest_mod <- ranger(
          median_house_value ~ .,
          data = cv_fold$data[cv_fold$in_id, ],
          num.trees = param_grid[grid_idx, ]$num.trees,
          mtry = param_grid[grid_idx, ]$mtry,
          min.node.size = param_grid[grid_idx, ]$min.node.size
        )
        # OOS Predictions
        forest_preds <- predict(forest_mod, cv_fold$data[-cv_fold$in_id, ])$predictions
        # Calculate RMSE
        rmse_vec(
          truth = cv_fold$data[-cv_fold$in_id, ][["median_house_value"]],
          estimate = forest_preds
        )
      }
    )
  }
)

# Get best model
best_mod <- which.min(map_dbl(forest_reg_grid, mean))
forest_reg_grid_fit = ranger(
  median_house_value ~ .,
  data = train,
  num.trees = param_grid[best_mod, ]$num.trees,
  mtry = param_grid[best_mod, ]$mtry,
  min.node.size = param_grid[best_mod, ]$min.node.size
)

# Summarize score info
walk(forest_reg_grid, metrics_summary)
```

## Final predictions
```{r Final Predictions 2}
# Extract model
final_model = forest_reg_grid_fit

# Make predictions
final_preds = predict(final_model, test)$predictions

# Final RMSE
cat(
  "Test RMSE:", rmse_vec(test$median_house_value, final_preds)
)
```
